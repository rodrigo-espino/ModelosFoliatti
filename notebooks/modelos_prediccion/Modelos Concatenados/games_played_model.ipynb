{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5d3eee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import warnings\n",
    "import joblib\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2500be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_general = pd.read_csv('../../../data/data_general.csv')\n",
    "\n",
    "df_data_general['INITIAL_TIME'] = pd.to_datetime(df_data_general['INITIAL_TIME'])\n",
    "df_data_general['FINAL_TIME'] = pd.to_datetime(df_data_general['FINAL_TIME'])\n",
    "\n",
    "df_data_general['INITIAL_TIME'] = df_data_general['INITIAL_TIME'].dt.to_period('D')\n",
    "df_data_general['INITIAL_TIME'] = df_data_general['INITIAL_TIME'].dt.to_timestamp()\n",
    "\n",
    "\n",
    "df_data_general['FINAL_TIME'] = df_data_general['FINAL_TIME'].dt.to_period('D')\n",
    "df_data_general['FINAL_TIME'] = df_data_general['FINAL_TIME'].dt.to_timestamp()\n",
    "\n",
    "df_data_general['Weekday']= df_data_general['INITIAL_TIME'].dt.strftime('%A')\n",
    "df_data_general['number_of_day'] = df_data_general['INITIAL_TIME'].dt.day_of_week\n",
    "\n",
    "df_data_general['TIME_ON_DEVICE_MIN'] = df_data_general['TIME_ON_DEVICE_SEC'] / 60\n",
    "\n",
    "df_data_general['Hour'] = df_data_general['INITIAL_TIME'].dt.hour\n",
    "df_data_general['Weekday'] = df_data_general['INITIAL_TIME'].dt.weekday   # 0=Lunes, 6=Domingo\n",
    "df_data_general['Weekend'] = (df_data_general['Weekday'] >= 5).astype(int)\n",
    "df_data_general['Month'] = df_data_general['INITIAL_TIME'].dt.month\n",
    "\n",
    "df_data_general = df_data_general[df_data_general['TIME_ON_DEVICE_MIN'] < 600 ]\n",
    "\n",
    "df_data_general = df_data_general[df_data_general['WIN_TOTAL'] > 0]\n",
    "df_data_general['NET_SPEND'] = df_data_general['FINAL_AMOUNT'] - df_data_general['INITIAL_AMOUNT']\n",
    "df_data_general = df_data_general[df_data_general['NET_SPEND'] < 10000 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506cd1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéÆ Ensemble Regression Model listo para predecir GAMES_PLAYED_TOTAL!\n",
      "üìñ Lee las instrucciones en los comentarios para comenzar.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.utils import resample\n",
    "from tensorflow.keras.models import load_model, Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class CasinoEnsembleRegressionOptimized:\n",
    "    def __init__(self, model_paths):\n",
    "        \"\"\"\n",
    "        Inicializa el modelo ensemble de regresi√≥n cargando los modelos pre-entrenados\n",
    "        \n",
    "        Args:\n",
    "            model_paths (dict): Diccionario con las rutas de los modelos y scalers\n",
    "        \"\"\"\n",
    "        self.model_paths = model_paths\n",
    "        self.models = {}\n",
    "        self.scalers = {}\n",
    "        self.ensemble_scaler = StandardScaler()\n",
    "        self.target_scaler = StandardScaler()  # Para escalar el target en algunos modelos\n",
    "        self.trained_regressors = {}\n",
    "        \n",
    "        # Cargar modelos pre-entrenados\n",
    "        self.load_pretrained_models()\n",
    "    \n",
    "    def load_pretrained_models(self):\n",
    "        \"\"\"Carga los modelos y scalers pre-entrenados\"\"\"\n",
    "        try:\n",
    "            # Cargar modelos de Keras\n",
    "            self.models['time'] = load_model(self.model_paths['tiempo_model'])\n",
    "            self.models['bet'] = load_model(self.model_paths['bet_model'])\n",
    "            self.models['win'] = load_model(self.model_paths['win_model'])\n",
    "            \n",
    "            # Cargar scalers con joblib\n",
    "            self.scalers['time'] = joblib.load(self.model_paths['tiempo_scaler'])\n",
    "            self.scalers['bet'] = joblib.load(self.model_paths['bet_scaler'])\n",
    "            self.scalers['win'] = joblib.load(self.model_paths['win_scaler'])\n",
    "            \n",
    "            print(\"‚úÖ Todos los modelos y scalers cargados correctamente\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error cargando modelos: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def create_business_features(self, base_data, tiempo_pred=None, bet_pred=None):\n",
    "        \"\"\"\n",
    "        Crea las features de negocio necesarias para los modelos\n",
    "        \"\"\"\n",
    "        features = base_data[['INITIAL_AMOUNT', 'AVG_BET', 'Cluster']].copy()\n",
    "        \n",
    "        if tiempo_pred is not None:\n",
    "            features['tiempo_pred'] = tiempo_pred\n",
    "            \n",
    "        if bet_pred is not None:\n",
    "            features['bet_pred'] = bet_pred\n",
    "            features['total_money_handled'] = bet_pred\n",
    "            features['house_edge_effect'] = bet_pred * 0.05\n",
    "            features['money_multiplier'] = bet_pred / (base_data['INITIAL_AMOUNT'] + 1)\n",
    "            features['reinvestment_indicator'] = np.where(bet_pred > base_data['INITIAL_AMOUNT'], 1, 0)\n",
    "            features['excess_betting'] = np.maximum(0, bet_pred - base_data['INITIAL_AMOUNT'])\n",
    "            features['money_at_risk'] = np.minimum(bet_pred, base_data['INITIAL_AMOUNT'])\n",
    "            features['cluster_risk_adjusted'] = base_data['Cluster'] * features['money_multiplier']\n",
    "            \n",
    "        return features\n",
    "    \n",
    "    def predict_session(self, initial_amount, avg_bet, cluster, weekday=1, weekend=0, month=1):\n",
    "        \"\"\"\n",
    "        Hacer predicci√≥n completa para una sesi√≥n usando los modelos MLP en secuencia\n",
    "        \"\"\"\n",
    "        if not self.models:\n",
    "            return {\"error\": \"Modelos no cargados correctamente\"}\n",
    "        \n",
    "        # Preparar datos base\n",
    "        base_data = pd.DataFrame({\n",
    "            'INITIAL_AMOUNT': [initial_amount],\n",
    "            'AVG_BET': [avg_bet], \n",
    "            'Cluster': [cluster],\n",
    "            'Weekday': [weekday],\n",
    "            'Weekend': [weekend],\n",
    "            'Month': [month]\n",
    "        })\n",
    "        \n",
    "        try:\n",
    "            results = {}\n",
    "            \n",
    "            # 1. Predecir TIEMPO\n",
    "            if 'time' in self.models:\n",
    "                try:\n",
    "                    X_tiempo = self.create_business_features(base_data)\n",
    "                    X_tiempo_scaled = self.scalers['time'].transform(X_tiempo)\n",
    "                    tiempo_pred = float(self.models['time'].predict(X_tiempo_scaled, verbose=0)[0][0])\n",
    "                    tiempo_pred = max(0, tiempo_pred)\n",
    "                    results['time_on_device'] = tiempo_pred\n",
    "                except Exception as e:\n",
    "                    print(f\"Error prediciendo tiempo: {e}\")\n",
    "                    tiempo_pred = 30.0\n",
    "                    results['time_on_device'] = tiempo_pred\n",
    "            else:\n",
    "                tiempo_pred = 30.0\n",
    "                results['time_on_device'] = tiempo_pred\n",
    "            \n",
    "            # 2. Predecir BET TOTAL\n",
    "            if 'bet' in self.models:\n",
    "                try:\n",
    "                    X_bet = self.create_business_features(base_data, tiempo_pred=tiempo_pred)\n",
    "                    X_bet_scaled = self.scalers['bet'].transform(X_bet)\n",
    "                    bet_pred = float(self.models['bet'].predict(X_bet_scaled, verbose=0)[0][0])\n",
    "                    bet_pred = max(0, bet_pred)\n",
    "                    results['bet_total'] = bet_pred\n",
    "                except Exception as e:\n",
    "                    print(f\"Error prediciendo bet: {e}\")\n",
    "                    bet_pred = initial_amount * 2\n",
    "                    results['bet_total'] = bet_pred\n",
    "            else:\n",
    "                bet_pred = initial_amount * 2\n",
    "                results['bet_total'] = bet_pred\n",
    "            \n",
    "            # 3. Predecir WIN TOTAL\n",
    "            if 'win' in self.models:\n",
    "                try:\n",
    "                    X_win = self.create_business_features(base_data, tiempo_pred=tiempo_pred, bet_pred=bet_pred)\n",
    "                    X_win_scaled = self.scalers['win'].transform(X_win)\n",
    "                    win_pred = float(self.models['win'].predict(X_win_scaled, verbose=0)[0][0])\n",
    "                    win_pred = max(0, win_pred)\n",
    "                    results['win_total'] = win_pred\n",
    "                except Exception as e:\n",
    "                    print(f\"Error prediciendo win: {e}\")\n",
    "                    win_pred = bet_pred * 0.95\n",
    "                    results['win_total'] = win_pred\n",
    "            else:\n",
    "                win_pred = bet_pred * 0.95\n",
    "                results['win_total'] = win_pred\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Error en predicci√≥n: {str(e)}\"}\n",
    "    \n",
    "    def generate_ensemble_features_batch(self, input_data, batch_size=1000):\n",
    "        \"\"\"\n",
    "        Versi√≥n OPTIMIZADA que procesa en lotes para mejor rendimiento\n",
    "        \"\"\"\n",
    "        ensemble_features = []\n",
    "        total_batches = len(input_data) // batch_size + (1 if len(input_data) % batch_size > 0 else 1)\n",
    "        \n",
    "        print(f\"üîÑ Procesando {len(input_data)} sesiones en {total_batches} lotes de {batch_size}...\")\n",
    "        \n",
    "        for batch_idx in range(0, len(input_data), batch_size):\n",
    "            batch_end = min(batch_idx + batch_size, len(input_data))\n",
    "            batch_data = input_data.iloc[batch_idx:batch_end]\n",
    "            \n",
    "            current_batch = (batch_idx // batch_size) + 1\n",
    "            print(f\"üì¶ Procesando lote {current_batch}/{total_batches} ({len(batch_data)} sesiones)...\")\n",
    "            \n",
    "            # Procesar tiempo en batch\n",
    "            X_tiempo_batch = []\n",
    "            for _, row in batch_data.iterrows():\n",
    "                base_data = pd.DataFrame({\n",
    "                    'INITIAL_AMOUNT': [row['INITIAL_AMOUNT']],\n",
    "                    'AVG_BET': [row['AVG_BET']],\n",
    "                    'Cluster': [row['Cluster']]\n",
    "                })\n",
    "                X_tiempo = self.create_business_features(base_data)\n",
    "                X_tiempo_batch.append(X_tiempo.iloc[0].values)\n",
    "            \n",
    "            X_tiempo_batch = np.array(X_tiempo_batch)\n",
    "            X_tiempo_scaled = self.scalers['time'].transform(X_tiempo_batch)\n",
    "            tiempo_preds = self.models['time'].predict(X_tiempo_scaled, verbose=0).flatten()\n",
    "            tiempo_preds = np.maximum(0, tiempo_preds)\n",
    "            \n",
    "            # Procesar bet en batch\n",
    "            X_bet_batch = []\n",
    "            for i, (_, row) in enumerate(batch_data.iterrows()):\n",
    "                base_data = pd.DataFrame({\n",
    "                    'INITIAL_AMOUNT': [row['INITIAL_AMOUNT']],\n",
    "                    'AVG_BET': [row['AVG_BET']],\n",
    "                    'Cluster': [row['Cluster']]\n",
    "                })\n",
    "                X_bet = self.create_business_features(base_data, tiempo_pred=tiempo_preds[i])\n",
    "                X_bet_batch.append(X_bet.iloc[0].values)\n",
    "            \n",
    "            X_bet_batch = np.array(X_bet_batch)\n",
    "            X_bet_scaled = self.scalers['bet'].transform(X_bet_batch)\n",
    "            bet_preds = self.models['bet'].predict(X_bet_scaled, verbose=0).flatten()\n",
    "            bet_preds = np.maximum(0, bet_preds)\n",
    "            \n",
    "            # Procesar win en batch\n",
    "            X_win_batch = []\n",
    "            for i, (_, row) in enumerate(batch_data.iterrows()):\n",
    "                base_data = pd.DataFrame({\n",
    "                    'INITIAL_AMOUNT': [row['INITIAL_AMOUNT']],\n",
    "                    'AVG_BET': [row['AVG_BET']],\n",
    "                    'Cluster': [row['Cluster']]\n",
    "                })\n",
    "                X_win = self.create_business_features(base_data, tiempo_pred=tiempo_preds[i], bet_pred=bet_preds[i])\n",
    "                X_win_batch.append(X_win.iloc[0].values)\n",
    "            \n",
    "            X_win_batch = np.array(X_win_batch)\n",
    "            X_win_scaled = self.scalers['win'].transform(X_win_batch)\n",
    "            win_preds = self.models['win'].predict(X_win_scaled, verbose=0).flatten()\n",
    "            win_preds = np.maximum(0, win_preds)\n",
    "            \n",
    "            # Crear features del ensemble para este batch\n",
    "            for i, (_, row) in enumerate(batch_data.iterrows()):\n",
    "                feature_vector = {\n",
    "                    'time_on_device': tiempo_preds[i],\n",
    "                    'bet_total': bet_preds[i],\n",
    "                    'win_total': win_preds[i],\n",
    "                    'initial_amount': row['INITIAL_AMOUNT'],\n",
    "                    'cluster': row['Cluster'],\n",
    "                    'avg_bet': row['AVG_BET']\n",
    "                }\n",
    "                ensemble_features.append(feature_vector)\n",
    "            \n",
    "            # Progreso\n",
    "            progress = min(((batch_idx + batch_size) / len(input_data)) * 100, 100)\n",
    "            print(f\"‚úÖ Lote {current_batch} completado. Progreso: {progress:.1f}%\")\n",
    "        \n",
    "        result_df = pd.DataFrame(ensemble_features)\n",
    "        \n",
    "        print(f\"‚úÖ Features del ensemble generadas exitosamente:\")\n",
    "        print(f\"   - {len(result_df)} sesiones procesadas\")\n",
    "        print(f\"   - Variables generadas por modelos MLP: time_on_device, bet_total, win_total\")\n",
    "        print(f\"   - Variables originales: initial_amount, cluster, avg_bet\")\n",
    "        print(f\"   - Total features para regresi√≥n: {result_df.shape[1]}\")\n",
    "        \n",
    "        # Mostrar estad√≠sticas de las predicciones generadas\n",
    "        print(f\"\\nüìä Estad√≠sticas de predicciones generadas:\")\n",
    "        print(f\"   time_on_device: {result_df['time_on_device'].mean():.2f} ¬± {result_df['time_on_device'].std():.2f}\")\n",
    "        print(f\"   bet_total: ${result_df['bet_total'].mean():.2f} ¬± ${result_df['bet_total'].std():.2f}\")\n",
    "        print(f\"   win_total: ${result_df['win_total'].mean():.2f} ¬± ${result_df['win_total'].std():.2f}\")\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    def generate_ensemble_features(self, input_data):\n",
    "        \"\"\"\n",
    "        Wrapper que usa la versi√≥n batch optimizada\n",
    "        \"\"\"\n",
    "        return self.generate_ensemble_features_batch(input_data, batch_size=1000)\n",
    "    \n",
    "    def prepare_ensemble_features(self, data, target_col='GAMES_PLAYED_TOTAL'):\n",
    "        \"\"\"\n",
    "        Prepara las features para el modelo ensemble usando predicciones batch\n",
    "        \"\"\"\n",
    "        # Verificar que tenemos las features base necesarias\n",
    "        required_base_features = ['INITIAL_AMOUNT', 'AVG_BET', 'Cluster']\n",
    "        missing_base = [f for f in required_base_features if f not in data.columns]\n",
    "        if missing_base:\n",
    "            raise ValueError(f\"Features base faltantes para generar predicciones: {missing_base}\")\n",
    "        \n",
    "        # Verificar que tenemos la columna objetivo\n",
    "        if target_col not in data.columns:\n",
    "            raise ValueError(f\"Columna objetivo '{target_col}' no encontrada\")\n",
    "        \n",
    "        print(\"üîÑ Generando predicciones usando modelos individuales (procesamiento batch)...\")\n",
    "        X = self.generate_ensemble_features(data)\n",
    "        y = data[target_col].iloc[:len(X)]  # Asegurar mismo length en caso de filas fallidas\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def train_ensemble_models_sampled(self, data, target_col='GAMES_PLAYED_TOTAL', sample_size=10000, test_size=0.2):\n",
    "        \"\"\"\n",
    "        Entrena usando una muestra aleatoria para acelerar el proceso\n",
    "        \"\"\"\n",
    "        print(f\"üéØ Usando muestreo aleatorio de {sample_size} sesiones de {len(data)} totales\")\n",
    "        \n",
    "        # Muestreo aleatorio simple\n",
    "        if len(data) > sample_size:\n",
    "            sampled_data = resample(data, n_samples=sample_size, random_state=42, replace=False)\n",
    "        else:\n",
    "            sampled_data = data.copy()\n",
    "            sample_size = len(sampled_data)\n",
    "        \n",
    "        print(f\"üìä Muestra seleccionada: {len(sampled_data)} sesiones\")\n",
    "        print(f\"üìä Distribuci√≥n del target:\")\n",
    "        print(f\"   Media: {sampled_data[target_col].mean():.2f}\")\n",
    "        print(f\"   Mediana: {sampled_data[target_col].median():.2f}\")\n",
    "        print(f\"   Std: {sampled_data[target_col].std():.2f}\")\n",
    "        print(f\"   Min: {sampled_data[target_col].min()}\")\n",
    "        print(f\"   Max: {sampled_data[target_col].max()}\")\n",
    "        \n",
    "        # Entrenar con la muestra\n",
    "        return self.train_ensemble_models(sampled_data, target_col, test_size)\n",
    "    \n",
    "    def create_rnn_model(self, input_shape, target_mean=None, target_std=None):\n",
    "        \"\"\"Crea modelo RNN/LSTM para regresi√≥n\"\"\"\n",
    "        model = Sequential([\n",
    "            LSTM(64, return_sequences=True, input_shape=(input_shape, 1)),\n",
    "            Dropout(0.2),\n",
    "            LSTM(32, return_sequences=False),\n",
    "            Dropout(0.2),\n",
    "            Dense(16, activation='relu'),\n",
    "            Dense(1, activation='linear')  # Para regresi√≥n\n",
    "        ])\n",
    "        \n",
    "        model.compile(optimizer=Adam(learning_rate=0.001), \n",
    "                     loss='mse', \n",
    "                     metrics=['mae'])\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train_mlp_model(self, X_train, y_train, X_test, y_test):\n",
    "        \"\"\"Entrena y eval√∫a modelo MLP para regresi√≥n\"\"\"\n",
    "        print(\"\\nüß† Entrenando modelo MLP...\")\n",
    "        \n",
    "        # Par√°metros para MLP\n",
    "        param_grid = {\n",
    "            'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50), (100, 100)],\n",
    "            'activation': ['relu', 'tanh'],\n",
    "            'learning_rate_init': [0.001, 0.01],\n",
    "            'alpha': [0.0001, 0.001, 0.01],\n",
    "            'max_iter': [1000]\n",
    "        }\n",
    "        \n",
    "        mlp = MLPRegressor(random_state=42)\n",
    "        grid_search = GridSearchCV(mlp, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        best_mlp = grid_search.best_estimator_\n",
    "        self.trained_regressors['MLP'] = best_mlp\n",
    "        \n",
    "        # Predicciones\n",
    "        y_pred = best_mlp.predict(X_test)\n",
    "        \n",
    "        # M√©tricas\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        \n",
    "        print(f\"Mejores par√°metros: {grid_search.best_params_}\")\n",
    "        print(f\"MSE: {mse:.4f}\")\n",
    "        print(f\"RMSE: {rmse:.4f}\")\n",
    "        print(f\"MAE: {mae:.4f}\")\n",
    "        print(f\"R¬≤: {r2:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'model': best_mlp,\n",
    "            'mse': mse,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'r2_score': r2,\n",
    "            'predictions': y_pred\n",
    "        }\n",
    "    \n",
    "    def train_random_forest_model(self, X_train, y_train, X_test, y_test):\n",
    "        \"\"\"Entrena y eval√∫a modelo Random Forest para regresi√≥n\"\"\"\n",
    "        print(\"\\nüå≥ Entrenando modelo Random Forest...\")\n",
    "        \n",
    "        # Par√°metros para Random Forest\n",
    "        param_grid = {\n",
    "            'max_depth': [10, 20, 30, None],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'max_features': ['sqrt', 'log2', None]\n",
    "        }\n",
    "        \n",
    "        rf = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "        grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        best_rf = grid_search.best_estimator_\n",
    "        self.trained_regressors['RandomForest'] = best_rf\n",
    "        \n",
    "        # Predicciones\n",
    "        y_pred = best_rf.predict(X_test)\n",
    "        \n",
    "        # M√©tricas\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        \n",
    "        print(f\"Mejores par√°metros: {grid_search.best_params_}\")\n",
    "        print(f\"MSE: {mse:.4f}\")\n",
    "        print(f\"RMSE: {rmse:.4f}\")\n",
    "        print(f\"MAE: {mae:.4f}\")\n",
    "        print(f\"R¬≤: {r2:.4f}\")\n",
    "        \n",
    "        # Importancia de features\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': X_train.columns,\n",
    "            'importance': best_rf.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(\"\\nImportancia de features:\")\n",
    "        print(feature_importance)\n",
    "        \n",
    "        return {\n",
    "            'model': best_rf,\n",
    "            'mse': mse,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'r2_score': r2,\n",
    "            'predictions': y_pred,\n",
    "            'feature_importance': feature_importance\n",
    "        }\n",
    "    \n",
    "    def train_xgboost_model(self, X_train, y_train, X_test, y_test):\n",
    "        \"\"\"Entrena y eval√∫a modelo XGBoost para regresi√≥n\"\"\"\n",
    "        print(\"\\nüöÄ Entrenando modelo XGBoost...\")\n",
    "        \n",
    "        # Par√°metros para XGBoost\n",
    "        param_grid = {\n",
    "            'max_depth': [3, 6, 10],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'subsample': [0.8, 1.0],\n",
    "            'colsample_bytree': [0.8, 1.0]\n",
    "        }\n",
    "        \n",
    "        xgb_reg = xgb.XGBRegressor(random_state=42, n_jobs=-1)\n",
    "        grid_search = GridSearchCV(xgb_reg, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        best_xgb = grid_search.best_estimator_\n",
    "        self.trained_regressors['XGBoost'] = best_xgb\n",
    "        \n",
    "        # Predicciones\n",
    "        y_pred = best_xgb.predict(X_test)\n",
    "        \n",
    "        # M√©tricas\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        \n",
    "        print(f\"Mejores par√°metros: {grid_search.best_params_}\")\n",
    "        print(f\"MSE: {mse:.4f}\")\n",
    "        print(f\"RMSE: {rmse:.4f}\")\n",
    "        print(f\"MAE: {mae:.4f}\")\n",
    "        print(f\"R¬≤: {r2:.4f}\")\n",
    "        \n",
    "        # Importancia de features\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': X_train.columns,\n",
    "            'importance': best_xgb.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(\"\\nImportancia de features:\")\n",
    "        print(feature_importance)\n",
    "        \n",
    "        return {\n",
    "            'model': best_xgb,\n",
    "            'mse': mse,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'r2_score': r2,\n",
    "            'predictions': y_pred,\n",
    "            'feature_importance': feature_importance\n",
    "        }\n",
    "    \n",
    "    def train_rnn_model(self, X_train, y_train, X_test, y_test):\n",
    "        \"\"\"Entrena y eval√∫a modelo RNN para regresi√≥n\"\"\"\n",
    "        print(\"\\nüîÑ Entrenando modelo RNN/LSTM...\")\n",
    "        \n",
    "        # Ajustar datos para RNN (necesita 3D: [samples, timesteps, features])\n",
    "        X_train_rnn = X_train.values.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "        X_test_rnn = X_test.values.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "        \n",
    "        # Escalar target para mejor entrenamiento\n",
    "        y_train_scaled = self.target_scaler.fit_transform(y_train.values.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Crear y entrenar modelo\n",
    "        rnn_model = self.create_rnn_model(X_train.shape[1])\n",
    "        \n",
    "        # Callbacks\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        \n",
    "        # Entrenar\n",
    "        history = rnn_model.fit(\n",
    "            X_train_rnn, y_train_scaled,\n",
    "            epochs=100,\n",
    "            batch_size=32,\n",
    "            validation_split=0.2,\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        self.trained_regressors['RNN'] = rnn_model\n",
    "        \n",
    "        # Predicciones\n",
    "        y_pred_scaled = rnn_model.predict(X_test_rnn, verbose=0).flatten()\n",
    "        y_pred = self.target_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # M√©tricas\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        \n",
    "        print(f\"MSE: {mse:.4f}\")\n",
    "        print(f\"RMSE: {rmse:.4f}\")\n",
    "        print(f\"MAE: {mae:.4f}\")\n",
    "        print(f\"R¬≤: {r2:.4f}\")\n",
    "        print(f\"√âpocas entrenadas: {len(history.history['loss'])}\")\n",
    "        \n",
    "        return {\n",
    "            'model': rnn_model,\n",
    "            'mse': mse,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'r2_score': r2,\n",
    "            'predictions': y_pred,\n",
    "            'history': history\n",
    "        }\n",
    "    \n",
    "    def train_ensemble_models(self, data, target_col='GAMES_PLAYED_TOTAL', test_size=0.2):\n",
    "        \"\"\"\n",
    "        Entrena todos los modelos ensemble usando las predicciones de los modelos individuales\n",
    "        \"\"\"\n",
    "        print(\"üöÄ Iniciando entrenamiento de modelos ensemble para regresi√≥n...\")\n",
    "        print(\"üéØ Target: Predicci√≥n de GAMES_PLAYED_TOTAL\")\n",
    "        print(\"üìù Las features time_on_device, bet_total, win_total se generar√°n desde los modelos pre-entrenados\")\n",
    "        \n",
    "        # Verificar columnas requeridas\n",
    "        required_cols = ['INITIAL_AMOUNT', 'AVG_BET', 'Cluster', target_col]\n",
    "        missing_cols = [col for col in required_cols if col not in data.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Columnas requeridas faltantes: {missing_cols}\")\n",
    "        \n",
    "        print(f\"üìã Columnas disponibles: {list(data.columns)}\")\n",
    "        print(f\"üìä Dataset shape: {data.shape}\")\n",
    "        \n",
    "        # Preparar features (genera predicciones desde modelos MLP usando batch processing)\n",
    "        X, y = self.prepare_ensemble_features(data, target_col)\n",
    "        \n",
    "        print(f\"üìä Datos preparados: {X.shape[0]} muestras, {X.shape[1]} features\")\n",
    "        print(f\"üìà Estad√≠sticas del target:\")\n",
    "        print(f\"   Media: {y.mean():.2f}\")\n",
    "        print(f\"   Mediana: {y.median():.2f}\")\n",
    "        print(f\"   Std: {y.std():.2f}\")\n",
    "        print(f\"   Min: {y.min()}, Max: {y.max()}\")\n",
    "        print(f\"üéØ Features del ensemble: {list(X.columns)}\")\n",
    "        \n",
    "        # Dividir datos\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Escalar features\n",
    "        X_train_scaled = self.ensemble_scaler.fit_transform(X_train)\n",
    "        X_test_scaled = self.ensemble_scaler.transform(X_test)\n",
    "        \n",
    "        X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "        X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "        \n",
    "        # Entrenar modelos\n",
    "        results = {}\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"MLP Start\")\n",
    "        results['MLP'] = self.train_mlp_model(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "        print(\"MLP End\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"RF Start\")\n",
    "        results['RandomForest'] = self.train_random_forest_model(X_train, y_train, X_test, y_test)\n",
    "        print(\"RF End\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"XGBoost Start\")\n",
    "        results['XGBoost'] = self.train_xgboost_model(X_train, y_train, X_test, y_test)\n",
    "        print(\"XGBoost End\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"RNN Start\")\n",
    "        results['RNN'] = self.train_rnn_model(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "        print(\"RNN End\")\n",
    "        \n",
    "        # Resumen de resultados\n",
    "        self.print_results_summary(results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def predict_games_played(self, initial_amount, avg_bet, cluster, weekday=1, weekend=0, month=1, model_name='best'):\n",
    "        \"\"\"\n",
    "        Predice el n√∫mero de juegos que se jugar√°n para una sesi√≥n espec√≠fica\n",
    "        \"\"\"\n",
    "        # Generar predicciones de los modelos individuales\n",
    "        predictions = self.predict_session(initial_amount, avg_bet, cluster, weekday, weekend, month)\n",
    "        \n",
    "        if 'error' in predictions:\n",
    "            return predictions\n",
    "        \n",
    "        # Crear feature vector para ensemble\n",
    "        feature_vector = pd.DataFrame({\n",
    "            'time_on_device': [predictions['time_on_device']],\n",
    "            'bet_total': [predictions['bet_total']],\n",
    "            'win_total': [predictions['win_total']],\n",
    "            'initial_amount': [initial_amount],\n",
    "            'cluster': [cluster],\n",
    "            'avg_bet': [avg_bet]\n",
    "        })\n",
    "        \n",
    "        # Seleccionar modelo\n",
    "        if model_name == 'best':\n",
    "            # Seleccionar el modelo con mejor R¬≤ (por defecto RandomForest)\n",
    "            model_name = 'RandomForest'\n",
    "        \n",
    "        if model_name not in self.trained_regressors:\n",
    "            return {\"error\": f\"Modelo {model_name} no encontrado\"}\n",
    "        \n",
    "        model = self.trained_regressors[model_name]\n",
    "        \n",
    "        # Hacer predicci√≥n\n",
    "        if model_name in ['MLP', 'RNN']:\n",
    "            # MLP y RNN usan escalado\n",
    "            feature_vector_scaled = self.ensemble_scaler.transform(feature_vector)\n",
    "            feature_vector_scaled = pd.DataFrame(feature_vector_scaled, columns=feature_vector.columns)\n",
    "            \n",
    "            if model_name == 'RNN':\n",
    "                # RNN necesita formato 3D\n",
    "                feature_vector_rnn = feature_vector_scaled.values.reshape((1, feature_vector_scaled.shape[1], 1))\n",
    "                games_pred_scaled = model.predict(feature_vector_rnn, verbose=0)[0][0]\n",
    "                games_pred = self.target_scaler.inverse_transform([[games_pred_scaled]])[0][0]\n",
    "            else:\n",
    "                games_pred = model.predict(feature_vector_scaled)[0]\n",
    "        else:\n",
    "            # RandomForest y XGBoost no necesitan escalado\n",
    "            games_pred = model.predict(feature_vector)[0]\n",
    "        \n",
    "        # Asegurar que la predicci√≥n sea positiva\n",
    "        games_pred = max(0, games_pred)\n",
    "        \n",
    "        result = {\n",
    "            'games_played_prediction': float(games_pred),\n",
    "            'model_used': model_name,\n",
    "            'individual_predictions': {\n",
    "                'time_on_device': predictions['time_on_device'],\n",
    "                'bet_total': predictions['bet_total'],\n",
    "                'win_total': predictions['win_total']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def print_results_summary(self, results):\n",
    "        \"\"\"Imprime un resumen comparativo de todos los modelos\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üìä RESUMEN COMPARATIVO DE MODELOS DE REGRESI√ìN\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        summary_data = []\n",
    "        for model in results.keys():\n",
    "            summary_data.append({\n",
    "                'Modelo': model,\n",
    "                'RMSE': results[model]['rmse'],\n",
    "                'MAE': results[model]['mae'],\n",
    "                'R¬≤': results[model]['r2_score'],\n",
    "                'MSE': results[model]['mse']\n",
    "            })\n",
    "        \n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        summary_df = summary_df.sort_values('R¬≤', ascending=False)\n",
    "        \n",
    "        print(summary_df.round(4).to_string(index=False))\n",
    "        \n",
    "        best_model = summary_df.iloc[0]['Modelo']\n",
    "        print(f\"\\nüèÜ Mejor modelo: {best_model}\")\n",
    "        print(f\"   R¬≤: {summary_df.iloc[0]['R¬≤']:.4f}\")\n",
    "        print(f\"   RMSE: {summary_df.iloc[0]['RMSE']:.4f}\")\n",
    "        print(f\"   MAE: {summary_df.iloc[0]['MAE']:.4f}\")\n",
    "        \n",
    "        print(f\"\\nüìà Interpretaci√≥n de R¬≤:\")\n",
    "        best_r2 = summary_df.iloc[0]['R¬≤']\n",
    "        if best_r2 >= 0.9:\n",
    "            print(\"   ‚úÖ Excelente ajuste (R¬≤ ‚â• 0.9)\")\n",
    "        elif best_r2 >= 0.7:\n",
    "            print(\"   ‚úÖ Buen ajuste (0.7 ‚â§ R¬≤ < 0.9)\")\n",
    "        elif best_r2 >= 0.5:\n",
    "            print(\"   ‚ö†Ô∏è Ajuste moderado (0.5 ‚â§ R¬≤ < 0.7)\")\n",
    "        else:\n",
    "            print(\"   ‚ùå Ajuste pobre (R¬≤ < 0.5)\")\n",
    "    \n",
    "    def save_ensemble_models(self, save_path):\n",
    "        \"\"\"Guarda los modelos entrenados\"\"\"\n",
    "        import os\n",
    "        \n",
    "        # Crear directorio si no existe\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        \n",
    "        for model_name, model in self.trained_regressors.items():\n",
    "            if model_name == 'RNN':\n",
    "                # Guardar modelo RNN (Keras)\n",
    "                model_path = f\"{save_path}/ensemble_{model_name.lower()}_model.h5\"\n",
    "                model.save(model_path)\n",
    "                print(f\"‚úÖ Modelo {model_name} guardado en: {model_path}\")\n",
    "            else:\n",
    "                # Guardar otros modelos (scikit-learn/XGBoost)\n",
    "                model_path = f\"{save_path}/ensemble_{model_name.lower()}_model.pkl\"\n",
    "                joblib.dump(model, model_path)\n",
    "                print(f\"‚úÖ Modelo {model_name} guardado en: {model_path}\")\n",
    "        \n",
    "        # Guardar scalers\n",
    "        scaler_path = f\"{save_path}/ensemble_scaler.pkl\"\n",
    "        joblib.dump(self.ensemble_scaler, scaler_path)\n",
    "        print(f\"‚úÖ Ensemble scaler guardado en: {scaler_path}\")\n",
    "        \n",
    "        target_scaler_path = f\"{save_path}/target_scaler.pkl\"\n",
    "        joblib.dump(self.target_scaler, target_scaler_path)\n",
    "        print(f\"‚úÖ Target scaler guardado en: {target_scaler_path}\")\n",
    "    \n",
    "    def load_ensemble_models(self, load_path):\n",
    "        \"\"\"Carga los modelos ensemble previamente entrenados\"\"\"\n",
    "        try:\n",
    "            import tensorflow as tf\n",
    "            \n",
    "            # Cargar modelos ensemble\n",
    "            model_files = {\n",
    "                'MLP': f\"{load_path}/ensemble_mlp_model.pkl\",\n",
    "                'RandomForest': f\"{load_path}/ensemble_randomforest_model.pkl\", \n",
    "                'XGBoost': f\"{load_path}/ensemble_xgboost_model.pkl\",\n",
    "                'RNN': f\"{load_path}/ensemble_rnn_model.h5\"\n",
    "            }\n",
    "            \n",
    "            for model_name, model_path in model_files.items():\n",
    "                try:\n",
    "                    if model_name == 'RNN':\n",
    "                        self.trained_regressors[model_name] = tf.keras.models.load_model(model_path)\n",
    "                    else:\n",
    "                        self.trained_regressors[model_name] = joblib.load(model_path)\n",
    "                    print(f\"‚úÖ Modelo {model_name} cargado desde: {model_path}\")\n",
    "                except FileNotFoundError:\n",
    "                    print(f\"‚ö†Ô∏è Modelo {model_name} no encontrado en: {model_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error cargando {model_name}: {e}\")\n",
    "            \n",
    "            # Cargar scalers\n",
    "            try:\n",
    "                scaler_path = f\"{load_path}/ensemble_scaler.pkl\"\n",
    "                self.ensemble_scaler = joblib.load(scaler_path)\n",
    "                print(f\"‚úÖ Ensemble scaler cargado desde: {scaler_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error cargando ensemble scaler: {e}\")\n",
    "            \n",
    "            try:\n",
    "                target_scaler_path = f\"{load_path}/target_scaler.pkl\"\n",
    "                self.target_scaler = joblib.load(target_scaler_path)\n",
    "                print(f\"‚úÖ Target scaler cargado desde: {target_scaler_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error cargando target scaler: {e}\")\n",
    "            \n",
    "            print(f\"üéØ Modelos ensemble cargados: {list(self.trained_regressors.keys())}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error cargando modelos ensemble: {e}\")\n",
    "            raise\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCIONES DE USO Y EJEMPLOS\n",
    "# ============================================================================\n",
    "\n",
    "def main_train_ensemble_sampled():\n",
    "    \"\"\"\n",
    "    Funci√≥n principal para entrenar el ensemble con muestreo (RECOMENDADO)\n",
    "    \"\"\"\n",
    "    # Configurar rutas de modelos (ajusta seg√∫n tus rutas)\n",
    "    model_paths = {\n",
    "        'tiempo_model': 'path/to/tiempo_model.h5',\n",
    "        'bet_model': 'path/to/bet_model.h5', \n",
    "        'win_model': 'path/to/win_model.h5',\n",
    "        'tiempo_scaler': 'path/to/tiempo_scaler.pkl',\n",
    "        'bet_scaler': 'path/to/bet_scaler.pkl',\n",
    "        'win_scaler': 'path/to/win_scaler.pkl'\n",
    "    }\n",
    "    \n",
    "    # Cargar tus datos (ajusta seg√∫n tu dataset)\n",
    "    # data = pd.read_csv('path/to/your/data.csv')\n",
    "    \n",
    "    # Inicializar modelo ensemble\n",
    "    ensemble_model = CasinoEnsembleRegressionOptimized(model_paths)\n",
    "    \n",
    "    # Entrenar con muestreo estratificado (10K sesiones)\n",
    "    print(\"üéØ Entrenamiento con muestreo para regresi√≥n...\")\n",
    "    results = ensemble_model.train_ensemble_models_sampled(\n",
    "        data=data,  # Tu DataFrame\n",
    "        sample_size=10000,  # Usar 10K sesiones en lugar de 200K+\n",
    "        target_col='GAMES_PLAYED_TOTAL',\n",
    "        test_size=0.2\n",
    "    )\n",
    "    \n",
    "    # Guardar modelos\n",
    "    ensemble_model.save_ensemble_models('models/ensemble_regression/')\n",
    "    \n",
    "    return ensemble_model, results\n",
    "\n",
    "def main_train_ensemble_full():\n",
    "    \"\"\"\n",
    "    Funci√≥n principal para entrenar con dataset completo (OPTIMIZADO)\n",
    "    \"\"\"\n",
    "    # Configurar rutas de modelos\n",
    "    model_paths = {\n",
    "        'tiempo_model': 'path/to/tiempo_model.h5',\n",
    "        'bet_model': 'path/to/bet_model.h5', \n",
    "        'win_model': 'path/to/win_model.h5',\n",
    "        'tiempo_scaler': 'path/to/tiempo_scaler.pkl',\n",
    "        'bet_scaler': 'path/to/bet_scaler.pkl',\n",
    "        'win_scaler': 'path/to/win_scaler.pkl'\n",
    "    }\n",
    "    \n",
    "    # Cargar datos\n",
    "    # data = pd.read_csv('path/to/your/data.csv')\n",
    "    \n",
    "    # Inicializar modelo ensemble\n",
    "    ensemble_model = CasinoEnsembleRegressionOptimized(model_paths)\n",
    "    \n",
    "    # Entrenar con dataset completo (procesamiento batch optimizado)\n",
    "    print(\"üöÄ Entrenamiento con dataset completo (procesamiento batch)...\")\n",
    "    results = ensemble_model.train_ensemble_models(\n",
    "        data=data,\n",
    "        target_col='GAMES_PLAYED_TOTAL',\n",
    "        test_size=0.2\n",
    "    )\n",
    "    \n",
    "    # Guardar modelos\n",
    "    ensemble_model.save_ensemble_models('models/ensemble_regression/')\n",
    "    \n",
    "    return ensemble_model, results\n",
    "\n",
    "def example_predictions():\n",
    "    \"\"\"\n",
    "    Ejemplo de c√≥mo hacer predicciones de regresi√≥n\n",
    "    \"\"\"\n",
    "    # Cargar modelo entrenado\n",
    "    model_paths = {\n",
    "        'tiempo_model': 'path/to/tiempo_model.h5',\n",
    "        'bet_model': 'path/to/bet_model.h5', \n",
    "        'win_model': 'path/to/win_model.h5',\n",
    "        'tiempo_scaler': 'path/to/tiempo_scaler.pkl',\n",
    "        'bet_scaler': 'path/to/bet_scaler.pkl',\n",
    "        'win_scaler': 'path/to/win_scaler.pkl'\n",
    "    }\n",
    "    \n",
    "    ensemble_model = CasinoEnsembleRegressionOptimized(model_paths)\n",
    "    \n",
    "    # Cargar modelos ensemble pre-entrenados\n",
    "    ensemble_model.load_ensemble_models('models/ensemble_regression/')\n",
    "    \n",
    "    # Hacer predicciones individuales\n",
    "    examples = [\n",
    "        (100, 5, 0),    # $100 inicial, $5 apuesta promedio, cluster 0\n",
    "        (500, 20, 1),   # $500 inicial, $20 apuesta promedio, cluster 1  \n",
    "        (50, 2, 2),     # $50 inicial, $2 apuesta promedio, cluster 2\n",
    "    ]\n",
    "    \n",
    "    print(\"üéÆ Ejemplos de predicciones de GAMES_PLAYED_TOTAL:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for initial, avg_bet, cluster in examples:\n",
    "        for model_name in ['MLP', 'RandomForest', 'XGBoost', 'RNN']:\n",
    "            result = ensemble_model.predict_games_played(\n",
    "                initial_amount=initial,\n",
    "                avg_bet=avg_bet,\n",
    "                cluster=cluster,\n",
    "                model_name=model_name\n",
    "            )\n",
    "            \n",
    "            if 'error' not in result:\n",
    "                print(f\"\\nüí∞ Sesi√≥n: ${initial} inicial, ${avg_bet} apuesta promedio, cluster {cluster}\")\n",
    "                print(f\"   üéØ Juegos predichos ({model_name}): {result['games_played_prediction']:.1f}\")\n",
    "                print(f\"   üìà Predicciones individuales:\")\n",
    "                print(f\"       Tiempo en dispositivo: {result['individual_predictions']['time_on_device']:.1f} min\")\n",
    "                print(f\"       Apuesta total: ${result['individual_predictions']['bet_total']:.2f}\")\n",
    "                print(f\"       Ganancia total: ${result['individual_predictions']['win_total']:.2f}\")\n",
    "\n",
    "def analyze_model_performance(results):\n",
    "    \"\"\"\n",
    "    An√°lisis detallado del rendimiento de los modelos\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üîç AN√ÅLISIS DETALLADO DEL RENDIMIENTO\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for model_name, metrics in results.items():\n",
    "        print(f\"\\nüìä {model_name}:\")\n",
    "        print(f\"   RMSE: {metrics['rmse']:.4f}\")\n",
    "        print(f\"   MAE: {metrics['mae']:.4f}\")  \n",
    "        print(f\"   R¬≤: {metrics['r2_score']:.4f}\")\n",
    "        print(f\"   MSE: {metrics['mse']:.4f}\")\n",
    "        \n",
    "        # An√°lisis espec√≠fico por modelo\n",
    "        if 'feature_importance' in metrics:\n",
    "            print(f\"   Top 3 features importantes:\")\n",
    "            top_features = metrics['feature_importance'].head(3)\n",
    "            for _, row in top_features.iterrows():\n",
    "                print(f\"     - {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Descomentar la opci√≥n que prefieras:\n",
    "    \n",
    "    # OPCI√ìN 1: Entrenamiento r√°pido con muestra\n",
    "    # ensemble_model, results = main_train_ensemble_sampled()\n",
    "    \n",
    "    # OPCI√ìN 2: Entrenamiento completo optimizado  \n",
    "    # ensemble_model, results = main_train_ensemble_full()\n",
    "    \n",
    "    # OPCI√ìN 3: Hacer predicciones (requiere modelos entrenados)\n",
    "    # example_predictions()\n",
    "    \n",
    "    # OPCI√ìN 4: An√°lisis de rendimiento\n",
    "    # analyze_model_performance(results)\n",
    "    \n",
    "    print(\"üéÆ Ensemble Regression Model listo para predecir GAMES_PLAYED_TOTAL!\")\n",
    "    print(\"üìñ Lee las instrucciones en los comentarios para comenzar.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4f50181",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 11:31:51.559743: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Todos los modelos y scalers cargados correctamente\n",
      "üöÄ Entrenamiento con dataset completo (procesamiento batch)...\n",
      "üöÄ Iniciando entrenamiento de modelos ensemble para regresi√≥n...\n",
      "üéØ Target: Predicci√≥n de GAMES_PLAYED_TOTAL\n",
      "üìù Las features time_on_device, bet_total, win_total se generar√°n desde los modelos pre-entrenados\n",
      "üìã Columnas disponibles: ['Unnamed: 0.1', 'Unnamed: 0', 'PLAYER_ID', 'DOB', 'GENDER', 'AVG_BET', 'BET_TOTAL', 'INITIAL_AMOUNT', 'INITIAL_TIME', 'FINAL_TIME', 'INITIAL_PROMO_AMOUNT', 'FINAL_AMOUNT', 'FINAL_PROMO_AMOUNT', 'MACHINE_ID', 'WIN_TOTAL', 'GAMES_PLAYED_TOTAL', 'GAMES_WON_TOTAL', 'TIME_ON_DEVICE_SEC', 'PLAYER_LEVEL_ID', 'Casino', 'AVG_BET_std', 'BET_TOTAL_std', 'INITIAL_AMOUNT_std', 'INITIAL_PROMO_AMOUNT_std', 'FINAL_AMOUNT_std', 'FINAL_PROMO_AMOUNT_std', 'WIN_TOTAL_std', 'GAMES_PLAYED_TOTAL_std', 'GAMES_WON_TOTAL_std', 'TIME_ON_DEVICE_SEC_std', 'Edad', 'Rango_Edad', 'Rango_Edad_le', 'Cluster', 'Weekday', 'number_of_day', 'TIME_ON_DEVICE_MIN', 'Hour', 'Weekend', 'Month', 'NET_SPEND']\n",
      "üìä Dataset shape: (209079, 41)\n",
      "üîÑ Generando predicciones usando modelos individuales (procesamiento batch)...\n",
      "üîÑ Procesando 209079 sesiones en 210 lotes de 1000...\n",
      "üì¶ Procesando lote 1/210 (1000 sesiones)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 11:31:53.707776: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Lote 1 completado. Progreso: 0.5%\n",
      "üì¶ Procesando lote 2/210 (1000 sesiones)...\n",
      "‚úÖ Lote 2 completado. Progreso: 1.0%\n",
      "üì¶ Procesando lote 3/210 (1000 sesiones)...\n",
      "‚úÖ Lote 3 completado. Progreso: 1.4%\n",
      "üì¶ Procesando lote 4/210 (1000 sesiones)...\n",
      "‚úÖ Lote 4 completado. Progreso: 1.9%\n",
      "üì¶ Procesando lote 5/210 (1000 sesiones)...\n",
      "‚úÖ Lote 5 completado. Progreso: 2.4%\n",
      "üì¶ Procesando lote 6/210 (1000 sesiones)...\n",
      "‚úÖ Lote 6 completado. Progreso: 2.9%\n",
      "üì¶ Procesando lote 7/210 (1000 sesiones)...\n",
      "‚úÖ Lote 7 completado. Progreso: 3.3%\n",
      "üì¶ Procesando lote 8/210 (1000 sesiones)...\n",
      "‚úÖ Lote 8 completado. Progreso: 3.8%\n",
      "üì¶ Procesando lote 9/210 (1000 sesiones)...\n",
      "‚úÖ Lote 9 completado. Progreso: 4.3%\n",
      "üì¶ Procesando lote 10/210 (1000 sesiones)...\n",
      "‚úÖ Lote 10 completado. Progreso: 4.8%\n",
      "üì¶ Procesando lote 11/210 (1000 sesiones)...\n",
      "‚úÖ Lote 11 completado. Progreso: 5.3%\n",
      "üì¶ Procesando lote 12/210 (1000 sesiones)...\n",
      "‚úÖ Lote 12 completado. Progreso: 5.7%\n",
      "üì¶ Procesando lote 13/210 (1000 sesiones)...\n",
      "‚úÖ Lote 13 completado. Progreso: 6.2%\n",
      "üì¶ Procesando lote 14/210 (1000 sesiones)...\n",
      "‚úÖ Lote 14 completado. Progreso: 6.7%\n",
      "üì¶ Procesando lote 15/210 (1000 sesiones)...\n",
      "‚úÖ Lote 15 completado. Progreso: 7.2%\n",
      "üì¶ Procesando lote 16/210 (1000 sesiones)...\n",
      "‚úÖ Lote 16 completado. Progreso: 7.7%\n",
      "üì¶ Procesando lote 17/210 (1000 sesiones)...\n",
      "‚úÖ Lote 17 completado. Progreso: 8.1%\n",
      "üì¶ Procesando lote 18/210 (1000 sesiones)...\n",
      "‚úÖ Lote 18 completado. Progreso: 8.6%\n",
      "üì¶ Procesando lote 19/210 (1000 sesiones)...\n",
      "‚úÖ Lote 19 completado. Progreso: 9.1%\n",
      "üì¶ Procesando lote 20/210 (1000 sesiones)...\n",
      "‚úÖ Lote 20 completado. Progreso: 9.6%\n",
      "üì¶ Procesando lote 21/210 (1000 sesiones)...\n",
      "‚úÖ Lote 21 completado. Progreso: 10.0%\n",
      "üì¶ Procesando lote 22/210 (1000 sesiones)...\n",
      "‚úÖ Lote 22 completado. Progreso: 10.5%\n",
      "üì¶ Procesando lote 23/210 (1000 sesiones)...\n",
      "‚úÖ Lote 23 completado. Progreso: 11.0%\n",
      "üì¶ Procesando lote 24/210 (1000 sesiones)...\n",
      "‚úÖ Lote 24 completado. Progreso: 11.5%\n",
      "üì¶ Procesando lote 25/210 (1000 sesiones)...\n",
      "‚úÖ Lote 25 completado. Progreso: 12.0%\n",
      "üì¶ Procesando lote 26/210 (1000 sesiones)...\n",
      "‚úÖ Lote 26 completado. Progreso: 12.4%\n",
      "üì¶ Procesando lote 27/210 (1000 sesiones)...\n",
      "‚úÖ Lote 27 completado. Progreso: 12.9%\n",
      "üì¶ Procesando lote 28/210 (1000 sesiones)...\n",
      "‚úÖ Lote 28 completado. Progreso: 13.4%\n",
      "üì¶ Procesando lote 29/210 (1000 sesiones)...\n",
      "‚úÖ Lote 29 completado. Progreso: 13.9%\n",
      "üì¶ Procesando lote 30/210 (1000 sesiones)...\n",
      "‚úÖ Lote 30 completado. Progreso: 14.3%\n",
      "üì¶ Procesando lote 31/210 (1000 sesiones)...\n",
      "‚úÖ Lote 31 completado. Progreso: 14.8%\n",
      "üì¶ Procesando lote 32/210 (1000 sesiones)...\n",
      "‚úÖ Lote 32 completado. Progreso: 15.3%\n",
      "üì¶ Procesando lote 33/210 (1000 sesiones)...\n",
      "‚úÖ Lote 33 completado. Progreso: 15.8%\n",
      "üì¶ Procesando lote 34/210 (1000 sesiones)...\n",
      "‚úÖ Lote 34 completado. Progreso: 16.3%\n",
      "üì¶ Procesando lote 35/210 (1000 sesiones)...\n",
      "‚úÖ Lote 35 completado. Progreso: 16.7%\n",
      "üì¶ Procesando lote 36/210 (1000 sesiones)...\n",
      "‚úÖ Lote 36 completado. Progreso: 17.2%\n",
      "üì¶ Procesando lote 37/210 (1000 sesiones)...\n",
      "‚úÖ Lote 37 completado. Progreso: 17.7%\n",
      "üì¶ Procesando lote 38/210 (1000 sesiones)...\n",
      "‚úÖ Lote 38 completado. Progreso: 18.2%\n",
      "üì¶ Procesando lote 39/210 (1000 sesiones)...\n",
      "‚úÖ Lote 39 completado. Progreso: 18.7%\n",
      "üì¶ Procesando lote 40/210 (1000 sesiones)...\n",
      "‚úÖ Lote 40 completado. Progreso: 19.1%\n",
      "üì¶ Procesando lote 41/210 (1000 sesiones)...\n",
      "‚úÖ Lote 41 completado. Progreso: 19.6%\n",
      "üì¶ Procesando lote 42/210 (1000 sesiones)...\n",
      "‚úÖ Lote 42 completado. Progreso: 20.1%\n",
      "üì¶ Procesando lote 43/210 (1000 sesiones)...\n",
      "‚úÖ Lote 43 completado. Progreso: 20.6%\n",
      "üì¶ Procesando lote 44/210 (1000 sesiones)...\n",
      "‚úÖ Lote 44 completado. Progreso: 21.0%\n",
      "üì¶ Procesando lote 45/210 (1000 sesiones)...\n",
      "‚úÖ Lote 45 completado. Progreso: 21.5%\n",
      "üì¶ Procesando lote 46/210 (1000 sesiones)...\n",
      "‚úÖ Lote 46 completado. Progreso: 22.0%\n",
      "üì¶ Procesando lote 47/210 (1000 sesiones)...\n",
      "‚úÖ Lote 47 completado. Progreso: 22.5%\n",
      "üì¶ Procesando lote 48/210 (1000 sesiones)...\n",
      "‚úÖ Lote 48 completado. Progreso: 23.0%\n",
      "üì¶ Procesando lote 49/210 (1000 sesiones)...\n",
      "‚úÖ Lote 49 completado. Progreso: 23.4%\n",
      "üì¶ Procesando lote 50/210 (1000 sesiones)...\n",
      "‚úÖ Lote 50 completado. Progreso: 23.9%\n",
      "üì¶ Procesando lote 51/210 (1000 sesiones)...\n",
      "‚úÖ Lote 51 completado. Progreso: 24.4%\n",
      "üì¶ Procesando lote 52/210 (1000 sesiones)...\n",
      "‚úÖ Lote 52 completado. Progreso: 24.9%\n",
      "üì¶ Procesando lote 53/210 (1000 sesiones)...\n",
      "‚úÖ Lote 53 completado. Progreso: 25.3%\n",
      "üì¶ Procesando lote 54/210 (1000 sesiones)...\n",
      "‚úÖ Lote 54 completado. Progreso: 25.8%\n",
      "üì¶ Procesando lote 55/210 (1000 sesiones)...\n",
      "‚úÖ Lote 55 completado. Progreso: 26.3%\n",
      "üì¶ Procesando lote 56/210 (1000 sesiones)...\n",
      "‚úÖ Lote 56 completado. Progreso: 26.8%\n",
      "üì¶ Procesando lote 57/210 (1000 sesiones)...\n",
      "‚úÖ Lote 57 completado. Progreso: 27.3%\n",
      "üì¶ Procesando lote 58/210 (1000 sesiones)...\n",
      "‚úÖ Lote 58 completado. Progreso: 27.7%\n",
      "üì¶ Procesando lote 59/210 (1000 sesiones)...\n",
      "‚úÖ Lote 59 completado. Progreso: 28.2%\n",
      "üì¶ Procesando lote 60/210 (1000 sesiones)...\n",
      "‚úÖ Lote 60 completado. Progreso: 28.7%\n",
      "üì¶ Procesando lote 61/210 (1000 sesiones)...\n",
      "‚úÖ Lote 61 completado. Progreso: 29.2%\n",
      "üì¶ Procesando lote 62/210 (1000 sesiones)...\n",
      "‚úÖ Lote 62 completado. Progreso: 29.7%\n",
      "üì¶ Procesando lote 63/210 (1000 sesiones)...\n",
      "‚úÖ Lote 63 completado. Progreso: 30.1%\n",
      "üì¶ Procesando lote 64/210 (1000 sesiones)...\n",
      "‚úÖ Lote 64 completado. Progreso: 30.6%\n",
      "üì¶ Procesando lote 65/210 (1000 sesiones)...\n",
      "‚úÖ Lote 65 completado. Progreso: 31.1%\n",
      "üì¶ Procesando lote 66/210 (1000 sesiones)...\n",
      "‚úÖ Lote 66 completado. Progreso: 31.6%\n",
      "üì¶ Procesando lote 67/210 (1000 sesiones)...\n",
      "‚úÖ Lote 67 completado. Progreso: 32.0%\n",
      "üì¶ Procesando lote 68/210 (1000 sesiones)...\n",
      "‚úÖ Lote 68 completado. Progreso: 32.5%\n",
      "üì¶ Procesando lote 69/210 (1000 sesiones)...\n",
      "‚úÖ Lote 69 completado. Progreso: 33.0%\n",
      "üì¶ Procesando lote 70/210 (1000 sesiones)...\n",
      "‚úÖ Lote 70 completado. Progreso: 33.5%\n",
      "üì¶ Procesando lote 71/210 (1000 sesiones)...\n",
      "‚úÖ Lote 71 completado. Progreso: 34.0%\n",
      "üì¶ Procesando lote 72/210 (1000 sesiones)...\n",
      "‚úÖ Lote 72 completado. Progreso: 34.4%\n",
      "üì¶ Procesando lote 73/210 (1000 sesiones)...\n",
      "‚úÖ Lote 73 completado. Progreso: 34.9%\n",
      "üì¶ Procesando lote 74/210 (1000 sesiones)...\n",
      "‚úÖ Lote 74 completado. Progreso: 35.4%\n",
      "üì¶ Procesando lote 75/210 (1000 sesiones)...\n",
      "‚úÖ Lote 75 completado. Progreso: 35.9%\n",
      "üì¶ Procesando lote 76/210 (1000 sesiones)...\n",
      "‚úÖ Lote 76 completado. Progreso: 36.3%\n",
      "üì¶ Procesando lote 77/210 (1000 sesiones)...\n",
      "‚úÖ Lote 77 completado. Progreso: 36.8%\n",
      "üì¶ Procesando lote 78/210 (1000 sesiones)...\n",
      "‚úÖ Lote 78 completado. Progreso: 37.3%\n",
      "üì¶ Procesando lote 79/210 (1000 sesiones)...\n",
      "‚úÖ Lote 79 completado. Progreso: 37.8%\n",
      "üì¶ Procesando lote 80/210 (1000 sesiones)...\n",
      "‚úÖ Lote 80 completado. Progreso: 38.3%\n",
      "üì¶ Procesando lote 81/210 (1000 sesiones)...\n",
      "‚úÖ Lote 81 completado. Progreso: 38.7%\n",
      "üì¶ Procesando lote 82/210 (1000 sesiones)...\n",
      "‚úÖ Lote 82 completado. Progreso: 39.2%\n",
      "üì¶ Procesando lote 83/210 (1000 sesiones)...\n",
      "‚úÖ Lote 83 completado. Progreso: 39.7%\n",
      "üì¶ Procesando lote 84/210 (1000 sesiones)...\n",
      "‚úÖ Lote 84 completado. Progreso: 40.2%\n",
      "üì¶ Procesando lote 85/210 (1000 sesiones)...\n",
      "‚úÖ Lote 85 completado. Progreso: 40.7%\n",
      "üì¶ Procesando lote 86/210 (1000 sesiones)...\n",
      "‚úÖ Lote 86 completado. Progreso: 41.1%\n",
      "üì¶ Procesando lote 87/210 (1000 sesiones)...\n",
      "‚úÖ Lote 87 completado. Progreso: 41.6%\n",
      "üì¶ Procesando lote 88/210 (1000 sesiones)...\n",
      "‚úÖ Lote 88 completado. Progreso: 42.1%\n",
      "üì¶ Procesando lote 89/210 (1000 sesiones)...\n",
      "‚úÖ Lote 89 completado. Progreso: 42.6%\n",
      "üì¶ Procesando lote 90/210 (1000 sesiones)...\n",
      "‚úÖ Lote 90 completado. Progreso: 43.0%\n",
      "üì¶ Procesando lote 91/210 (1000 sesiones)...\n",
      "‚úÖ Lote 91 completado. Progreso: 43.5%\n",
      "üì¶ Procesando lote 92/210 (1000 sesiones)...\n",
      "‚úÖ Lote 92 completado. Progreso: 44.0%\n",
      "üì¶ Procesando lote 93/210 (1000 sesiones)...\n",
      "‚úÖ Lote 93 completado. Progreso: 44.5%\n",
      "üì¶ Procesando lote 94/210 (1000 sesiones)...\n",
      "‚úÖ Lote 94 completado. Progreso: 45.0%\n",
      "üì¶ Procesando lote 95/210 (1000 sesiones)...\n",
      "‚úÖ Lote 95 completado. Progreso: 45.4%\n",
      "üì¶ Procesando lote 96/210 (1000 sesiones)...\n",
      "‚úÖ Lote 96 completado. Progreso: 45.9%\n",
      "üì¶ Procesando lote 97/210 (1000 sesiones)...\n",
      "‚úÖ Lote 97 completado. Progreso: 46.4%\n",
      "üì¶ Procesando lote 98/210 (1000 sesiones)...\n",
      "‚úÖ Lote 98 completado. Progreso: 46.9%\n",
      "üì¶ Procesando lote 99/210 (1000 sesiones)...\n",
      "‚úÖ Lote 99 completado. Progreso: 47.4%\n",
      "üì¶ Procesando lote 100/210 (1000 sesiones)...\n",
      "‚úÖ Lote 100 completado. Progreso: 47.8%\n",
      "üì¶ Procesando lote 101/210 (1000 sesiones)...\n",
      "‚úÖ Lote 101 completado. Progreso: 48.3%\n",
      "üì¶ Procesando lote 102/210 (1000 sesiones)...\n",
      "‚úÖ Lote 102 completado. Progreso: 48.8%\n",
      "üì¶ Procesando lote 103/210 (1000 sesiones)...\n",
      "‚úÖ Lote 103 completado. Progreso: 49.3%\n",
      "üì¶ Procesando lote 104/210 (1000 sesiones)...\n",
      "‚úÖ Lote 104 completado. Progreso: 49.7%\n",
      "üì¶ Procesando lote 105/210 (1000 sesiones)...\n",
      "‚úÖ Lote 105 completado. Progreso: 50.2%\n",
      "üì¶ Procesando lote 106/210 (1000 sesiones)...\n",
      "‚úÖ Lote 106 completado. Progreso: 50.7%\n",
      "üì¶ Procesando lote 107/210 (1000 sesiones)...\n",
      "‚úÖ Lote 107 completado. Progreso: 51.2%\n",
      "üì¶ Procesando lote 108/210 (1000 sesiones)...\n",
      "‚úÖ Lote 108 completado. Progreso: 51.7%\n",
      "üì¶ Procesando lote 109/210 (1000 sesiones)...\n",
      "‚úÖ Lote 109 completado. Progreso: 52.1%\n",
      "üì¶ Procesando lote 110/210 (1000 sesiones)...\n",
      "‚úÖ Lote 110 completado. Progreso: 52.6%\n",
      "üì¶ Procesando lote 111/210 (1000 sesiones)...\n",
      "‚úÖ Lote 111 completado. Progreso: 53.1%\n",
      "üì¶ Procesando lote 112/210 (1000 sesiones)...\n",
      "‚úÖ Lote 112 completado. Progreso: 53.6%\n",
      "üì¶ Procesando lote 113/210 (1000 sesiones)...\n",
      "‚úÖ Lote 113 completado. Progreso: 54.0%\n",
      "üì¶ Procesando lote 114/210 (1000 sesiones)...\n",
      "‚úÖ Lote 114 completado. Progreso: 54.5%\n",
      "üì¶ Procesando lote 115/210 (1000 sesiones)...\n",
      "‚úÖ Lote 115 completado. Progreso: 55.0%\n",
      "üì¶ Procesando lote 116/210 (1000 sesiones)...\n",
      "‚úÖ Lote 116 completado. Progreso: 55.5%\n",
      "üì¶ Procesando lote 117/210 (1000 sesiones)...\n",
      "‚úÖ Lote 117 completado. Progreso: 56.0%\n",
      "üì¶ Procesando lote 118/210 (1000 sesiones)...\n",
      "‚úÖ Lote 118 completado. Progreso: 56.4%\n",
      "üì¶ Procesando lote 119/210 (1000 sesiones)...\n",
      "‚úÖ Lote 119 completado. Progreso: 56.9%\n",
      "üì¶ Procesando lote 120/210 (1000 sesiones)...\n",
      "‚úÖ Lote 120 completado. Progreso: 57.4%\n",
      "üì¶ Procesando lote 121/210 (1000 sesiones)...\n",
      "‚úÖ Lote 121 completado. Progreso: 57.9%\n",
      "üì¶ Procesando lote 122/210 (1000 sesiones)...\n",
      "‚úÖ Lote 122 completado. Progreso: 58.4%\n",
      "üì¶ Procesando lote 123/210 (1000 sesiones)...\n",
      "‚úÖ Lote 123 completado. Progreso: 58.8%\n",
      "üì¶ Procesando lote 124/210 (1000 sesiones)...\n",
      "‚úÖ Lote 124 completado. Progreso: 59.3%\n",
      "üì¶ Procesando lote 125/210 (1000 sesiones)...\n",
      "‚úÖ Lote 125 completado. Progreso: 59.8%\n",
      "üì¶ Procesando lote 126/210 (1000 sesiones)...\n",
      "‚úÖ Lote 126 completado. Progreso: 60.3%\n",
      "üì¶ Procesando lote 127/210 (1000 sesiones)...\n",
      "‚úÖ Lote 127 completado. Progreso: 60.7%\n",
      "üì¶ Procesando lote 128/210 (1000 sesiones)...\n",
      "‚úÖ Lote 128 completado. Progreso: 61.2%\n",
      "üì¶ Procesando lote 129/210 (1000 sesiones)...\n",
      "‚úÖ Lote 129 completado. Progreso: 61.7%\n",
      "üì¶ Procesando lote 130/210 (1000 sesiones)...\n",
      "‚úÖ Lote 130 completado. Progreso: 62.2%\n",
      "üì¶ Procesando lote 131/210 (1000 sesiones)...\n",
      "‚úÖ Lote 131 completado. Progreso: 62.7%\n",
      "üì¶ Procesando lote 132/210 (1000 sesiones)...\n",
      "‚úÖ Lote 132 completado. Progreso: 63.1%\n",
      "üì¶ Procesando lote 133/210 (1000 sesiones)...\n",
      "‚úÖ Lote 133 completado. Progreso: 63.6%\n",
      "üì¶ Procesando lote 134/210 (1000 sesiones)...\n",
      "‚úÖ Lote 134 completado. Progreso: 64.1%\n",
      "üì¶ Procesando lote 135/210 (1000 sesiones)...\n",
      "‚úÖ Lote 135 completado. Progreso: 64.6%\n",
      "üì¶ Procesando lote 136/210 (1000 sesiones)...\n",
      "‚úÖ Lote 136 completado. Progreso: 65.0%\n",
      "üì¶ Procesando lote 137/210 (1000 sesiones)...\n",
      "‚úÖ Lote 137 completado. Progreso: 65.5%\n",
      "üì¶ Procesando lote 138/210 (1000 sesiones)...\n",
      "‚úÖ Lote 138 completado. Progreso: 66.0%\n",
      "üì¶ Procesando lote 139/210 (1000 sesiones)...\n",
      "‚úÖ Lote 139 completado. Progreso: 66.5%\n",
      "üì¶ Procesando lote 140/210 (1000 sesiones)...\n",
      "‚úÖ Lote 140 completado. Progreso: 67.0%\n",
      "üì¶ Procesando lote 141/210 (1000 sesiones)...\n",
      "‚úÖ Lote 141 completado. Progreso: 67.4%\n",
      "üì¶ Procesando lote 142/210 (1000 sesiones)...\n",
      "‚úÖ Lote 142 completado. Progreso: 67.9%\n",
      "üì¶ Procesando lote 143/210 (1000 sesiones)...\n",
      "‚úÖ Lote 143 completado. Progreso: 68.4%\n",
      "üì¶ Procesando lote 144/210 (1000 sesiones)...\n",
      "‚úÖ Lote 144 completado. Progreso: 68.9%\n",
      "üì¶ Procesando lote 145/210 (1000 sesiones)...\n",
      "‚úÖ Lote 145 completado. Progreso: 69.4%\n",
      "üì¶ Procesando lote 146/210 (1000 sesiones)...\n",
      "‚úÖ Lote 146 completado. Progreso: 69.8%\n",
      "üì¶ Procesando lote 147/210 (1000 sesiones)...\n",
      "‚úÖ Lote 147 completado. Progreso: 70.3%\n",
      "üì¶ Procesando lote 148/210 (1000 sesiones)...\n",
      "‚úÖ Lote 148 completado. Progreso: 70.8%\n",
      "üì¶ Procesando lote 149/210 (1000 sesiones)...\n",
      "‚úÖ Lote 149 completado. Progreso: 71.3%\n",
      "üì¶ Procesando lote 150/210 (1000 sesiones)...\n",
      "‚úÖ Lote 150 completado. Progreso: 71.7%\n",
      "üì¶ Procesando lote 151/210 (1000 sesiones)...\n",
      "‚úÖ Lote 151 completado. Progreso: 72.2%\n",
      "üì¶ Procesando lote 152/210 (1000 sesiones)...\n",
      "‚úÖ Lote 152 completado. Progreso: 72.7%\n",
      "üì¶ Procesando lote 153/210 (1000 sesiones)...\n",
      "‚úÖ Lote 153 completado. Progreso: 73.2%\n",
      "üì¶ Procesando lote 154/210 (1000 sesiones)...\n",
      "‚úÖ Lote 154 completado. Progreso: 73.7%\n",
      "üì¶ Procesando lote 155/210 (1000 sesiones)...\n",
      "‚úÖ Lote 155 completado. Progreso: 74.1%\n",
      "üì¶ Procesando lote 156/210 (1000 sesiones)...\n",
      "‚úÖ Lote 156 completado. Progreso: 74.6%\n",
      "üì¶ Procesando lote 157/210 (1000 sesiones)...\n",
      "‚úÖ Lote 157 completado. Progreso: 75.1%\n",
      "üì¶ Procesando lote 158/210 (1000 sesiones)...\n",
      "‚úÖ Lote 158 completado. Progreso: 75.6%\n",
      "üì¶ Procesando lote 159/210 (1000 sesiones)...\n",
      "‚úÖ Lote 159 completado. Progreso: 76.0%\n",
      "üì¶ Procesando lote 160/210 (1000 sesiones)...\n",
      "‚úÖ Lote 160 completado. Progreso: 76.5%\n",
      "üì¶ Procesando lote 161/210 (1000 sesiones)...\n",
      "‚úÖ Lote 161 completado. Progreso: 77.0%\n",
      "üì¶ Procesando lote 162/210 (1000 sesiones)...\n",
      "‚úÖ Lote 162 completado. Progreso: 77.5%\n",
      "üì¶ Procesando lote 163/210 (1000 sesiones)...\n",
      "‚úÖ Lote 163 completado. Progreso: 78.0%\n",
      "üì¶ Procesando lote 164/210 (1000 sesiones)...\n",
      "‚úÖ Lote 164 completado. Progreso: 78.4%\n",
      "üì¶ Procesando lote 165/210 (1000 sesiones)...\n",
      "‚úÖ Lote 165 completado. Progreso: 78.9%\n",
      "üì¶ Procesando lote 166/210 (1000 sesiones)...\n",
      "‚úÖ Lote 166 completado. Progreso: 79.4%\n",
      "üì¶ Procesando lote 167/210 (1000 sesiones)...\n",
      "‚úÖ Lote 167 completado. Progreso: 79.9%\n",
      "üì¶ Procesando lote 168/210 (1000 sesiones)...\n",
      "‚úÖ Lote 168 completado. Progreso: 80.4%\n",
      "üì¶ Procesando lote 169/210 (1000 sesiones)...\n",
      "‚úÖ Lote 169 completado. Progreso: 80.8%\n",
      "üì¶ Procesando lote 170/210 (1000 sesiones)...\n",
      "‚úÖ Lote 170 completado. Progreso: 81.3%\n",
      "üì¶ Procesando lote 171/210 (1000 sesiones)...\n",
      "‚úÖ Lote 171 completado. Progreso: 81.8%\n",
      "üì¶ Procesando lote 172/210 (1000 sesiones)...\n",
      "‚úÖ Lote 172 completado. Progreso: 82.3%\n",
      "üì¶ Procesando lote 173/210 (1000 sesiones)...\n",
      "‚úÖ Lote 173 completado. Progreso: 82.7%\n",
      "üì¶ Procesando lote 174/210 (1000 sesiones)...\n",
      "‚úÖ Lote 174 completado. Progreso: 83.2%\n",
      "üì¶ Procesando lote 175/210 (1000 sesiones)...\n",
      "‚úÖ Lote 175 completado. Progreso: 83.7%\n",
      "üì¶ Procesando lote 176/210 (1000 sesiones)...\n",
      "‚úÖ Lote 176 completado. Progreso: 84.2%\n",
      "üì¶ Procesando lote 177/210 (1000 sesiones)...\n",
      "‚úÖ Lote 177 completado. Progreso: 84.7%\n",
      "üì¶ Procesando lote 178/210 (1000 sesiones)...\n",
      "‚úÖ Lote 178 completado. Progreso: 85.1%\n",
      "üì¶ Procesando lote 179/210 (1000 sesiones)...\n",
      "‚úÖ Lote 179 completado. Progreso: 85.6%\n",
      "üì¶ Procesando lote 180/210 (1000 sesiones)...\n",
      "‚úÖ Lote 180 completado. Progreso: 86.1%\n",
      "üì¶ Procesando lote 181/210 (1000 sesiones)...\n",
      "‚úÖ Lote 181 completado. Progreso: 86.6%\n",
      "üì¶ Procesando lote 182/210 (1000 sesiones)...\n",
      "‚úÖ Lote 182 completado. Progreso: 87.0%\n",
      "üì¶ Procesando lote 183/210 (1000 sesiones)...\n",
      "‚úÖ Lote 183 completado. Progreso: 87.5%\n",
      "üì¶ Procesando lote 184/210 (1000 sesiones)...\n",
      "‚úÖ Lote 184 completado. Progreso: 88.0%\n",
      "üì¶ Procesando lote 185/210 (1000 sesiones)...\n",
      "‚úÖ Lote 185 completado. Progreso: 88.5%\n",
      "üì¶ Procesando lote 186/210 (1000 sesiones)...\n",
      "‚úÖ Lote 186 completado. Progreso: 89.0%\n",
      "üì¶ Procesando lote 187/210 (1000 sesiones)...\n",
      "‚úÖ Lote 187 completado. Progreso: 89.4%\n",
      "üì¶ Procesando lote 188/210 (1000 sesiones)...\n",
      "‚úÖ Lote 188 completado. Progreso: 89.9%\n",
      "üì¶ Procesando lote 189/210 (1000 sesiones)...\n",
      "‚úÖ Lote 189 completado. Progreso: 90.4%\n",
      "üì¶ Procesando lote 190/210 (1000 sesiones)...\n",
      "‚úÖ Lote 190 completado. Progreso: 90.9%\n",
      "üì¶ Procesando lote 191/210 (1000 sesiones)...\n",
      "‚úÖ Lote 191 completado. Progreso: 91.4%\n",
      "üì¶ Procesando lote 192/210 (1000 sesiones)...\n",
      "‚úÖ Lote 192 completado. Progreso: 91.8%\n",
      "üì¶ Procesando lote 193/210 (1000 sesiones)...\n",
      "‚úÖ Lote 193 completado. Progreso: 92.3%\n",
      "üì¶ Procesando lote 194/210 (1000 sesiones)...\n",
      "‚úÖ Lote 194 completado. Progreso: 92.8%\n",
      "üì¶ Procesando lote 195/210 (1000 sesiones)...\n",
      "‚úÖ Lote 195 completado. Progreso: 93.3%\n",
      "üì¶ Procesando lote 196/210 (1000 sesiones)...\n",
      "‚úÖ Lote 196 completado. Progreso: 93.7%\n",
      "üì¶ Procesando lote 197/210 (1000 sesiones)...\n",
      "‚úÖ Lote 197 completado. Progreso: 94.2%\n",
      "üì¶ Procesando lote 198/210 (1000 sesiones)...\n",
      "‚úÖ Lote 198 completado. Progreso: 94.7%\n",
      "üì¶ Procesando lote 199/210 (1000 sesiones)...\n",
      "‚úÖ Lote 199 completado. Progreso: 95.2%\n",
      "üì¶ Procesando lote 200/210 (1000 sesiones)...\n",
      "‚úÖ Lote 200 completado. Progreso: 95.7%\n",
      "üì¶ Procesando lote 201/210 (1000 sesiones)...\n",
      "‚úÖ Lote 201 completado. Progreso: 96.1%\n",
      "üì¶ Procesando lote 202/210 (1000 sesiones)...\n",
      "‚úÖ Lote 202 completado. Progreso: 96.6%\n",
      "üì¶ Procesando lote 203/210 (1000 sesiones)...\n",
      "‚úÖ Lote 203 completado. Progreso: 97.1%\n",
      "üì¶ Procesando lote 204/210 (1000 sesiones)...\n",
      "‚úÖ Lote 204 completado. Progreso: 97.6%\n",
      "üì¶ Procesando lote 205/210 (1000 sesiones)...\n",
      "‚úÖ Lote 205 completado. Progreso: 98.0%\n",
      "üì¶ Procesando lote 206/210 (1000 sesiones)...\n",
      "‚úÖ Lote 206 completado. Progreso: 98.5%\n",
      "üì¶ Procesando lote 207/210 (1000 sesiones)...\n",
      "‚úÖ Lote 207 completado. Progreso: 99.0%\n",
      "üì¶ Procesando lote 208/210 (1000 sesiones)...\n",
      "‚úÖ Lote 208 completado. Progreso: 99.5%\n",
      "üì¶ Procesando lote 209/210 (1000 sesiones)...\n",
      "‚úÖ Lote 209 completado. Progreso: 100.0%\n",
      "üì¶ Procesando lote 210/210 (79 sesiones)...\n",
      "‚úÖ Lote 210 completado. Progreso: 100.0%\n",
      "‚úÖ Features del ensemble generadas exitosamente:\n",
      "   - 209079 sesiones procesadas\n",
      "   - Variables generadas por modelos MLP: time_on_device, bet_total, win_total\n",
      "   - Variables originales: initial_amount, cluster, avg_bet\n",
      "   - Total features para regresi√≥n: 6\n",
      "\n",
      "üìä Estad√≠sticas de predicciones generadas:\n",
      "   time_on_device: 15.53 ¬± 19.71\n",
      "   bet_total: $763.59 ¬± $1496.02\n",
      "   win_total: $680.09 ¬± $1757.58\n",
      "üìä Datos preparados: 209079 muestras, 6 features\n",
      "üìà Estad√≠sticas del target:\n",
      "   Media: 210.10\n",
      "   Mediana: 101.00\n",
      "   Std: 369.09\n",
      "   Min: 0.0, Max: 66698.0\n",
      "üéØ Features del ensemble: ['time_on_device', 'bet_total', 'win_total', 'initial_amount', 'cluster', 'avg_bet']\n",
      "\n",
      "============================================================\n",
      "MLP Start\n",
      "\n",
      "üß† Entrenando modelo MLP...\n",
      "Mejores par√°metros: {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50), 'learning_rate_init': 0.001, 'max_iter': 1000}\n",
      "MSE: 28044.1417\n",
      "RMSE: 167.4639\n",
      "MAE: 102.3077\n",
      "R¬≤: 0.7563\n",
      "MLP End\n",
      "\n",
      "============================================================\n",
      "RF Start\n",
      "\n",
      "üå≥ Entrenando modelo Random Forest...\n",
      "Mejores par√°metros: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "MSE: 28665.3554\n",
      "RMSE: 169.3085\n",
      "MAE: 100.7386\n",
      "R¬≤: 0.7509\n",
      "\n",
      "Importancia de features:\n",
      "          feature  importance\n",
      "0  time_on_device        0.41\n",
      "4         cluster        0.25\n",
      "2       win_total        0.17\n",
      "1       bet_total        0.10\n",
      "5         avg_bet        0.06\n",
      "3  initial_amount        0.02\n",
      "RF End\n",
      "\n",
      "============================================================\n",
      "XGBoost Start\n",
      "\n",
      "üöÄ Entrenando modelo XGBoost...\n",
      "Mejores par√°metros: {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 300, 'subsample': 1.0}\n",
      "MSE: 28333.3495\n",
      "RMSE: 168.3251\n",
      "MAE: 103.2106\n",
      "R¬≤: 0.7538\n",
      "\n",
      "Importancia de features:\n",
      "          feature  importance\n",
      "0  time_on_device        0.51\n",
      "4         cluster        0.45\n",
      "2       win_total        0.02\n",
      "1       bet_total        0.00\n",
      "5         avg_bet        0.00\n",
      "3  initial_amount        0.00\n",
      "XGBoost End\n",
      "\n",
      "============================================================\n",
      "RNN Start\n",
      "\n",
      "üîÑ Entrenando modelo RNN/LSTM...\n",
      "MSE: 28345.1383\n",
      "RMSE: 168.3601\n",
      "MAE: 102.8920\n",
      "R¬≤: 0.7537\n",
      "√âpocas entrenadas: 18\n",
      "RNN End\n",
      "\n",
      "================================================================================\n",
      "üìä RESUMEN COMPARATIVO DE MODELOS DE REGRESI√ìN\n",
      "================================================================================\n",
      "      Modelo   RMSE    MAE   R¬≤      MSE\n",
      "         MLP 167.46 102.31 0.76 28044.14\n",
      "     XGBoost 168.33 103.21 0.75 28333.35\n",
      "         RNN 168.36 102.89 0.75 28345.14\n",
      "RandomForest 169.31 100.74 0.75 28665.36\n",
      "\n",
      "üèÜ Mejor modelo: MLP\n",
      "   R¬≤: 0.7563\n",
      "   RMSE: 167.4639\n",
      "   MAE: 102.3077\n",
      "\n",
      "üìà Interpretaci√≥n de R¬≤:\n",
      "   ‚úÖ Buen ajuste (0.7 ‚â§ R¬≤ < 0.9)\n"
     ]
    }
   ],
   "source": [
    "model_paths = {\n",
    "    'tiempo_model': '../../../models/tiempo_model.h5',\n",
    "    'tiempo_scaler': '../../../models/tiempo_scaler.pkl',\n",
    "    'bet_model': '../../../models/bet_model.h5',\n",
    "    'bet_scaler': '../../../models/bet_scaler.pkl',\n",
    "    'win_model': '../../../models/win_model.h5',\n",
    "    'win_scaler': '../../../models/win_scaler.pkl'\n",
    "}\n",
    "ensemble_model = CasinoEnsembleRegressionOptimized(model_paths)\n",
    "\n",
    "# Entrenar con dataset completo (procesamiento batch optimizado)\n",
    "print(\"üöÄ Entrenamiento con dataset completo (procesamiento batch)...\")\n",
    "results = ensemble_model.train_ensemble_models(\n",
    "    data=df_data_general,\n",
    "    target_col='GAMES_PLAYED_TOTAL',\n",
    "    test_size=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1596258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelo MLP guardado en: games_played/ensemble_mlp_model.pkl\n",
      "‚úÖ Modelo RandomForest guardado en: games_played/ensemble_randomforest_model.pkl\n",
      "‚úÖ Modelo XGBoost guardado en: games_played/ensemble_xgboost_model.pkl\n",
      "‚úÖ Modelo RNN guardado en: games_played/ensemble_rnn_model.h5\n",
      "‚úÖ Ensemble scaler guardado en: games_played/ensemble_scaler.pkl\n",
      "‚úÖ Target scaler guardado en: games_played/target_scaler.pkl\n"
     ]
    }
   ],
   "source": [
    "ensemble_model.save_ensemble_models('games_played')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
