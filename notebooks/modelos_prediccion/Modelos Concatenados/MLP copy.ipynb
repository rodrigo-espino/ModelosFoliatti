{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4af5bcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a90ab4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_general = pd.read_csv('../../../data/data_general.csv')\n",
    "\n",
    "df_data_general['INITIAL_TIME'] = pd.to_datetime(df_data_general['INITIAL_TIME'])\n",
    "df_data_general['FINAL_TIME'] = pd.to_datetime(df_data_general['FINAL_TIME'])\n",
    "\n",
    "df_data_general['INITIAL_TIME'] = df_data_general['INITIAL_TIME'].dt.to_period('D')\n",
    "df_data_general['INITIAL_TIME'] = df_data_general['INITIAL_TIME'].dt.to_timestamp()\n",
    "\n",
    "\n",
    "df_data_general['FINAL_TIME'] = df_data_general['FINAL_TIME'].dt.to_period('D')\n",
    "df_data_general['FINAL_TIME'] = df_data_general['FINAL_TIME'].dt.to_timestamp()\n",
    "\n",
    "df_data_general['Weekday']= df_data_general['INITIAL_TIME'].dt.strftime('%A')\n",
    "df_data_general['number_of_day'] = df_data_general['INITIAL_TIME'].dt.day_of_week\n",
    "\n",
    "df_data_general['TIME_ON_DEVICE_MIN'] = df_data_general['TIME_ON_DEVICE_SEC'] / 60\n",
    "\n",
    "df_data_general['Hour'] = df_data_general['INITIAL_TIME'].dt.hour\n",
    "df_data_general['Weekday'] = df_data_general['INITIAL_TIME'].dt.weekday   # 0=Lunes, 6=Domingo\n",
    "df_data_general['Weekend'] = (df_data_general['Weekday'] >= 5).astype(int)\n",
    "df_data_general['Month'] = df_data_general['INITIAL_TIME'].dt.month\n",
    "\n",
    "df_data_general = df_data_general[df_data_general['TIME_ON_DEVICE_MIN'] < 600 ]\n",
    "\n",
    "df_data_general = df_data_general[df_data_general['WIN_TOTAL'] > 0]\n",
    "df_data_general['NET_SPEND'] = df_data_general['FINAL_AMOUNT'] - df_data_general['INITIAL_AMOUNT']\n",
    "df_data_general = df_data_general[df_data_general['NET_SPEND'] < 10000 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8de6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "# from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras import layers\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# class MultiArchitectureCasinoML:\n",
    "#     def __init__(self):\n",
    "#         self.scalers = {}\n",
    "#         self.models = {}\n",
    "#         self.feature_names = {}\n",
    "#         self.model_types = ['MLP', 'RNN', 'LSTM', 'GRU', 'CNN1D', 'CNN2D', 'TRANSFORMER', 'AUTOENCODER']\n",
    "        \n",
    "#     def filter_outliers(self, df, method='iqr', factor=1.5):\n",
    "#         \"\"\"Filtrar outliers del dataset\"\"\"\n",
    "#         print(f\"\\n=== FILTRADO DE OUTLIERS (Método: {method.upper()}) ===\")\n",
    "#         print(f\"Dataset original: {len(df)} registros\")\n",
    "        \n",
    "#         df_filtered = df.copy()\n",
    "#         numeric_cols = ['BET_TOTAL', 'WIN_TOTAL', 'TIME_ON_DEVICE_MIN', 'INITIAL_AMOUNT']\n",
    "        \n",
    "#         if method == 'iqr':\n",
    "#             for col in numeric_cols:\n",
    "#                 if col in df_filtered.columns:\n",
    "#                     Q1 = df_filtered[col].quantile(0.25)\n",
    "#                     Q3 = df_filtered[col].quantile(0.75)\n",
    "#                     IQR = Q3 - Q1\n",
    "#                     lower_bound = Q1 - factor * IQR\n",
    "#                     upper_bound = Q3 + factor * IQR\n",
    "                    \n",
    "#                     before_count = len(df_filtered)\n",
    "#                     df_filtered = df_filtered[\n",
    "#                         (df_filtered[col] >= lower_bound) & \n",
    "#                         (df_filtered[col] <= upper_bound)\n",
    "#                     ]\n",
    "#                     removed = before_count - len(df_filtered)\n",
    "#                     if removed > 0:\n",
    "#                         print(f\"{col}: Removidos {removed} outliers (rango: {lower_bound:.1f} - {upper_bound:.1f})\")\n",
    "        \n",
    "#         elif method == 'zscore':\n",
    "#             from scipy import stats\n",
    "#             for col in numeric_cols:\n",
    "#                 if col in df_filtered.columns:\n",
    "#                     z_scores = np.abs(stats.zscore(df_filtered[col]))\n",
    "#                     before_count = len(df_filtered)\n",
    "#                     df_filtered = df_filtered[z_scores < 3]\n",
    "#                     removed = before_count - len(df_filtered)\n",
    "#                     if removed > 0:\n",
    "#                         print(f\"{col}: Removidos {removed} outliers (z-score >= 3)\")\n",
    "        \n",
    "#         elif method == 'percentile':\n",
    "#             for col in numeric_cols:\n",
    "#                 if col in df_filtered.columns:\n",
    "#                     lower_percentile = df_filtered[col].quantile(0.02)\n",
    "#                     upper_percentile = df_filtered[col].quantile(0.98)\n",
    "#                     before_count = len(df_filtered)\n",
    "#                     df_filtered = df_filtered[\n",
    "#                         (df_filtered[col] >= lower_percentile) & \n",
    "#                         (df_filtered[col] <= upper_percentile)\n",
    "#                     ]\n",
    "#                     removed = before_count - len(df_filtered)\n",
    "#                     if removed > 0:\n",
    "#                         print(f\"{col}: Removidos {removed} outliers (percentiles 2-98)\")\n",
    "        \n",
    "#         print(f\"Dataset final: {len(df_filtered)} registros\")\n",
    "#         print(f\"Total removidos: {len(df) - len(df_filtered)} ({(len(df) - len(df_filtered))/len(df)*100:.1f}%)\")\n",
    "        \n",
    "#         return df_filtered\n",
    "    \n",
    "#     def create_business_features(self, df, tiempo_pred=None, bet_pred=None):\n",
    "#         \"\"\"Crear features que reflejen la lógica real del negocio de casino\"\"\"\n",
    "#         features = df[['INITIAL_AMOUNT', 'AVG_BET', 'Cluster']].copy()\n",
    "#         feature_names = ['INITIAL_AMOUNT', 'AVG_BET', 'Cluster']\n",
    "        \n",
    "#         if tiempo_pred is not None:\n",
    "#             features['tiempo_pred'] = tiempo_pred\n",
    "#             feature_names.append('tiempo_pred')\n",
    "            \n",
    "#         if bet_pred is not None:\n",
    "#             features['bet_pred'] = bet_pred\n",
    "#             features['total_money_handled'] = bet_pred\n",
    "#             features['house_edge_effect'] = bet_pred * 0.05\n",
    "#             features['money_multiplier'] = bet_pred / (df['INITIAL_AMOUNT'] + 1)\n",
    "#             features['reinvestment_indicator'] = np.where(bet_pred > df['INITIAL_AMOUNT'], 1, 0)\n",
    "#             features['excess_betting'] = np.maximum(0, bet_pred - df['INITIAL_AMOUNT'])\n",
    "#             features['money_at_risk'] = np.minimum(bet_pred, df['INITIAL_AMOUNT'])\n",
    "#             features['cluster_risk_adjusted'] = df['Cluster'] * features['money_multiplier']\n",
    "            \n",
    "#             feature_names.extend(['bet_pred', 'total_money_handled', 'house_edge_effect', \n",
    "#                                 'money_multiplier', 'reinvestment_indicator', \n",
    "#                                 'excess_betting', 'money_at_risk', 'cluster_risk_adjusted'])\n",
    "            \n",
    "#         return features, feature_names\n",
    "    \n",
    "#     def prepare_sequence_data(self, X, sequence_length=10):\n",
    "#         \"\"\"Preparar datos para modelos secuenciales\"\"\"\n",
    "#         if len(X) < sequence_length:\n",
    "#             X_padded = np.pad(X, ((sequence_length - len(X), 0), (0, 0)), mode='edge')\n",
    "#             X = X_padded\n",
    "        \n",
    "#         X_seq = []\n",
    "#         for i in range(len(X) - sequence_length + 1):\n",
    "#             X_seq.append(X[i:(i + sequence_length)])\n",
    "        \n",
    "#         if len(X_seq) == 0:\n",
    "#             if len(X) == 0:\n",
    "#                 return np.zeros((1, sequence_length, X.shape[1] if len(X.shape) > 1 else 1))\n",
    "#             X_seq = [np.pad(X, ((max(0, sequence_length - len(X)), 0), (0, 0)), mode='edge')[-sequence_length:]]\n",
    "            \n",
    "#         return np.array(X_seq)\n",
    "    \n",
    "#     def prepare_cnn2d_data(self, X, height=4, width=None):\n",
    "#         \"\"\"Preparar datos para CNN2D\"\"\"\n",
    "#         n_features = X.shape[1]\n",
    "        \n",
    "#         if width is None:\n",
    "#             width = n_features // height\n",
    "#             if n_features % height != 0:\n",
    "#                 pad_size = height - (n_features % height)\n",
    "#                 X = np.pad(X, ((0, 0), (0, pad_size)), mode='constant')\n",
    "#                 n_features = X.shape[1]\n",
    "#                 width = n_features // height\n",
    "        \n",
    "#         X_2d = X.reshape(X.shape[0], height, width, 1)\n",
    "#         return X_2d\n",
    "    \n",
    "#     def create_mlp_model(self, input_shape, model_type='tiempo'):\n",
    "#         \"\"\"Crear modelo MLP\"\"\"\n",
    "#         model = keras.Sequential()\n",
    "        \n",
    "#         if model_type == 'tiempo':\n",
    "#             model.add(layers.Dense(128, activation='relu', input_shape=(input_shape,)))\n",
    "#             model.add(layers.BatchNormalization())\n",
    "#             model.add(layers.Dropout(0.3))\n",
    "#             model.add(layers.Dense(64, activation='relu'))\n",
    "#             model.add(layers.BatchNormalization())\n",
    "#             model.add(layers.Dropout(0.2))\n",
    "#             model.add(layers.Dense(32, activation='relu'))\n",
    "#             model.add(layers.Dropout(0.1))\n",
    "#             model.add(layers.Dense(1, activation='linear'))\n",
    "            \n",
    "#         elif model_type == 'bet':\n",
    "#             model.add(layers.Dense(256, activation='relu', input_shape=(input_shape,)))\n",
    "#             model.add(layers.BatchNormalization())\n",
    "#             model.add(layers.Dropout(0.4))\n",
    "#             model.add(layers.Dense(128, activation='relu'))\n",
    "#             model.add(layers.BatchNormalization())\n",
    "#             model.add(layers.Dropout(0.3))\n",
    "#             model.add(layers.Dense(64, activation='relu'))\n",
    "#             model.add(layers.BatchNormalization())\n",
    "#             model.add(layers.Dropout(0.2))\n",
    "#             model.add(layers.Dense(32, activation='relu'))\n",
    "#             model.add(layers.Dropout(0.1))\n",
    "#             model.add(layers.Dense(1, activation='linear'))\n",
    "            \n",
    "#         elif model_type == 'win':\n",
    "#             model.add(layers.Dense(512, activation='relu', input_shape=(input_shape,)))\n",
    "#             model.add(layers.BatchNormalization())\n",
    "#             model.add(layers.Dropout(0.4))\n",
    "#             model.add(layers.Dense(256, activation='relu'))\n",
    "#             model.add(layers.BatchNormalization())\n",
    "#             model.add(layers.Dropout(0.3))\n",
    "#             model.add(layers.Dense(128, activation='relu'))\n",
    "#             model.add(layers.BatchNormalization())\n",
    "#             model.add(layers.Dropout(0.3))\n",
    "#             model.add(layers.Dense(64, activation='relu'))\n",
    "#             model.add(layers.BatchNormalization())\n",
    "#             model.add(layers.Dropout(0.2))\n",
    "#             model.add(layers.Dense(32, activation='relu'))\n",
    "#             model.add(layers.Dropout(0.1))\n",
    "#             model.add(layers.Dense(1, activation='linear'))\n",
    "        \n",
    "#         model.compile(\n",
    "#             optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "#             loss='mse',\n",
    "#             metrics=['mae']\n",
    "#         )\n",
    "        \n",
    "#         return model\n",
    "    \n",
    "#     def create_rnn_model(self, input_shape, model_type='tiempo'):\n",
    "#         \"\"\"Crear modelo RNN\"\"\"\n",
    "#         model = keras.Sequential()\n",
    "        \n",
    "#         if model_type == 'tiempo':\n",
    "#             model.add(layers.SimpleRNN(64, return_sequences=True, input_shape=input_shape))\n",
    "#             model.add(layers.Dropout(0.3))\n",
    "#             model.add(layers.SimpleRNN(32, return_sequences=False))\n",
    "#             model.add(layers.Dropout(0.2))\n",
    "#             model.add(layers.Dense(16, activation='relu'))\n",
    "#             model.add(layers.Dense(1, activation='linear'))\n",
    "            \n",
    "#         elif model_type == 'bet':\n",
    "#             model.add(layers.SimpleRNN(128, return_sequences=True, input_shape=input_shape))\n",
    "#             model.add(layers.Dropout(0.4))\n",
    "#             model.add(layers.SimpleRNN(64, return_sequences=True))\n",
    "#             model.add(layers.Dropout(0.3))\n",
    "#             model.add(layers.SimpleRNN(32, return_sequences=False))\n",
    "#             model.add(layers.Dropout(0.2))\n",
    "#             model.add(layers.Dense(16, activation='relu'))\n",
    "#             model.add(layers.Dense(1, activation='linear'))\n",
    "            \n",
    "#         elif model_type == 'win':\n",
    "#             model.add(layers.SimpleRNN(256, return_sequences=True, input_shape=input_shape))\n",
    "#             model.add(layers.Dropout(0.4))\n",
    "#             model.add(layers.SimpleRNN(128, return_sequences=True))\n",
    "#             model.add(layers.Dropout(0.3))\n",
    "#             model.add(layers.SimpleRNN(64, return_sequences=False))\n",
    "#             model.add(layers.Dropout(0.2))\n",
    "#             model.add(layers.Dense(32, activation='relu'))\n",
    "#             model.add(layers.Dense(16, activation='relu'))\n",
    "#             model.add(layers.Dense(1, activation='linear'))\n",
    "        \n",
    "#         model.compile(\n",
    "#             optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "#             loss='mse',\n",
    "#             metrics=['mae']\n",
    "#         )\n",
    "        \n",
    "#         return model\n",
    "    \n",
    "#     def create_lstm_model(self, input_shape, model_type='tiempo'):\n",
    "#         \"\"\"Crear modelo LSTM\"\"\"\n",
    "#         model = keras.Sequential()\n",
    "        \n",
    "#         if model_type == 'tiempo':\n",
    "#             model.add(layers.LSTM(64, return_sequences=True, input_shape=input_shape))\n",
    "#             model.add(layers.Dropout(0.3))\n",
    "#             model.add(layers.LSTM(32, return_sequences=False))\n",
    "#             model.add(layers.Dropout(0.2))\n",
    "#             model.add(layers.Dense(16, activation='relu'))\n",
    "#             model.add(layers.Dense(1, activation='linear'))\n",
    "            \n",
    "#         elif model_type == 'bet':\n",
    "#             model.add(layers.LSTM(128, return_sequences=True, input_shape=input_shape))\n",
    "#             model.add(layers.Dropout(0.4))\n",
    "#             model.add(layers.LSTM(64, return_sequences=True))\n",
    "#             model.add(layers.Dropout(0.3))\n",
    "#             model.add(layers.LSTM(32, return_sequences=False))\n",
    "#             model.add(layers.Dropout(0.2))\n",
    "#             model.add(layers.Dense(16, activation='relu'))\n",
    "#             model.add(layers.Dense(1, activation='linear'))\n",
    "            \n",
    "#         elif model_type == 'win':\n",
    "#             model.add(layers.LSTM(256, return_sequences=True, input_shape=input_shape))\n",
    "#             model.add(layers.Dropout(0.4))\n",
    "#             model.add(layers.LSTM(128, return_sequences=True))\n",
    "#             model.add(layers.Dropout(0.3))\n",
    "#             model.add(layers.LSTM(64, return_sequences=False))\n",
    "#             model.add(layers.Dropout(0.2))\n",
    "#             model.add(layers.Dense(32, activation='relu'))\n",
    "#             model.add(layers.Dense(16, activation='relu'))\n",
    "#             model.add(layers.Dense(1, activation='linear'))\n",
    "        \n",
    "#         model.compile(\n",
    "#             optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "#             loss='mse',\n",
    "#             metrics=['mae']\n",
    "#         )\n",
    "        \n",
    "#         return model\n",
    "    \n",
    "#     def create_gru_model(self, input_shape, model_type='tiempo'):\n",
    "#         \"\"\"Crear modelo GRU\"\"\"\n",
    "#         model = keras.Sequential()\n",
    "        \n",
    "#         if model_type == 'tiempo':\n",
    "#             model.add(layers.GRU(64, return_sequences=True, input_shape=input_shape))\n",
    "#             model.add(layers.Dropout(0.3))\n",
    "#             model.add(layers.GRU(32, return_sequences=False))\n",
    "#             model.add(layers.Dropout(0.2))\n",
    "#             model.add(layers.Dense(16, activation='relu'))\n",
    "#             model.add(layers.Dense(1, activation='linear'))\n",
    "            \n",
    "#         elif model_type == 'bet':\n",
    "#             model.add(layers.GRU(128, return_sequences=True, input_shape=input_shape))\n",
    "#             model.add(layers.Dropout(0.4))\n",
    "#             model.add(layers.GRU(64, return_sequences=True))\n",
    "#             model.add(layers.Dropout(0.3))\n",
    "#             model.add(layers.GRU(32, return_sequences=False))\n",
    "#             model.add(layers.Dropout(0.2))\n",
    "#             model.add(layers.Dense(16, activation='relu'))\n",
    "#             model.add(layers.Dense(1, activation='linear'))\n",
    "            \n",
    "#         elif model_type == 'win':\n",
    "#             model.add(layers.GRU(256, return_sequences=True, input_shape=input_shape))\n",
    "#             model.add(layers.Dropout(0.4))\n",
    "#             model.add(layers.GRU(128, return_sequences=True))\n",
    "#             model.add(layers.Dropout(0.3))\n",
    "#             model.add(layers.GRU(64, return_sequences=False))\n",
    "#             model.add(layers.Dropout(0.2))\n",
    "#             model.add(layers.Dense(32, activation='relu'))\n",
    "#             model.add(layers.Dense(16, activation='relu'))\n",
    "#             model.add(layers.Dense(1, activation='linear'))\n",
    "        \n",
    "#         model.compile(\n",
    "#             optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "#             loss='mse',\n",
    "#             metrics=['mae']\n",
    "#         )\n",
    "        \n",
    "#         return model\n",
    "    \n",
    "#     def create_cnn1d_model(self, input_shape, model_type='tiempo'):\n",
    "#         \"\"\"Crear modelo CNN 1D\"\"\"\n",
    "#         model = keras.Sequential()\n",
    "        \n",
    "#         if model_type == 'tiempo':\n",
    "#             model.add(layers.Conv1D(64, 3, activation='relu', input_shape=input_shape))\n",
    "#             model.add(layers.BatchNormalization())\n",
    "#             model.add(layers.MaxPooling1D(2))\n",
    "#             model.add(layers.Conv1D(32, 3, activation='relu'))\n",
    "#             model.add(layers.BatchNormalization())\n",
    "#             model.add(layers.GlobalMaxPooling1D())\n",
    "#             model.add(layers.Dense(50, activation='relu'))\n",
    "#             model.add(layers.Dropout(0.3))\n",
    "#             model.add(layers.Dense(1, activation='linear'))\n",
    "            \n",
    "#         elif model_type == 'bet':\n",
    "#             model.add(layers.Conv1D(128, 3, activation='relu', input_shape=input_shape))\n",
    "#             model.add(layers.BatchNormalization())\n",
    "#             model.add(layers.MaxPooling1D(2))\n",
    "#             model.add(layers.Conv1D(64, 3, activation='relu'))\n",
    "#             model.add(layers.BatchNormalization())\n",
    "#             model.add(layers.Conv1D(32, 3, activation='relu'))\n",
    "#             model.add(layers.GlobalMaxPooling1D())\n",
    "#             model.add(layers.Dense(100, activation='relu'))\n",
    "#             model.add(layers.Dropout(0.4))\n",
    "#             model.add(layers.Dense(50, activation='relu'))\n",
    "#             model.add(layers.Dropout(0.3))\n",
    "#             model.add(layers.Dense(1, activation='linear'))\n",
    "            \n",
    "#         elif model_type == 'win':\n",
    "#             model.add(layers.Conv1D(256, 3, activation='relu', input_shape=input_shape))\n",
    "#             model.add(layers.BatchNormalization())\n",
    "#             model.add(layers.MaxPooling1D(2))\n",
    "#             model.add(layers.Conv1D(128, 3, activation='relu'))\n",
    "#             model.add(layers.BatchNormalization())\n",
    "#             model.add(layers.Conv1D(64, 3, activation='relu'))\n",
    "#             model.add(layers.BatchNormalization())\n",
    "#             model.add(layers.GlobalMaxPooling1D())\n",
    "#             model.add(layers.Dense(200, activation='relu'))\n",
    "#             model.add(layers.Dropout(0.4))\n",
    "#             model.add(layers.Dense(100, activation='relu'))\n",
    "#             model.add(layers.Dropout(0.3))\n",
    "#             model.add(layers.Dense(50, activation='relu'))\n",
    "#             model.add(layers.Dropout(0.2))\n",
    "#             model.add(layers.Dense(1, activation='linear'))\n",
    "        \n",
    "#         model.compile(\n",
    "#             optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "#             loss='mse',\n",
    "#             metrics=['mae']\n",
    "#         )\n",
    "        \n",
    "#         return model\n",
    "    \n",
    "#     def create_cnn2d_model(self, input_shape, model_type='tiempo'):\n",
    "#         \"\"\"Crear modelo CNN 2D\"\"\"\n",
    "#         model = keras.Sequential()\n",
    "        \n",
    "#         if model_type == 'tiempo':\n",
    "#             model.add(layers.Conv2D(32, (2, 2), activation='relu', input_shape=input_shape))\n",
    "#             model.add(layers.BatchNormalization())\n",
    "#             model.add(layers.Conv2D(16, (2, 2), activation='relu'))\n",
    "#             model.add(layers.BatchNormalization())\n",
    "#             model.add(layers.GlobalMaxPooling2D())\n",
    "#             model.add(layers.Dense(32, activation='relu'))\n",
    "#             model.add(layers.Dropout(0.3))\n",
    "#             model.add(layers.Dense(1, activation='linear'))\n",
    "            \n",
    "#         elif model_type == 'bet':\n",
    "#             model.add(layers.Conv2D(64, (2, 2), activation='relu', input_shape=input_shape))\n",
    "#             model.add(layers.BatchNormalization())\n",
    "#             model.add(layers.Conv2D(32, (2, 2), activation='relu'))\n",
    "#             model.add(layers.BatchNormalization())\n",
    "#             model.add(layers.Conv2D(16, (2, 2), activation='relu'))\n",
    "#             model.add(layers.GlobalMaxPooling2D())\n",
    "#             model.add(layers.Dense(64, activation='relu'))\n",
    "#             model.add(layers.Dropout(0.4))\n",
    "#             model.add(layers.Dense(32, activation='relu'))\n",
    "#             model.add(layers.Dropout(0.3))\n",
    "#             model.add(layers.Dense(1, activation='linear'))\n",
    "            \n",
    "#         elif model_type == 'win':\n",
    "#             model.add(layers.Conv2D(128, (2, 2), activation='relu', input_shape=input_shape))\n",
    "#             model.add(layers.BatchNormalization())\n",
    "#             model.add(layers.Conv2D(64, (2, 2), activation='relu'))\n",
    "#             model.add(layers.BatchNormalization())\n",
    "#             model.add(layers.Conv2D(32, (2, 2), activation='relu'))\n",
    "#             model.add(layers.BatchNormalization())\n",
    "#             model.add(layers.GlobalMaxPooling2D())\n",
    "#             model.add(layers.Dense(128, activation='relu'))\n",
    "#             model.add(layers.Dropout(0.4))\n",
    "#             model.add(layers.Dense(64, activation='relu'))\n",
    "#             model.add(layers.Dropout(0.3))\n",
    "#             model.add(layers.Dense(32, activation='relu'))\n",
    "#             model.add(layers.Dropout(0.2))\n",
    "#             model.add(layers.Dense(1, activation='linear'))\n",
    "        \n",
    "#         model.compile(\n",
    "#             optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "#             loss='mse',\n",
    "#             metrics=['mae']\n",
    "#         )\n",
    "        \n",
    "#         return model\n",
    "    \n",
    "#     def create_transformer_model(self, input_shape, model_type='tiempo'):\n",
    "#         \"\"\"Crear modelo Transformer\"\"\"\n",
    "#         inputs = keras.Input(shape=input_shape)\n",
    "        \n",
    "#         if model_type == 'tiempo':\n",
    "#             attention = layers.MultiHeadAttention(num_heads=4, key_dim=16)(inputs, inputs)\n",
    "#             attention = layers.Dropout(0.3)(attention)\n",
    "#             attention = layers.LayerNormalization()(attention + inputs)\n",
    "            \n",
    "#             ff = layers.Dense(64, activation='relu')(attention)\n",
    "#             ff = layers.Dropout(0.3)(ff)\n",
    "#             ff = layers.Dense(input_shape[-1])(ff)\n",
    "#             ff = layers.LayerNormalization()(ff + attention)\n",
    "            \n",
    "#         elif model_type == 'win':\n",
    "#             attention = layers.MultiHeadAttention(num_heads=8, key_dim=64)(inputs, inputs)\n",
    "#             attention = layers.Dropout(0.4)(attention)\n",
    "#             attention = layers.LayerNormalization()(attention + inputs)\n",
    "            \n",
    "#             ff = layers.Dense(256, activation='relu')(attention)\n",
    "#             ff = layers.Dropout(0.4)(ff)\n",
    "#             ff = layers.Dense(128, activation='relu')(ff)\n",
    "#             ff = layers.Dropout(0.3)(ff)\n",
    "#             ff = layers.Dense(64, activation='relu')(ff)\n",
    "#             ff = layers.Dropout(0.2)(ff)\n",
    "#             ff = layers.Dense(input_shape[-1])(ff)\n",
    "#             ff = layers.LayerNormalization()(ff + attention)\n",
    "        \n",
    "#         pooled = layers.GlobalAveragePooling1D()(ff)\n",
    "#         outputs = layers.Dense(1, activation='linear')(pooled)\n",
    "        \n",
    "#         model = keras.Model(inputs, outputs)\n",
    "#         model.compile(\n",
    "#             optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "#             loss='mse',\n",
    "#             metrics=['mae']\n",
    "#         )\n",
    "        \n",
    "#         return model\n",
    "    \n",
    "#     def create_autoencoder_model(self, input_shape, model_type='tiempo'):\n",
    "#         \"\"\"Crear modelo Autoencoder\"\"\"\n",
    "#         input_layer = keras.Input(shape=(input_shape,))\n",
    "        \n",
    "#         if model_type == 'tiempo':\n",
    "#             encoded = layers.Dense(64, activation='relu')(input_layer)\n",
    "#             encoded = layers.BatchNormalization()(encoded)\n",
    "#             encoded = layers.Dropout(0.3)(encoded)\n",
    "#             encoded = layers.Dense(32, activation='relu')(encoded)\n",
    "#             encoded = layers.BatchNormalization()(encoded)\n",
    "#             encoded = layers.Dropout(0.2)(encoded)\n",
    "#             encoded = layers.Dense(16, activation='relu')(encoded)\n",
    "            \n",
    "#             decoded = layers.Dense(32, activation='relu')(encoded)\n",
    "#             decoded = layers.BatchNormalization()(decoded)\n",
    "#             decoded = layers.Dropout(0.2)(decoded)\n",
    "#             decoded = layers.Dense(64, activation='relu')(decoded)\n",
    "#             decoded = layers.BatchNormalization()(decoded)\n",
    "#             decoded = layers.Dropout(0.3)(decoded)\n",
    "#             decoded = layers.Dense(input_shape, activation='linear')(decoded)\n",
    "            \n",
    "#             predictor = layers.Dense(8, activation='relu')(encoded)\n",
    "#             predictor = layers.Dropout(0.2)(predictor)\n",
    "#             output = layers.Dense(1, activation='linear')(predictor)\n",
    "            \n",
    "#         elif model_type == 'bet':\n",
    "#             encoded = layers.Dense(128, activation='relu')(input_layer)\n",
    "#             encoded = layers.BatchNormalization()(encoded)\n",
    "#             encoded = layers.Dropout(0.4)(encoded)\n",
    "#             encoded = layers.Dense(64, activation='relu')(encoded)\n",
    "#             encoded = layers.BatchNormalization()(encoded)\n",
    "#             encoded = layers.Dropout(0.3)(encoded)\n",
    "#             encoded = layers.Dense(32, activation='relu')(encoded)\n",
    "            \n",
    "#             decoded = layers.Dense(64, activation='relu')(encoded)\n",
    "#             decoded = layers.BatchNormalization()(decoded)\n",
    "#             decoded = layers.Dropout(0.3)(decoded)\n",
    "#             decoded = layers.Dense(128, activation='relu')(decoded)\n",
    "#             decoded = layers.BatchNormalization()(decoded)\n",
    "#             decoded = layers.Dropout(0.4)(decoded)\n",
    "#             decoded = layers.Dense(input_shape, activation='linear')(decoded)\n",
    "            \n",
    "#             predictor = layers.Dense(16, activation='relu')(encoded)\n",
    "#             predictor = layers.Dropout(0.3)(predictor)\n",
    "#             output = layers.Dense(1, activation='linear')(predictor)\n",
    "            \n",
    "#         elif model_type == 'win':\n",
    "#             encoded = layers.Dense(256, activation='relu')(input_layer)\n",
    "#             encoded = layers.BatchNormalization()(encoded)\n",
    "#             encoded = layers.Dropout(0.4)(encoded)\n",
    "#             encoded = layers.Dense(128, activation='relu')(encoded)\n",
    "#             encoded = layers.BatchNormalization()(encoded)\n",
    "#             encoded = layers.Dropout(0.3)(encoded)\n",
    "#             encoded = layers.Dense(64, activation='relu')(encoded)\n",
    "            \n",
    "#             decoded = layers.Dense(128, activation='relu')(encoded)\n",
    "#             decoded = layers.BatchNormalization()(decoded)\n",
    "#             decoded = layers.Dropout(0.3)(decoded)\n",
    "#             decoded = layers.Dense(256, activation='relu')(decoded)\n",
    "#             decoded = layers.BatchNormalization()(decoded)\n",
    "#             decoded = layers.Dropout(0.4)(decoded)\n",
    "#             decoded = layers.Dense(input_shape, activation='linear')(decoded)\n",
    "            \n",
    "#             predictor = layers.Dense(32, activation='relu')(encoded)\n",
    "#             predictor = layers.Dropout(0.3)(predictor)\n",
    "#             predictor = layers.Dense(16, activation='relu')(predictor)\n",
    "#             predictor = layers.Dropout(0.2)(predictor)\n",
    "#             output = layers.Dense(1, activation='linear')(predictor)\n",
    "        \n",
    "#         model = keras.Model(input_layer, [output, decoded])\n",
    "#         model.compile(\n",
    "#             optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "#             loss=['mse', 'mse'],\n",
    "#             loss_weights=[0.7, 0.3],\n",
    "#             metrics={'dense_output': ['mae'], 'decoded_output': ['mae']}\n",
    "#         )\n",
    "        \n",
    "#         return model\n",
    "    \n",
    "#     def generate_random_data(self, X_train, n_samples=1000):\n",
    "#         \"\"\"Generar datos aleatorios basados en estadísticas de entrenamiento\"\"\"\n",
    "#         np.random.seed(42)\n",
    "#         random_data = np.zeros((n_samples, X_train.shape[1]))\n",
    "        \n",
    "#         for i in range(X_train.shape[1]):\n",
    "#             feature_mean = X_train[:, i].mean()\n",
    "#             feature_std = X_train[:, i].std()\n",
    "#             feature_min = X_train[:, i].min()\n",
    "#             feature_max = X_train[:, i].max()\n",
    "            \n",
    "#             random_data[:, i] = np.random.normal(feature_mean, feature_std, size=n_samples)\n",
    "#             random_data[:, i] = np.clip(random_data[:, i], feature_min, feature_max)\n",
    "        \n",
    "#         return random_data\n",
    "    \n",
    "#     def create_individual_feature_visualizations(self, X_real_scaled, y_real, X_random_scaled, y_pred_random, \n",
    "#                                            X_real_original, X_random_original, feature_names, model_name, \n",
    "#                                            architecture, output_dir='visualizations', filter_outliers=True):\n",
    "#         \"\"\"Crear visualizaciones individuales para cada variable con valores sin escalar\"\"\"\n",
    "#         if not os.path.exists(output_dir):\n",
    "#             os.makedirs(output_dir)\n",
    "        \n",
    "#         n_features = X_real_original.shape[1]\n",
    "#         print(f\"\\n=== CREANDO VISUALIZACIONES PARA {n_features} VARIABLES - {model_name.upper()} ({architecture}) ===\")\n",
    "        \n",
    "#         # FIX: Ensure all arrays have the same length\n",
    "#         min_len = min(len(X_real_original), len(y_real))\n",
    "#         X_real_original = X_real_original[:min_len]\n",
    "#         y_real = y_real[:min_len]\n",
    "        \n",
    "#         # Also ensure random data matches\n",
    "#         min_random_len = min(len(X_random_original), len(y_pred_random))\n",
    "#         X_random_original = X_random_original[:min_random_len]\n",
    "#         y_pred_random = y_pred_random[:min_random_len]\n",
    "        \n",
    "#         print(f\"Real data points: {len(X_real_original)}, Random data points: {len(X_random_original)}\")\n",
    "        \n",
    "#         fig, axes = plt.subplots(2, (n_features + 1) // 2, figsize=(6 * ((n_features + 1) // 2), 12))\n",
    "#         if n_features == 1:\n",
    "#             axes = [axes]\n",
    "#         elif (n_features + 1) // 2 == 1:\n",
    "#             axes = axes.reshape(-1, 1)\n",
    "#         axes = axes.flatten()\n",
    "        \n",
    "#         for i, feature_name in enumerate(feature_names):\n",
    "#             if i >= len(axes):\n",
    "#                 break\n",
    "                \n",
    "#             df_viz = pd.DataFrame({\n",
    "#                 'Feature_Value': np.concatenate([X_real_original[:, i], X_random_original[:, i]]),\n",
    "#                 'Predictions': np.concatenate([y_real, y_pred_random]),\n",
    "#                 'Data_Type': ['Real'] * len(X_real_original) + ['Random'] * len(X_random_original)\n",
    "#             })\n",
    "            \n",
    "#             if filter_outliers:\n",
    "#                 feature_p5 = df_viz['Feature_Value'].quantile(0.05)\n",
    "#                 feature_p95 = df_viz['Feature_Value'].quantile(0.95)\n",
    "#                 pred_p5 = df_viz['Predictions'].quantile(0.05)\n",
    "#                 pred_p95 = df_viz['Predictions'].quantile(0.95)\n",
    "                \n",
    "#                 mask = (\n",
    "#                     (df_viz['Feature_Value'] >= feature_p5) & \n",
    "#                     (df_viz['Feature_Value'] <= feature_p95) &\n",
    "#                     (df_viz['Predictions'] >= pred_p5) &\n",
    "#                     (df_viz['Predictions'] <= pred_p95)\n",
    "#                 )\n",
    "                \n",
    "#                 df_viz_filtered = df_viz[mask].copy()\n",
    "#                 removed_points = len(df_viz) - len(df_viz_filtered)\n",
    "                \n",
    "#                 if removed_points > 0:\n",
    "#                     print(f\"{feature_name}: Removidos {removed_points} outliers en visualización\")\n",
    "                \n",
    "#                 df_viz = df_viz_filtered\n",
    "            \n",
    "#             ax = axes[i]\n",
    "            \n",
    "#             real_data = df_viz[df_viz['Data_Type'] == 'Real']\n",
    "#             random_data = df_viz[df_viz['Data_Type'] == 'Random']\n",
    "            \n",
    "#             ax.scatter(real_data['Feature_Value'], real_data['Predictions'], \n",
    "#                     color='blue', alpha=0.6, s=30, label='Real Data')\n",
    "#             ax.scatter(random_data['Feature_Value'], random_data['Predictions'], \n",
    "#                     color='red', alpha=0.6, s=30, label='Random Data')\n",
    "            \n",
    "#             colors = {'Real': 'blue', 'Random': 'red'}\n",
    "#             for data_type, color in colors.items():\n",
    "#                 subset = df_viz[df_viz['Data_Type'] == data_type]\n",
    "#                 if len(subset) > 1:\n",
    "#                     lr = LinearRegression()\n",
    "#                     X_trend = subset['Feature_Value'].values.reshape(-1, 1)\n",
    "#                     y_trend = subset['Predictions'].values\n",
    "                    \n",
    "#                     if np.var(X_trend) > 1e-10:\n",
    "#                         lr.fit(X_trend, y_trend)\n",
    "                        \n",
    "#                         x_line = np.linspace(subset['Feature_Value'].min(), \n",
    "#                                         subset['Feature_Value'].max(), 100)\n",
    "#                         y_line = lr.predict(x_line.reshape(-1, 1))\n",
    "                        \n",
    "#                         try:\n",
    "#                             r2 = lr.score(X_trend, y_trend)\n",
    "#                             ax.plot(x_line, y_line, color=color, linestyle='--', \n",
    "#                                 alpha=0.8, linewidth=2, \n",
    "#                                 label=f'{data_type} Trend (R²={r2:.3f})')\n",
    "#                         except:\n",
    "#                             pass\n",
    "            \n",
    "#             ax.set_xlabel(f'{feature_name}')\n",
    "#             ax.set_ylabel('Predicciones')\n",
    "#             ax.set_title(f'{feature_name}\\n({len(df_viz)} puntos) - Valores Originales')\n",
    "#             ax.legend(fontsize=8)\n",
    "#             ax.grid(True, alpha=0.3)\n",
    "        \n",
    "#         for i in range(len(feature_names), len(axes)):\n",
    "#             axes[i].set_visible(False)\n",
    "        \n",
    "#         filter_status = \"CON FILTRO\" if filter_outliers else \"SIN FILTRO\"\n",
    "#         fig.suptitle(f'Análisis por Variable - {model_name.upper()} ({architecture}) ({filter_status}) - Valores Originales', \n",
    "#                     fontsize=16, y=0.98)\n",
    "#         plt.tight_layout()\n",
    "#         plt.subplots_adjust(top=0.93)\n",
    "        \n",
    "#         suffix = \"_filtered\" if filter_outliers else \"_original\"\n",
    "#         filename = f\"{output_dir}/{model_name}_{architecture.lower()}_individual_features{suffix}.png\"\n",
    "#         plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "#         plt.close()\n",
    "        \n",
    "#         print(f\"Gráfico guardado: {filename}\")\n",
    "        \n",
    "#         return df_viz\n",
    "#     def train_single_model(self, X_train, X_test, y_train, y_test, model_type, target_name, architecture='MLP'):\n",
    "#         \"\"\"Entrenar un solo modelo con arquitectura específica\"\"\"\n",
    "#         print(f\"\\n=== {architecture} - {target_name.upper()} ===\")\n",
    "        \n",
    "#         X_train_original = X_train.copy()\n",
    "#         X_test_original = X_test.copy()\n",
    "        \n",
    "#         if architecture in ['RNN', 'LSTM', 'GRU', 'CNN1D', 'TRANSFORMER']:\n",
    "#             sequence_length = min(10, len(X_train))\n",
    "#             X_train_prepared = self.prepare_sequence_data(X_train, sequence_length)\n",
    "#             X_test_prepared = self.prepare_sequence_data(X_test, sequence_length)\n",
    "            \n",
    "#             if len(y_train) > len(X_train_prepared):\n",
    "#                 y_train = y_train[len(y_train) - len(X_train_prepared):]\n",
    "#             if len(y_test) > len(X_test_prepared):\n",
    "#                 y_test = y_test[len(y_test) - len(X_test_prepared):]\n",
    "                \n",
    "#             input_shape = X_train_prepared.shape[1:]\n",
    "            \n",
    "#         elif architecture == 'CNN2D':\n",
    "#             X_train_prepared = self.prepare_cnn2d_data(X_train)\n",
    "#             X_test_prepared = self.prepare_cnn2d_data(X_test)\n",
    "#             input_shape = X_train_prepared.shape[1:]\n",
    "            \n",
    "#         else:\n",
    "#             X_train_prepared = X_train_original\n",
    "#             X_test_prepared = X_test_original\n",
    "#             input_shape = X_train.shape[1]\n",
    "        \n",
    "#         model = None\n",
    "#         if architecture == 'MLP':\n",
    "#             model = self.create_mlp_model(input_shape, model_type)\n",
    "#         elif architecture == 'RNN':\n",
    "#             model = self.create_rnn_model(input_shape, model_type)\n",
    "#         elif architecture == 'LSTM':\n",
    "#             model = self.create_lstm_model(input_shape, model_type)\n",
    "#         elif architecture == 'GRU':\n",
    "#             model = self.create_gru_model(input_shape, model_type)\n",
    "#         elif architecture == 'CNN1D':\n",
    "#             model = self.create_cnn1d_model(input_shape, model_type)\n",
    "#         elif architecture == 'CNN2D':\n",
    "#             model = self.create_cnn2d_model(input_shape, model_type)\n",
    "#         elif architecture == 'TRANSFORMER':\n",
    "#             model = self.create_transformer_model(input_shape, model_type)\n",
    "#         elif architecture == 'AUTOENCODER':\n",
    "#             model = self.create_autoencoder_model(input_shape, model_type)\n",
    "#         else:\n",
    "#             print(f\"Error: Arquitectura '{architecture}' no reconocida\")\n",
    "#             return None, None, 0, float('inf'), float('inf'), X_train_original, X_test_original\n",
    "        \n",
    "#         if model is None:\n",
    "#             print(f\"Error: No se pudo crear el modelo {architecture}\")\n",
    "#             return None, None, 0, float('inf'), float('inf'), X_train_original, X_test_original\n",
    "        \n",
    "#         early_stopping = keras.callbacks.EarlyStopping(\n",
    "#             monitor='val_loss', patience=20, restore_best_weights=True\n",
    "#         )\n",
    "#         reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "#             monitor='val_loss', factor=0.5, patience=10, min_lr=0.0001\n",
    "#         )\n",
    "        \n",
    "#         epochs = 150 if architecture in ['TRANSFORMER', 'AUTOENCODER'] else 100\n",
    "#         batch_size = 16 if architecture in ['RNN', 'LSTM', 'GRU'] else 32\n",
    "        \n",
    "#         try:\n",
    "#             if architecture == 'AUTOENCODER':\n",
    "#                 history = model.fit(\n",
    "#                     X_train_prepared, [y_train, X_train_prepared],\n",
    "#                     validation_split=0.2,\n",
    "#                     epochs=epochs,\n",
    "#                     batch_size=batch_size,\n",
    "#                     callbacks=[early_stopping, reduce_lr],\n",
    "#                     verbose=0\n",
    "#                 )\n",
    "#                 predictions = model.predict(X_test_prepared, verbose=0)[0].flatten()\n",
    "#             else:\n",
    "#                 history = model.fit(\n",
    "#                     X_train_prepared, y_train,\n",
    "#                     validation_split=0.2,\n",
    "#                     epochs=epochs,\n",
    "#                     batch_size=batch_size,\n",
    "#                     callbacks=[early_stopping, reduce_lr],\n",
    "#                     verbose=0\n",
    "#                 )\n",
    "#                 predictions = model.predict(X_test_prepared, verbose=0).flatten()\n",
    "            \n",
    "#             r2 = r2_score(y_test, predictions)\n",
    "#             mae = mean_absolute_error(y_test, predictions)\n",
    "#             rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "            \n",
    "#             print(f\"R² Score: {r2:.4f}\")\n",
    "#             print(f\"MAE: {mae:.4f}\")\n",
    "#             print(f\"RMSE: {rmse:.4f}\")\n",
    "#             print(f\"Parámetros: {model.count_params():,}\")\n",
    "#             print(f\"Épocas entrenadas: {len(history.history['loss'])}\")\n",
    "            \n",
    "#             return model, predictions, r2, mae, rmse, X_train_original, X_test_original\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error entrenando {architecture}: {e}\")\n",
    "#             import traceback\n",
    "#             traceback.print_exc()\n",
    "#             return None, None, 0, float('inf'), float('inf'), X_train_original, X_test_original\n",
    "    \n",
    "#     def analyze_business_patterns(self, df):\n",
    "#         \"\"\"Análisis de patrones de negocio\"\"\"\n",
    "#         print(f\"\\n=== ANÁLISIS DE PATRONES DE NEGOCIO ===\")\n",
    "#         reinvested = df['BET_TOTAL'] > df['INITIAL_AMOUNT']\n",
    "#         print(f\"Sesiones con reinversión: {reinvested.sum()} ({reinvested.sum()/len(df)*100:.1f}%)\")\n",
    "        \n",
    "#         net_winners = df['FINAL_AMOUNT'] > df['INITIAL_AMOUNT']\n",
    "#         net_losers = df['FINAL_AMOUNT'] < df['INITIAL_AMOUNT']\n",
    "        \n",
    "#         print(f\"Ganadores netos: {net_winners.sum()} ({net_winners.sum()/len(df)*100:.1f}%)\")\n",
    "#         print(f\"Perdedores netos: {net_losers.sum()} ({net_losers.sum()/len(df)*100:.1f}%)\")\n",
    "        \n",
    "#         win_bet_corr = np.corrcoef(df['WIN_TOTAL'], df['BET_TOTAL'])[0,1]\n",
    "#         print(f\"Correlación WIN vs BET: {win_bet_corr:.3f}\")\n",
    "        \n",
    "#         print(f\"Dataset shape: {df.shape}\")\n",
    "#         print(f\"Rango TIEMPO: {df['TIME_ON_DEVICE_MIN'].min():.1f} - {df['TIME_ON_DEVICE_MIN'].max():.1f}\")\n",
    "#         print(f\"Rango BET: {df['BET_TOTAL'].min():.1f} - {df['BET_TOTAL'].max():.1f}\")\n",
    "#         print(f\"Rango WIN: {df['WIN_TOTAL'].min():.1f} - {df['WIN_TOTAL'].max():.1f}\")\n",
    "    \n",
    "#     def train_and_visualize_all(self, df, architectures=['MLP'], filter_outliers=True, \n",
    "#                                outlier_method='iqr', filter_viz_outliers=True, \n",
    "#                                n_random_samples=1000, output_dir='multi_arch_results'):\n",
    "#         \"\"\"Entrenar múltiples arquitecturas y crear visualizaciones\"\"\"\n",
    "#         print(\"=\" * 80)\n",
    "#         print(\"ENTRENANDO MÚLTIPLES ARQUITECTURAS CON VISUALIZACIONES\")\n",
    "#         print(f\"Arquitecturas seleccionadas: {', '.join(architectures)}\")\n",
    "#         print(\"=\" * 80)\n",
    "        \n",
    "#         if not os.path.exists(output_dir):\n",
    "#             os.makedirs(output_dir)\n",
    "        \n",
    "#         viz_dir = f\"{output_dir}/visualizations\"\n",
    "#         if not os.path.exists(viz_dir):\n",
    "#             os.makedirs(viz_dir)\n",
    "        \n",
    "#         if filter_outliers:\n",
    "#             df_processed = self.filter_outliers(df, method=outlier_method)\n",
    "#         else:\n",
    "#             df_processed = df.copy()\n",
    "#             print(f\"\\nSin filtrado de outliers - Dataset: {len(df_processed)} registros\")\n",
    "        \n",
    "#         self.analyze_business_patterns(df_processed)\n",
    "        \n",
    "#         X_base = df_processed[['INITIAL_AMOUNT','AVG_BET','Cluster','Weekday','Weekend','Month']]\n",
    "#         y_tiempo = df_processed['TIME_ON_DEVICE_MIN']\n",
    "#         y_bet = df_processed['BET_TOTAL'] \n",
    "#         y_win = df_processed['WIN_TOTAL']\n",
    "        \n",
    "#         X_base_train, X_base_test, y_tiempo_train, y_tiempo_test = train_test_split(\n",
    "#             X_base, y_tiempo, test_size=0.2, random_state=42, \n",
    "#             stratify=pd.cut(y_tiempo, bins=5, labels=False)\n",
    "#         )\n",
    "        \n",
    "#         train_idx, test_idx = X_base_train.index, X_base_test.index\n",
    "        \n",
    "#         all_results = {'tiempo': {}, 'bet': {}, 'win': {}}\n",
    "#         all_visualizations = {'tiempo': {}, 'bet': {}, 'win': {}}\n",
    "        \n",
    "#         print(f\"\\n{'='*50}\")\n",
    "#         print(\"MODELOS PARA PREDICCIÓN DE TIEMPO\")\n",
    "#         print(f\"{'='*50}\")\n",
    "        \n",
    "#         X_tiempo_train, feature_names_tiempo = self.create_business_features(X_base_train)\n",
    "#         X_tiempo_test, _ = self.create_business_features(X_base_test)\n",
    "        \n",
    "#         self.feature_names['tiempo'] = feature_names_tiempo\n",
    "        \n",
    "#         scaler_tiempo = RobustScaler()\n",
    "#         X_tiempo_train_scaled = scaler_tiempo.fit_transform(X_tiempo_train)\n",
    "#         X_tiempo_test_scaled = scaler_tiempo.transform(X_tiempo_test)\n",
    "#         self.scalers['tiempo'] = scaler_tiempo\n",
    "        \n",
    "#         best_tiempo_r2 = -np.inf\n",
    "#         best_tiempo_arch = None\n",
    "#         best_tiempo_pred_train = None\n",
    "#         best_tiempo_pred_test = None\n",
    "        \n",
    "#         for arch in architectures:\n",
    "#             try:\n",
    "#                 model, predictions, r2, mae, rmse, X_train_orig, X_test_orig = self.train_single_model(\n",
    "#                     X_tiempo_train_scaled, X_tiempo_test_scaled, \n",
    "#                     y_tiempo_train.values, y_tiempo_test.values,\n",
    "#                     'tiempo', 'tiempo', arch\n",
    "#                 )\n",
    "                \n",
    "#                 if model is not None:\n",
    "#                     all_results['tiempo'][arch] = {\n",
    "#                         'R2': r2, 'MAE': mae, 'RMSE': rmse, \n",
    "#                         'model': model, 'predictions': predictions\n",
    "#                     }\n",
    "                    \n",
    "#                     X_random_tiempo = self.generate_random_data(X_tiempo_train_scaled, n_random_samples)\n",
    "                    \n",
    "#                     X_tiempo_test_original = scaler_tiempo.inverse_transform(X_test_orig)\n",
    "#                     X_random_tiempo_original = scaler_tiempo.inverse_transform(X_random_tiempo)\n",
    "                    \n",
    "#                     if arch in ['RNN', 'LSTM', 'GRU', 'CNN1D', 'TRANSFORMER']:\n",
    "#                         X_random_prepared = self.prepare_sequence_data(X_random_tiempo, min(10, len(X_tiempo_train_scaled)))\n",
    "#                     elif arch == 'CNN2D':\n",
    "#                         X_random_prepared = self.prepare_cnn2d_data(X_random_tiempo)\n",
    "#                     else:\n",
    "#                         X_random_prepared = X_random_tiempo\n",
    "                    \n",
    "#                     if arch == 'AUTOENCODER':\n",
    "#                         y_random_tiempo_pred = model.predict(X_random_prepared, verbose=0)[0].flatten()\n",
    "#                     else:\n",
    "#                         y_random_tiempo_pred = model.predict(X_random_prepared, verbose=0).flatten()\n",
    "                    \n",
    "#                     viz = self.create_individual_feature_visualizations(\n",
    "#                         X_tiempo_test_scaled, y_tiempo_test.values, \n",
    "#                         X_random_tiempo, y_random_tiempo_pred, \n",
    "#                         X_tiempo_test_original, X_random_tiempo_original,\n",
    "#                         feature_names_tiempo, 'tiempo', arch, viz_dir, filter_viz_outliers\n",
    "#                     )\n",
    "#                     all_visualizations['tiempo'][arch] = viz\n",
    "                    \n",
    "#                     if r2 > best_tiempo_r2:\n",
    "#                         best_tiempo_r2 = r2\n",
    "#                         best_tiempo_arch = arch\n",
    "#                         if arch in ['RNN', 'LSTM', 'GRU', 'CNN1D', 'TRANSFORMER']:\n",
    "#                             X_train_for_pred = self.prepare_sequence_data(X_tiempo_train_scaled, min(10, len(X_tiempo_train_scaled)))\n",
    "#                             X_test_for_pred = self.prepare_sequence_data(X_tiempo_test_scaled, min(10, len(X_tiempo_test_scaled)))\n",
    "#                         elif arch == 'CNN2D':\n",
    "#                             X_train_for_pred = self.prepare_cnn2d_data(X_tiempo_train_scaled)\n",
    "#                             X_test_for_pred = self.prepare_cnn2d_data(X_tiempo_test_scaled)\n",
    "#                         else:\n",
    "#                             X_train_for_pred = X_tiempo_train_scaled\n",
    "#                             X_test_for_pred = X_tiempo_test_scaled\n",
    "                        \n",
    "#                         if arch == 'AUTOENCODER':\n",
    "#                             best_tiempo_pred_train = model.predict(X_train_for_pred, verbose=0)[0].flatten()\n",
    "#                             best_tiempo_pred_test = model.predict(X_test_for_pred, verbose=0)[0].flatten()\n",
    "#                         else:\n",
    "#                             best_tiempo_pred_train = model.predict(X_train_for_pred, verbose=0).flatten()\n",
    "#                             best_tiempo_pred_test = model.predict(X_test_for_pred, verbose=0).flatten()\n",
    "                        \n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error con {arch} para tiempo: {e}\")\n",
    "#                 import traceback\n",
    "#                 traceback.print_exc()\n",
    "#                 continue\n",
    "        \n",
    "#         print(f\"\\n🏆 Mejor modelo TIEMPO: {best_tiempo_arch} (R² = {best_tiempo_r2:.4f})\")\n",
    "        \n",
    "#         print(f\"\\n{'='*50}\")\n",
    "#         print(\"MODELOS PARA PREDICCIÓN DE BET TOTAL\")\n",
    "#         print(f\"{'='*50}\")\n",
    "        \n",
    "#         X_bet_train, feature_names_bet = self.create_business_features(X_base_train, best_tiempo_pred_train)\n",
    "#         X_bet_test, _ = self.create_business_features(X_base_test, best_tiempo_pred_test)\n",
    "        \n",
    "#         self.feature_names['bet'] = feature_names_bet\n",
    "        \n",
    "#         scaler_bet = RobustScaler()\n",
    "#         X_bet_train_scaled = scaler_bet.fit_transform(X_bet_train)\n",
    "#         X_bet_test_scaled = scaler_bet.transform(X_bet_test)\n",
    "#         self.scalers['bet'] = scaler_bet\n",
    "        \n",
    "#         best_bet_r2 = -np.inf\n",
    "#         best_bet_arch = None\n",
    "#         best_bet_pred_test = None\n",
    "        \n",
    "#         for arch in architectures:\n",
    "#             try:\n",
    "#                 model, predictions, r2, mae, rmse, X_train_orig, X_test_orig = self.train_single_model(\n",
    "#                     X_bet_train_scaled, X_bet_test_scaled,\n",
    "#                     y_bet.loc[train_idx].values, y_bet.loc[test_idx].values,\n",
    "#                     'bet', 'bet_total', arch\n",
    "#                 )\n",
    "                \n",
    "#                 if model is not None:\n",
    "#                     all_results['bet'][arch] = {\n",
    "#                         'R2': r2, 'MAE': mae, 'RMSE': rmse, \n",
    "#                         'model': model, 'predictions': predictions\n",
    "#                     }\n",
    "                    \n",
    "#                     X_random_bet = self.generate_random_data(X_bet_train_scaled, n_random_samples)\n",
    "                    \n",
    "#                     X_bet_test_original = scaler_bet.inverse_transform(X_test_orig)\n",
    "#                     X_random_bet_original = scaler_bet.inverse_transform(X_random_bet)\n",
    "                    \n",
    "#                     if arch in ['RNN', 'LSTM', 'GRU', 'CNN1D', 'TRANSFORMER']:\n",
    "#                         X_random_prepared = self.prepare_sequence_data(X_random_bet, min(10, len(X_bet_train_scaled)))\n",
    "#                     elif arch == 'CNN2D':\n",
    "#                         X_random_prepared = self.prepare_cnn2d_data(X_random_bet)\n",
    "#                     else:\n",
    "#                         X_random_prepared = X_random_bet\n",
    "                    \n",
    "#                     if arch == 'AUTOENCODER':\n",
    "#                         y_random_bet_pred = model.predict(X_random_prepared, verbose=0)[0].flatten()\n",
    "#                     else:\n",
    "#                         y_random_bet_pred = model.predict(X_random_prepared, verbose=0).flatten()\n",
    "                    \n",
    "#                     viz = self.create_individual_feature_visualizations(\n",
    "#                         X_bet_test_scaled, y_bet.loc[test_idx].values,\n",
    "#                         X_random_bet, y_random_bet_pred,\n",
    "#                         X_bet_test_original, X_random_bet_original,\n",
    "#                         feature_names_bet, 'bet', arch, viz_dir, filter_viz_outliers\n",
    "#                     )\n",
    "#                     all_visualizations['bet'][arch] = viz\n",
    "                    \n",
    "#                     if r2 > best_bet_r2:\n",
    "#                         best_bet_r2 = r2\n",
    "#                         best_bet_arch = arch\n",
    "                        \n",
    "#                         if arch in ['RNN', 'LSTM', 'GRU', 'CNN1D', 'TRANSFORMER']:\n",
    "#                             X_test_for_pred = self.prepare_sequence_data(X_bet_test_scaled, min(10, len(X_bet_test_scaled)))\n",
    "#                         elif arch == 'CNN2D':\n",
    "#                             X_test_for_pred = self.prepare_cnn2d_data(X_bet_test_scaled)\n",
    "#                         else:\n",
    "#                             X_test_for_pred = X_bet_test_scaled\n",
    "                        \n",
    "#                         if arch == 'AUTOENCODER':\n",
    "#                             best_bet_pred_test = model.predict(X_test_for_pred, verbose=0)[0].flatten()\n",
    "#                         else:\n",
    "#                             best_bet_pred_test = model.predict(X_test_for_pred, verbose=0).flatten()\n",
    "                        \n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error con {arch} para bet: {e}\")\n",
    "#                 import traceback\n",
    "#                 traceback.print_exc()\n",
    "#                 continue\n",
    "        \n",
    "#         print(f\"\\n🏆 Mejor modelo BET: {best_bet_arch} (R² = {best_bet_r2:.4f})\")\n",
    "        \n",
    "#         print(f\"\\n{'='*50}\")\n",
    "#         print(\"MODELOS PARA PREDICCIÓN DE WIN TOTAL\")\n",
    "#         print(f\"{'='*50}\")\n",
    "        \n",
    "#         best_bet_model = all_results['bet'][best_bet_arch]['model']\n",
    "#         if best_bet_arch in ['RNN', 'LSTM', 'GRU', 'CNN1D', 'TRANSFORMER']:\n",
    "#             X_bet_train_prep = self.prepare_sequence_data(X_bet_train_scaled, min(10, len(X_bet_train_scaled)))\n",
    "#         elif best_bet_arch == 'CNN2D':\n",
    "#             X_bet_train_prep = self.prepare_cnn2d_data(X_bet_train_scaled)\n",
    "#         else:\n",
    "#             X_bet_train_prep = X_bet_train_scaled\n",
    "        \n",
    "#         if best_bet_arch == 'AUTOENCODER':\n",
    "#             best_bet_pred_train = best_bet_model.predict(X_bet_train_prep, verbose=0)[0].flatten()\n",
    "#         else:\n",
    "#             best_bet_pred_train = best_bet_model.predict(X_bet_train_prep, verbose=0).flatten()\n",
    "        \n",
    "#         X_win_train, feature_names_win = self.create_business_features(\n",
    "#             X_base_train, best_tiempo_pred_train, best_bet_pred_train\n",
    "#         )\n",
    "#         X_win_test, _ = self.create_business_features(\n",
    "#             X_base_test, best_tiempo_pred_test, best_bet_pred_test\n",
    "#         )\n",
    "        \n",
    "#         self.feature_names['win'] = feature_names_win\n",
    "        \n",
    "#         scaler_win = RobustScaler()\n",
    "#         X_win_train_scaled = scaler_win.fit_transform(X_win_train)\n",
    "#         X_win_test_scaled = scaler_win.transform(X_win_test)\n",
    "#         self.scalers['win'] = scaler_win\n",
    "        \n",
    "#         best_win_r2 = -np.inf\n",
    "#         best_win_arch = None\n",
    "        \n",
    "#         for arch in architectures:\n",
    "#             try:\n",
    "#                 model, predictions, r2, mae, rmse, X_train_orig, X_test_orig = self.train_single_model(\n",
    "#                     X_win_train_scaled, X_win_test_scaled,\n",
    "#                     y_win.loc[train_idx].values, y_win.loc[test_idx].values,\n",
    "#                     'win', 'win_total', arch\n",
    "#                 )\n",
    "                \n",
    "#                 if model is not None:\n",
    "#                     all_results['win'][arch] = {\n",
    "#                         'R2': r2, 'MAE': mae, 'RMSE': rmse, \n",
    "#                         'model': model, 'predictions': predictions\n",
    "#                     }\n",
    "                    \n",
    "#                     X_random_win = self.generate_random_data(X_win_train_scaled, n_random_samples)\n",
    "                    \n",
    "#                     X_win_test_original = scaler_win.inverse_transform(X_test_orig)\n",
    "#                     X_random_win_original = scaler_win.inverse_transform(X_random_win)\n",
    "                    \n",
    "#                     if arch in ['RNN', 'LSTM', 'GRU', 'CNN1D', 'TRANSFORMER']:\n",
    "#                         X_random_prepared = self.prepare_sequence_data(X_random_win, min(10, len(X_win_train_scaled)))\n",
    "#                     elif arch == 'CNN2D':\n",
    "#                         X_random_prepared = self.prepare_cnn2d_data(X_random_win)\n",
    "#                     else:\n",
    "#                         X_random_prepared = X_random_win\n",
    "                    \n",
    "#                     if arch == 'AUTOENCODER':\n",
    "#                         y_random_win_pred = model.predict(X_random_prepared, verbose=0)[0].flatten()\n",
    "#                     else:\n",
    "#                         y_random_win_pred = model.predict(X_random_prepared, verbose=0).flatten()\n",
    "                    \n",
    "#                     viz = self.create_individual_feature_visualizations(\n",
    "#                         X_win_test_scaled, y_win.loc[test_idx].values,\n",
    "#                         X_random_win, y_random_win_pred,\n",
    "#                         X_win_test_original, X_random_win_original,\n",
    "#                         feature_names_win, 'win', arch, viz_dir, filter_viz_outliers\n",
    "#                     )\n",
    "#                     all_visualizations['win'][arch] = viz\n",
    "                    \n",
    "#                     if r2 > best_win_r2:\n",
    "#                         best_win_r2 = r2\n",
    "#                         best_win_arch = arch\n",
    "                        \n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error con {arch} para win: {e}\")\n",
    "#                 import traceback\n",
    "#                 traceback.print_exc()\n",
    "#                 continue\n",
    "        \n",
    "#         if best_win_arch:\n",
    "#             print(f\"\\n🏆 Mejor modelo WIN: {best_win_arch} (R² = {best_win_r2:.4f})\")\n",
    "        \n",
    "#         print(f\"\\n{'='*80}\")\n",
    "#         print(\"RESUMEN COMPARATIVO DE TODAS LAS ARQUITECTURAS\")\n",
    "#         print(f\"{'='*80}\")\n",
    "        \n",
    "#         comparison_data = []\n",
    "#         for target in ['tiempo', 'bet', 'win']:\n",
    "#             for arch in architectures:\n",
    "#                 if arch in all_results[target]:\n",
    "#                     result = all_results[target][arch]\n",
    "#                     comparison_data.append({\n",
    "#                         'Target': target.upper(),\n",
    "#                         'Architecture': arch,\n",
    "#                         'R2_Score': result['R2'],\n",
    "#                         'MAE': result['MAE'],\n",
    "#                         'RMSE': result['RMSE'],\n",
    "#                         'N_Features': len(self.feature_names[target]),\n",
    "#                         'Features': ', '.join(self.feature_names[target])\n",
    "#                     })\n",
    "        \n",
    "#         comparison_df = pd.DataFrame(comparison_data)\n",
    "#         comparison_filename = f\"{output_dir}/architecture_comparison.csv\"\n",
    "#         comparison_df.to_csv(comparison_filename, index=False)\n",
    "#         print(f\"✓ Comparación guardada: {comparison_filename}\")\n",
    "        \n",
    "#         print(f\"\\n🏆 MEJORES MODELOS POR OBJETIVO:\")\n",
    "#         print(f\"   TIEMPO: {best_tiempo_arch} (R² = {best_tiempo_r2:.4f})\")\n",
    "#         print(f\"   BET: {best_bet_arch} (R² = {best_bet_r2:.4f})\")\n",
    "#         if best_win_arch:\n",
    "#             print(f\"   WIN: {best_win_arch} (R² = {best_win_r2:.4f})\")\n",
    "        \n",
    "#         arch_rankings = {}\n",
    "#         for arch in architectures:\n",
    "#             r2_scores = []\n",
    "#             for target in ['tiempo', 'bet', 'win']:\n",
    "#                 if arch in all_results[target]:\n",
    "#                     r2_scores.append(all_results[target][arch]['R2'])\n",
    "#             if r2_scores:\n",
    "#                 arch_rankings[arch] = np.mean(r2_scores)\n",
    "        \n",
    "#         print(f\"\\n📊 RANKING DE ARQUITECTURAS (R² promedio):\")\n",
    "#         for i, (arch, avg_r2) in enumerate(sorted(arch_rankings.items(), key=lambda x: x[1], reverse=True), 1):\n",
    "#             print(f\"   {i}. {arch}: {avg_r2:.4f}\")\n",
    "        \n",
    "#         print(f\"\\n📁 Resultados guardados en: '{output_dir}'\")\n",
    "#         print(f\"📁 Visualizaciones guardadas en: '{viz_dir}'\")\n",
    "        \n",
    "#         return {\n",
    "#             'results': all_results,\n",
    "#             'comparison': comparison_df,\n",
    "#             'visualizations': all_visualizations,\n",
    "#             'best_models': {\n",
    "#                 'tiempo': best_tiempo_arch,\n",
    "#                 'bet': best_bet_arch,\n",
    "#                 'win': best_win_arch\n",
    "#             }\n",
    "#         }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # ============================================================================\n",
    "# # FIX 2: Update create_individual_feature_visualizations to handle length mismatches\n",
    "# # ============================================================================\n",
    "\n",
    "# def create_individual_feature_visualizations(self, X_real_scaled, y_real, X_random_scaled, y_pred_random, \n",
    "#                                            X_real_original, X_random_original, feature_names, model_name, \n",
    "#                                            architecture, output_dir='visualizations', filter_outliers=True):\n",
    "#     \"\"\"Crear visualizaciones individuales para cada variable con valores sin escalar\"\"\"\n",
    "#     if not os.path.exists(output_dir):\n",
    "#         os.makedirs(output_dir)\n",
    "    \n",
    "#     n_features = X_real_original.shape[1]\n",
    "#     print(f\"\\n=== CREANDO VISUALIZACIONES PARA {n_features} VARIABLES - {model_name.upper()} ({architecture}) ===\")\n",
    "    \n",
    "#     # CRITICAL FIX: Ensure all arrays have the same length\n",
    "#     min_len = min(len(X_real_original), len(y_real))\n",
    "#     X_real_original = X_real_original[:min_len]\n",
    "#     y_real = y_real[:min_len]\n",
    "    \n",
    "#     # Also ensure random data matches\n",
    "#     min_random_len = min(len(X_random_original), len(y_pred_random))\n",
    "#     X_random_original = X_random_original[:min_random_len]\n",
    "#     y_pred_random = y_pred_random[:min_random_len]\n",
    "    \n",
    "#     print(f\"Real data points: {len(X_real_original)}, Random data points: {len(X_random_original)}\")\n",
    "    \n",
    "#     fig, axes = plt.subplots(2, (n_features + 1) // 2, figsize=(6 * ((n_features + 1) // 2), 12))\n",
    "#     if n_features == 1:\n",
    "#         axes = [axes]\n",
    "#     elif (n_features + 1) // 2 == 1:\n",
    "#         axes = axes.reshape(-1, 1)\n",
    "#     axes = axes.flatten()\n",
    "    \n",
    "#     for i, feature_name in enumerate(feature_names):\n",
    "#         if i >= len(axes):\n",
    "#             break\n",
    "            \n",
    "#         df_viz = pd.DataFrame({\n",
    "#             'Feature_Value': np.concatenate([X_real_original[:, i], X_random_original[:, i]]),\n",
    "#             'Predictions': np.concatenate([y_real, y_pred_random]),\n",
    "#             'Data_Type': ['Real'] * len(X_real_original) + ['Random'] * len(X_random_original)\n",
    "#         })\n",
    "        \n",
    "#         if filter_outliers:\n",
    "#             feature_p5 = df_viz['Feature_Value'].quantile(0.05)\n",
    "#             feature_p95 = df_viz['Feature_Value'].quantile(0.95)\n",
    "#             pred_p5 = df_viz['Predictions'].quantile(0.05)\n",
    "#             pred_p95 = df_viz['Predictions'].quantile(0.95)\n",
    "            \n",
    "#             mask = (\n",
    "#                 (df_viz['Feature_Value'] >= feature_p5) & \n",
    "#                 (df_viz['Feature_Value'] <= feature_p95) &\n",
    "#                 (df_viz['Predictions'] >= pred_p5) &\n",
    "#                 (df_viz['Predictions'] <= pred_p95)\n",
    "#             )\n",
    "            \n",
    "#             df_viz_filtered = df_viz[mask].copy()\n",
    "#             removed_points = len(df_viz) - len(df_viz_filtered)\n",
    "            \n",
    "#             if removed_points > 0:\n",
    "#                 print(f\"{feature_name}: Removidos {removed_points} outliers en visualización\")\n",
    "            \n",
    "#             df_viz = df_viz_filtered\n",
    "        \n",
    "#         ax = axes[i]\n",
    "        \n",
    "#         real_data = df_viz[df_viz['Data_Type'] == 'Real']\n",
    "#         random_data = df_viz[df_viz['Data_Type'] == 'Random']\n",
    "        \n",
    "#         ax.scatter(real_data['Feature_Value'], real_data['Predictions'], \n",
    "#                   color='blue', alpha=0.6, s=30, label='Real Data')\n",
    "#         ax.scatter(random_data['Feature_Value'], random_data['Predictions'], \n",
    "#                   color='red', alpha=0.6, s=30, label='Random Data')\n",
    "        \n",
    "#         colors = {'Real': 'blue', 'Random': 'red'}\n",
    "#         for data_type, color in colors.items():\n",
    "#             subset = df_viz[df_viz['Data_Type'] == data_type]\n",
    "#             if len(subset) > 1:\n",
    "#                 lr = LinearRegression()\n",
    "#                 X_trend = subset['Feature_Value'].values.reshape(-1, 1)\n",
    "#                 y_trend = subset['Predictions'].values\n",
    "                \n",
    "#                 if np.var(X_trend) > 1e-10:\n",
    "#                     lr.fit(X_trend, y_trend)\n",
    "                    \n",
    "#                     x_line = np.linspace(subset['Feature_Value'].min(), \n",
    "#                                        subset['Feature_Value'].max(), 100)\n",
    "#                     y_line = lr.predict(x_line.reshape(-1, 1))\n",
    "                    \n",
    "#                     try:\n",
    "#                         r2 = lr.score(X_trend, y_trend)\n",
    "#                         ax.plot(x_line, y_line, color=color, linestyle='--', \n",
    "#                                alpha=0.8, linewidth=2, \n",
    "#                                label=f'{data_type} Trend (R²={r2:.3f})')\n",
    "#                     except:\n",
    "#                         pass\n",
    "        \n",
    "#         ax.set_xlabel(f'{feature_name}')\n",
    "#         ax.set_ylabel('Predicciones')\n",
    "#         ax.set_title(f'{feature_name}\\n({len(df_viz)} puntos) - Valores Originales')\n",
    "#         ax.legend(fontsize=8)\n",
    "#         ax.grid(True, alpha=0.3)\n",
    "    \n",
    "#     for i in range(len(feature_names), len(axes)):\n",
    "#         axes[i].set_visible(False)\n",
    "    \n",
    "#     filter_status = \"CON FILTRO\" if filter_outliers else \"SIN FILTRO\"\n",
    "#     fig.suptitle(f'Análisis por Variable - {model_name.upper()} ({architecture}) ({filter_status}) - Valores Originales', \n",
    "#                  fontsize=16, y=0.98)\n",
    "#     plt.tight_layout()\n",
    "#     plt.subplots_adjust(top=0.93)\n",
    "    \n",
    "#     suffix = \"_filtered\" if filter_outliers else \"_original\"\n",
    "#     filename = f\"{output_dir}/{model_name}_{architecture.lower()}_individual_features{suffix}.png\"\n",
    "#     plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "#     plt.close()\n",
    "    \n",
    "#     print(f\"Gráfico guardado: {filename}\")\n",
    "    \n",
    "#     return df_viz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1caf4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class MultiArchitectureCasinoML:\n",
    "    def __init__(self):\n",
    "        self.scalers = {}\n",
    "        self.models = {}\n",
    "        self.feature_names = {}\n",
    "        self.model_types = ['MLP', 'RNN', 'LSTM', 'GRU', 'CNN1D', 'CNN2D', 'TRANSFORMER', 'AUTOENCODER']\n",
    "        \n",
    "    def filter_outliers(self, df, method='iqr', factor=1.5):\n",
    "        \"\"\"Filtrar outliers del dataset\"\"\"\n",
    "        print(f\"\\n=== FILTRADO DE OUTLIERS (Método: {method.upper()}) ===\")\n",
    "        print(f\"Dataset original: {len(df)} registros\")\n",
    "        \n",
    "        df_filtered = df.copy()\n",
    "        numeric_cols = ['BET_TOTAL', 'WIN_TOTAL', 'TIME_ON_DEVICE_MIN', 'INITIAL_AMOUNT']\n",
    "        \n",
    "        if method == 'iqr':\n",
    "            for col in numeric_cols:\n",
    "                if col in df_filtered.columns:\n",
    "                    Q1 = df_filtered[col].quantile(0.25)\n",
    "                    Q3 = df_filtered[col].quantile(0.75)\n",
    "                    IQR = Q3 - Q1\n",
    "                    lower_bound = Q1 - factor * IQR\n",
    "                    upper_bound = Q3 + factor * IQR\n",
    "                    \n",
    "                    before_count = len(df_filtered)\n",
    "                    df_filtered = df_filtered[\n",
    "                        (df_filtered[col] >= lower_bound) & \n",
    "                        (df_filtered[col] <= upper_bound)\n",
    "                    ]\n",
    "                    removed = before_count - len(df_filtered)\n",
    "                    if removed > 0:\n",
    "                        print(f\"{col}: Removidos {removed} outliers (rango: {lower_bound:.1f} - {upper_bound:.1f})\")\n",
    "        \n",
    "        elif method == 'zscore':\n",
    "            from scipy import stats\n",
    "            for col in numeric_cols:\n",
    "                if col in df_filtered.columns:\n",
    "                    z_scores = np.abs(stats.zscore(df_filtered[col]))\n",
    "                    before_count = len(df_filtered)\n",
    "                    df_filtered = df_filtered[z_scores < 3]\n",
    "                    removed = before_count - len(df_filtered)\n",
    "                    if removed > 0:\n",
    "                        print(f\"{col}: Removidos {removed} outliers (z-score >= 3)\")\n",
    "        \n",
    "        elif method == 'percentile':\n",
    "            for col in numeric_cols:\n",
    "                if col in df_filtered.columns:\n",
    "                    lower_percentile = df_filtered[col].quantile(0.02)\n",
    "                    upper_percentile = df_filtered[col].quantile(0.98)\n",
    "                    before_count = len(df_filtered)\n",
    "                    df_filtered = df_filtered[\n",
    "                        (df_filtered[col] >= lower_percentile) & \n",
    "                        (df_filtered[col] <= upper_percentile)\n",
    "                    ]\n",
    "                    removed = before_count - len(df_filtered)\n",
    "                    if removed > 0:\n",
    "                        print(f\"{col}: Removidos {removed} outliers (percentiles 2-98)\")\n",
    "        \n",
    "        print(f\"Dataset final: {len(df_filtered)} registros\")\n",
    "        print(f\"Total removidos: {len(df) - len(df_filtered)} ({(len(df) - len(df_filtered))/len(df)*100:.1f}%)\")\n",
    "        \n",
    "        return df_filtered\n",
    "    \n",
    "    def create_business_features(self, df, tiempo_pred=None, bet_pred=None):\n",
    "        \"\"\"Crear features que reflejen la lógica real del negocio de casino\"\"\"\n",
    "        features = df[['INITIAL_AMOUNT', 'AVG_BET', 'Cluster']].copy()\n",
    "        feature_names = ['INITIAL_AMOUNT', 'AVG_BET', 'Cluster']\n",
    "        \n",
    "        if tiempo_pred is not None:\n",
    "            # FIX: Ensure tiempo_pred matches the length of df\n",
    "            if len(tiempo_pred) != len(df):\n",
    "                print(f\"WARNING: tiempo_pred length ({len(tiempo_pred)}) != df length ({len(df)})\")\n",
    "                # Truncate or pad to match\n",
    "                if len(tiempo_pred) > len(df):\n",
    "                    tiempo_pred = tiempo_pred[:len(df)]\n",
    "                else:\n",
    "                    # Pad with mean value\n",
    "                    padding = np.full(len(df) - len(tiempo_pred), np.mean(tiempo_pred))\n",
    "                    tiempo_pred = np.concatenate([tiempo_pred, padding])\n",
    "            \n",
    "            features['tiempo_pred'] = tiempo_pred\n",
    "            feature_names.append('tiempo_pred')\n",
    "            \n",
    "        if bet_pred is not None:\n",
    "            # FIX: Ensure bet_pred matches the length of df\n",
    "            if len(bet_pred) != len(df):\n",
    "                print(f\"WARNING: bet_pred length ({len(bet_pred)}) != df length ({len(df)})\")\n",
    "                if len(bet_pred) > len(df):\n",
    "                    bet_pred = bet_pred[:len(df)]\n",
    "                else:\n",
    "                    padding = np.full(len(df) - len(bet_pred), np.mean(bet_pred))\n",
    "                    bet_pred = np.concatenate([bet_pred, padding])\n",
    "            \n",
    "            features['bet_pred'] = bet_pred\n",
    "            features['total_money_handled'] = bet_pred\n",
    "            features['house_edge_effect'] = bet_pred * 0.05\n",
    "            features['money_multiplier'] = bet_pred / (df['INITIAL_AMOUNT'].values + 1)\n",
    "            features['reinvestment_indicator'] = np.where(bet_pred > df['INITIAL_AMOUNT'].values, 1, 0)\n",
    "            features['excess_betting'] = np.maximum(0, bet_pred - df['INITIAL_AMOUNT'].values)\n",
    "            features['money_at_risk'] = np.minimum(bet_pred, df['INITIAL_AMOUNT'].values)\n",
    "            features['cluster_risk_adjusted'] = df['Cluster'].values * features['money_multiplier']\n",
    "            \n",
    "            feature_names.extend(['bet_pred', 'total_money_handled', 'house_edge_effect', \n",
    "                                'money_multiplier', 'reinvestment_indicator', \n",
    "                                'excess_betting', 'money_at_risk', 'cluster_risk_adjusted'])\n",
    "            \n",
    "        return features, feature_names\n",
    "    \n",
    "    def prepare_sequence_data(self, X, sequence_length=10):\n",
    "        \"\"\"Preparar datos para modelos secuenciales\"\"\"\n",
    "        if len(X) < sequence_length:\n",
    "            X_padded = np.pad(X, ((sequence_length - len(X), 0), (0, 0)), mode='edge')\n",
    "            X = X_padded\n",
    "        \n",
    "        X_seq = []\n",
    "        for i in range(len(X) - sequence_length + 1):\n",
    "            X_seq.append(X[i:(i + sequence_length)])\n",
    "        \n",
    "        if len(X_seq) == 0:\n",
    "            if len(X) == 0:\n",
    "                return np.zeros((1, sequence_length, X.shape[1] if len(X.shape) > 1 else 1))\n",
    "            X_seq = [np.pad(X, ((max(0, sequence_length - len(X)), 0), (0, 0)), mode='edge')[-sequence_length:]]\n",
    "            \n",
    "        return np.array(X_seq)\n",
    "    \n",
    "    def prepare_cnn2d_data(self, X, height=4, width=None):\n",
    "        \"\"\"Preparar datos para CNN2D\"\"\"\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        if width is None:\n",
    "            width = n_features // height\n",
    "            if n_features % height != 0:\n",
    "                pad_size = height - (n_features % height)\n",
    "                X = np.pad(X, ((0, 0), (0, pad_size)), mode='constant')\n",
    "                n_features = X.shape[1]\n",
    "                width = n_features // height\n",
    "        \n",
    "        X_2d = X.reshape(X.shape[0], height, width, 1)\n",
    "        return X_2d\n",
    "    \n",
    "    def create_mlp_model(self, input_shape, model_type='tiempo'):\n",
    "        \"\"\"Crear modelo MLP\"\"\"\n",
    "        model = keras.Sequential()\n",
    "        \n",
    "        if model_type == 'tiempo':\n",
    "            model.add(layers.Dense(128, activation='relu', input_shape=(input_shape,)))\n",
    "            model.add(layers.BatchNormalization())\n",
    "            model.add(layers.Dropout(0.3))\n",
    "            model.add(layers.Dense(64, activation='relu'))\n",
    "            model.add(layers.BatchNormalization())\n",
    "            model.add(layers.Dropout(0.2))\n",
    "            model.add(layers.Dense(32, activation='relu'))\n",
    "            model.add(layers.Dropout(0.1))\n",
    "            model.add(layers.Dense(1, activation='linear'))\n",
    "            \n",
    "        elif model_type == 'bet':\n",
    "            model.add(layers.Dense(256, activation='relu', input_shape=(input_shape,)))\n",
    "            model.add(layers.BatchNormalization())\n",
    "            model.add(layers.Dropout(0.4))\n",
    "            model.add(layers.Dense(128, activation='relu'))\n",
    "            model.add(layers.BatchNormalization())\n",
    "            model.add(layers.Dropout(0.3))\n",
    "            model.add(layers.Dense(64, activation='relu'))\n",
    "            model.add(layers.BatchNormalization())\n",
    "            model.add(layers.Dropout(0.2))\n",
    "            model.add(layers.Dense(32, activation='relu'))\n",
    "            model.add(layers.Dropout(0.1))\n",
    "            model.add(layers.Dense(1, activation='linear'))\n",
    "            \n",
    "        elif model_type == 'win':\n",
    "            model.add(layers.Dense(512, activation='relu', input_shape=(input_shape,)))\n",
    "            model.add(layers.BatchNormalization())\n",
    "            model.add(layers.Dropout(0.4))\n",
    "            model.add(layers.Dense(256, activation='relu'))\n",
    "            model.add(layers.BatchNormalization())\n",
    "            model.add(layers.Dropout(0.3))\n",
    "            model.add(layers.Dense(128, activation='relu'))\n",
    "            model.add(layers.BatchNormalization())\n",
    "            model.add(layers.Dropout(0.3))\n",
    "            model.add(layers.Dense(64, activation='relu'))\n",
    "            model.add(layers.BatchNormalization())\n",
    "            model.add(layers.Dropout(0.2))\n",
    "            model.add(layers.Dense(32, activation='relu'))\n",
    "            model.add(layers.Dropout(0.1))\n",
    "            model.add(layers.Dense(1, activation='linear'))\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def create_rnn_model(self, input_shape, model_type='tiempo'):\n",
    "        \"\"\"Crear modelo RNN\"\"\"\n",
    "        model = keras.Sequential()\n",
    "        \n",
    "        if model_type == 'tiempo':\n",
    "            model.add(layers.SimpleRNN(64, return_sequences=True, input_shape=input_shape))\n",
    "            model.add(layers.Dropout(0.3))\n",
    "            model.add(layers.SimpleRNN(32, return_sequences=False))\n",
    "            model.add(layers.Dropout(0.2))\n",
    "            model.add(layers.Dense(16, activation='relu'))\n",
    "            model.add(layers.Dense(1, activation='linear'))\n",
    "            \n",
    "        elif model_type == 'bet':\n",
    "            model.add(layers.SimpleRNN(128, return_sequences=True, input_shape=input_shape))\n",
    "            model.add(layers.Dropout(0.4))\n",
    "            model.add(layers.SimpleRNN(64, return_sequences=True))\n",
    "            model.add(layers.Dropout(0.3))\n",
    "            model.add(layers.SimpleRNN(32, return_sequences=False))\n",
    "            model.add(layers.Dropout(0.2))\n",
    "            model.add(layers.Dense(16, activation='relu'))\n",
    "            model.add(layers.Dense(1, activation='linear'))\n",
    "            \n",
    "        elif model_type == 'win':\n",
    "            model.add(layers.SimpleRNN(256, return_sequences=True, input_shape=input_shape))\n",
    "            model.add(layers.Dropout(0.4))\n",
    "            model.add(layers.SimpleRNN(128, return_sequences=True))\n",
    "            model.add(layers.Dropout(0.3))\n",
    "            model.add(layers.SimpleRNN(64, return_sequences=False))\n",
    "            model.add(layers.Dropout(0.2))\n",
    "            model.add(layers.Dense(32, activation='relu'))\n",
    "            model.add(layers.Dense(16, activation='relu'))\n",
    "            model.add(layers.Dense(1, activation='linear'))\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def create_lstm_model(self, input_shape, model_type='tiempo'):\n",
    "        \"\"\"Crear modelo LSTM\"\"\"\n",
    "        model = keras.Sequential()\n",
    "        \n",
    "        if model_type == 'tiempo':\n",
    "            model.add(layers.LSTM(64, return_sequences=True, input_shape=input_shape))\n",
    "            model.add(layers.Dropout(0.3))\n",
    "            model.add(layers.LSTM(32, return_sequences=False))\n",
    "            model.add(layers.Dropout(0.2))\n",
    "            model.add(layers.Dense(16, activation='relu'))\n",
    "            model.add(layers.Dense(1, activation='linear'))\n",
    "            \n",
    "        elif model_type == 'bet':\n",
    "            model.add(layers.LSTM(128, return_sequences=True, input_shape=input_shape))\n",
    "            model.add(layers.Dropout(0.4))\n",
    "            model.add(layers.LSTM(64, return_sequences=True))\n",
    "            model.add(layers.Dropout(0.3))\n",
    "            model.add(layers.LSTM(32, return_sequences=False))\n",
    "            model.add(layers.Dropout(0.2))\n",
    "            model.add(layers.Dense(16, activation='relu'))\n",
    "            model.add(layers.Dense(1, activation='linear'))\n",
    "            \n",
    "        elif model_type == 'win':\n",
    "            model.add(layers.LSTM(256, return_sequences=True, input_shape=input_shape))\n",
    "            model.add(layers.Dropout(0.4))\n",
    "            model.add(layers.LSTM(128, return_sequences=True))\n",
    "            model.add(layers.Dropout(0.3))\n",
    "            model.add(layers.LSTM(64, return_sequences=False))\n",
    "            model.add(layers.Dropout(0.2))\n",
    "            model.add(layers.Dense(32, activation='relu'))\n",
    "            model.add(layers.Dense(16, activation='relu'))\n",
    "            model.add(layers.Dense(1, activation='linear'))\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def create_gru_model(self, input_shape, model_type='tiempo'):\n",
    "        \"\"\"Crear modelo GRU\"\"\"\n",
    "        model = keras.Sequential()\n",
    "        \n",
    "        if model_type == 'tiempo':\n",
    "            model.add(layers.GRU(64, return_sequences=True, input_shape=input_shape))\n",
    "            model.add(layers.Dropout(0.3))\n",
    "            model.add(layers.GRU(32, return_sequences=False))\n",
    "            model.add(layers.Dropout(0.2))\n",
    "            model.add(layers.Dense(16, activation='relu'))\n",
    "            model.add(layers.Dense(1, activation='linear'))\n",
    "            \n",
    "        elif model_type == 'bet':\n",
    "            model.add(layers.GRU(128, return_sequences=True, input_shape=input_shape))\n",
    "            model.add(layers.Dropout(0.4))\n",
    "            model.add(layers.GRU(64, return_sequences=True))\n",
    "            model.add(layers.Dropout(0.3))\n",
    "            model.add(layers.GRU(32, return_sequences=False))\n",
    "            model.add(layers.Dropout(0.2))\n",
    "            model.add(layers.Dense(16, activation='relu'))\n",
    "            model.add(layers.Dense(1, activation='linear'))\n",
    "            \n",
    "        elif model_type == 'win':\n",
    "            model.add(layers.GRU(256, return_sequences=True, input_shape=input_shape))\n",
    "            model.add(layers.Dropout(0.4))\n",
    "            model.add(layers.GRU(128, return_sequences=True))\n",
    "            model.add(layers.Dropout(0.3))\n",
    "            model.add(layers.GRU(64, return_sequences=False))\n",
    "            model.add(layers.Dropout(0.2))\n",
    "            model.add(layers.Dense(32, activation='relu'))\n",
    "            model.add(layers.Dense(16, activation='relu'))\n",
    "            model.add(layers.Dense(1, activation='linear'))\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def create_cnn1d_model(self, input_shape, model_type='tiempo'):\n",
    "        \"\"\"Crear modelo CNN 1D\"\"\"\n",
    "        model = keras.Sequential()\n",
    "        \n",
    "        if model_type == 'tiempo':\n",
    "            model.add(layers.Conv1D(64, 3, activation='relu', input_shape=input_shape))\n",
    "            model.add(layers.BatchNormalization())\n",
    "            model.add(layers.MaxPooling1D(2))\n",
    "            model.add(layers.Conv1D(32, 3, activation='relu'))\n",
    "            model.add(layers.BatchNormalization())\n",
    "            model.add(layers.GlobalMaxPooling1D())\n",
    "            model.add(layers.Dense(50, activation='relu'))\n",
    "            model.add(layers.Dropout(0.3))\n",
    "            model.add(layers.Dense(1, activation='linear'))\n",
    "            \n",
    "        elif model_type == 'bet':\n",
    "            model.add(layers.Conv1D(128, 3, activation='relu', input_shape=input_shape))\n",
    "            model.add(layers.BatchNormalization())\n",
    "            model.add(layers.MaxPooling1D(2))\n",
    "            model.add(layers.Conv1D(64, 3, activation='relu'))\n",
    "            model.add(layers.BatchNormalization())\n",
    "            model.add(layers.Conv1D(32, 3, activation='relu'))\n",
    "            model.add(layers.GlobalMaxPooling1D())\n",
    "            model.add(layers.Dense(100, activation='relu'))\n",
    "            model.add(layers.Dropout(0.4))\n",
    "            model.add(layers.Dense(50, activation='relu'))\n",
    "            model.add(layers.Dropout(0.3))\n",
    "            model.add(layers.Dense(1, activation='linear'))\n",
    "            \n",
    "        elif model_type == 'win':\n",
    "            model.add(layers.Conv1D(256, 3, activation='relu', input_shape=input_shape))\n",
    "            model.add(layers.BatchNormalization())\n",
    "            model.add(layers.MaxPooling1D(2))\n",
    "            model.add(layers.Conv1D(128, 3, activation='relu'))\n",
    "            model.add(layers.BatchNormalization())\n",
    "            model.add(layers.Conv1D(64, 3, activation='relu'))\n",
    "            model.add(layers.BatchNormalization())\n",
    "            model.add(layers.GlobalMaxPooling1D())\n",
    "            model.add(layers.Dense(200, activation='relu'))\n",
    "            model.add(layers.Dropout(0.4))\n",
    "            model.add(layers.Dense(100, activation='relu'))\n",
    "            model.add(layers.Dropout(0.3))\n",
    "            model.add(layers.Dense(50, activation='relu'))\n",
    "            model.add(layers.Dropout(0.2))\n",
    "            model.add(layers.Dense(1, activation='linear'))\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def create_cnn2d_model(self, input_shape, model_type='tiempo'):\n",
    "        \"\"\"Crear modelo CNN 2D\"\"\"\n",
    "        model = keras.Sequential()\n",
    "        \n",
    "        if model_type == 'tiempo':\n",
    "            model.add(layers.Conv2D(32, (2, 2), activation='relu', input_shape=input_shape))\n",
    "            model.add(layers.BatchNormalization())\n",
    "            model.add(layers.Conv2D(16, (2, 2), activation='relu'))\n",
    "            model.add(layers.BatchNormalization())\n",
    "            model.add(layers.GlobalMaxPooling2D())\n",
    "            model.add(layers.Dense(32, activation='relu'))\n",
    "            model.add(layers.Dropout(0.3))\n",
    "            model.add(layers.Dense(1, activation='linear'))\n",
    "            \n",
    "        elif model_type == 'bet':\n",
    "            model.add(layers.Conv2D(64, (2, 2), activation='relu', input_shape=input_shape))\n",
    "            model.add(layers.BatchNormalization())\n",
    "            model.add(layers.Conv2D(32, (2, 2), activation='relu'))\n",
    "            model.add(layers.BatchNormalization())\n",
    "            model.add(layers.Conv2D(16, (2, 2), activation='relu'))\n",
    "            model.add(layers.GlobalMaxPooling2D())\n",
    "            model.add(layers.Dense(64, activation='relu'))\n",
    "            model.add(layers.Dropout(0.4))\n",
    "            model.add(layers.Dense(32, activation='relu'))\n",
    "            model.add(layers.Dropout(0.3))\n",
    "            model.add(layers.Dense(1, activation='linear'))\n",
    "            \n",
    "        elif model_type == 'win':\n",
    "            model.add(layers.Conv2D(128, (2, 2), activation='relu', input_shape=input_shape))\n",
    "            model.add(layers.BatchNormalization())\n",
    "            model.add(layers.Conv2D(64, (2, 2), activation='relu'))\n",
    "            model.add(layers.BatchNormalization())\n",
    "            model.add(layers.Conv2D(32, (2, 2), activation='relu'))\n",
    "            model.add(layers.BatchNormalization())\n",
    "            model.add(layers.GlobalMaxPooling2D())\n",
    "            model.add(layers.Dense(128, activation='relu'))\n",
    "            model.add(layers.Dropout(0.4))\n",
    "            model.add(layers.Dense(64, activation='relu'))\n",
    "            model.add(layers.Dropout(0.3))\n",
    "            model.add(layers.Dense(32, activation='relu'))\n",
    "            model.add(layers.Dropout(0.2))\n",
    "            model.add(layers.Dense(1, activation='linear'))\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def create_transformer_model(self, input_shape, model_type='tiempo'):\n",
    "        \"\"\"Crear modelo Transformer\"\"\"\n",
    "        inputs = keras.Input(shape=input_shape)\n",
    "        \n",
    "        if model_type == 'tiempo':\n",
    "            attention = layers.MultiHeadAttention(num_heads=4, key_dim=16)(inputs, inputs)\n",
    "            attention = layers.Dropout(0.3)(attention)\n",
    "            attention = layers.LayerNormalization()(attention + inputs)\n",
    "            \n",
    "            ff = layers.Dense(64, activation='relu')(attention)\n",
    "            ff = layers.Dropout(0.3)(ff)\n",
    "            ff = layers.Dense(input_shape[-1])(ff)\n",
    "            ff = layers.LayerNormalization()(ff + attention)\n",
    "            \n",
    "        elif model_type == 'win':\n",
    "            attention = layers.MultiHeadAttention(num_heads=8, key_dim=64)(inputs, inputs)\n",
    "            attention = layers.Dropout(0.4)(attention)\n",
    "            attention = layers.LayerNormalization()(attention + inputs)\n",
    "            \n",
    "            ff = layers.Dense(256, activation='relu')(attention)\n",
    "            ff = layers.Dropout(0.4)(ff)\n",
    "            ff = layers.Dense(128, activation='relu')(ff)\n",
    "            ff = layers.Dropout(0.3)(ff)\n",
    "            ff = layers.Dense(64, activation='relu')(ff)\n",
    "            ff = layers.Dropout(0.2)(ff)\n",
    "            ff = layers.Dense(input_shape[-1])(ff)\n",
    "            ff = layers.LayerNormalization()(ff + attention)\n",
    "        \n",
    "        pooled = layers.GlobalAveragePooling1D()(ff)\n",
    "        outputs = layers.Dense(1, activation='linear')(pooled)\n",
    "        \n",
    "        model = keras.Model(inputs, outputs)\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def create_autoencoder_model(self, input_shape, model_type='tiempo'):\n",
    "        \"\"\"Crear modelo Autoencoder\"\"\"\n",
    "        input_layer = keras.Input(shape=(input_shape,))\n",
    "        \n",
    "        if model_type == 'tiempo':\n",
    "            encoded = layers.Dense(64, activation='relu')(input_layer)\n",
    "            encoded = layers.BatchNormalization()(encoded)\n",
    "            encoded = layers.Dropout(0.3)(encoded)\n",
    "            encoded = layers.Dense(32, activation='relu')(encoded)\n",
    "            encoded = layers.BatchNormalization()(encoded)\n",
    "            encoded = layers.Dropout(0.2)(encoded)\n",
    "            encoded = layers.Dense(16, activation='relu')(encoded)\n",
    "            \n",
    "            decoded = layers.Dense(32, activation='relu')(encoded)\n",
    "            decoded = layers.BatchNormalization()(decoded)\n",
    "            decoded = layers.Dropout(0.2)(decoded)\n",
    "            decoded = layers.Dense(64, activation='relu')(decoded)\n",
    "            decoded = layers.BatchNormalization()(decoded)\n",
    "            decoded = layers.Dropout(0.3)(decoded)\n",
    "            decoded = layers.Dense(input_shape, activation='linear')(decoded)\n",
    "            \n",
    "            predictor = layers.Dense(8, activation='relu')(encoded)\n",
    "            predictor = layers.Dropout(0.2)(predictor)\n",
    "            output = layers.Dense(1, activation='linear')(predictor)\n",
    "            \n",
    "        elif model_type == 'bet':\n",
    "            encoded = layers.Dense(128, activation='relu')(input_layer)\n",
    "            encoded = layers.BatchNormalization()(encoded)\n",
    "            encoded = layers.Dropout(0.4)(encoded)\n",
    "            encoded = layers.Dense(64, activation='relu')(encoded)\n",
    "            encoded = layers.BatchNormalization()(encoded)\n",
    "            encoded = layers.Dropout(0.3)(encoded)\n",
    "            encoded = layers.Dense(32, activation='relu')(encoded)\n",
    "            \n",
    "            decoded = layers.Dense(64, activation='relu')(encoded)\n",
    "            decoded = layers.BatchNormalization()(decoded)\n",
    "            decoded = layers.Dropout(0.3)(decoded)\n",
    "            decoded = layers.Dense(128, activation='relu')(decoded)\n",
    "            decoded = layers.BatchNormalization()(decoded)\n",
    "            decoded = layers.Dropout(0.4)(decoded)\n",
    "            decoded = layers.Dense(input_shape, activation='linear')(decoded)\n",
    "            \n",
    "            predictor = layers.Dense(16, activation='relu')(encoded)\n",
    "            predictor = layers.Dropout(0.3)(predictor)\n",
    "            output = layers.Dense(1, activation='linear')(predictor)\n",
    "            \n",
    "        elif model_type == 'win':\n",
    "            encoded = layers.Dense(256, activation='relu')(input_layer)\n",
    "            encoded = layers.BatchNormalization()(encoded)\n",
    "            encoded = layers.Dropout(0.4)(encoded)\n",
    "            encoded = layers.Dense(128, activation='relu')(encoded)\n",
    "            encoded = layers.BatchNormalization()(encoded)\n",
    "            encoded = layers.Dropout(0.3)(encoded)\n",
    "            encoded = layers.Dense(64, activation='relu')(encoded)\n",
    "            \n",
    "            decoded = layers.Dense(128, activation='relu')(encoded)\n",
    "            decoded = layers.BatchNormalization()(decoded)\n",
    "            decoded = layers.Dropout(0.3)(decoded)\n",
    "            decoded = layers.Dense(256, activation='relu')(decoded)\n",
    "            decoded = layers.BatchNormalization()(decoded)\n",
    "            decoded = layers.Dropout(0.4)(decoded)\n",
    "            decoded = layers.Dense(input_shape, activation='linear')(decoded)\n",
    "            \n",
    "            predictor = layers.Dense(32, activation='relu')(encoded)\n",
    "            predictor = layers.Dropout(0.3)(predictor)\n",
    "            predictor = layers.Dense(16, activation='relu')(predictor)\n",
    "            predictor = layers.Dropout(0.2)(predictor)\n",
    "            output = layers.Dense(1, activation='linear')(predictor)\n",
    "        \n",
    "        model = keras.Model(input_layer, [output, decoded])\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "            loss=['mse', 'mse'],\n",
    "            loss_weights=[0.7, 0.3],\n",
    "            metrics={'dense_output': ['mae'], 'decoded_output': ['mae']}\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def generate_random_data(self, X_train, n_samples=1000):\n",
    "        \"\"\"Generar datos aleatorios basados en estadísticas de entrenamiento\"\"\"\n",
    "        np.random.seed(42)\n",
    "        random_data = np.zeros((n_samples, X_train.shape[1]))\n",
    "        \n",
    "        for i in range(X_train.shape[1]):\n",
    "            feature_mean = X_train[:, i].mean()\n",
    "            feature_std = X_train[:, i].std()\n",
    "            feature_min = X_train[:, i].min()\n",
    "            feature_max = X_train[:, i].max()\n",
    "            \n",
    "            random_data[:, i] = np.random.normal(feature_mean, feature_std, size=n_samples)\n",
    "            random_data[:, i] = np.clip(random_data[:, i], feature_min, feature_max)\n",
    "        \n",
    "        return random_data\n",
    "    \n",
    "    def create_individual_feature_visualizations(self, X_real_scaled, y_real, X_random_scaled, y_pred_random, \n",
    "                                           X_real_original, X_random_original, feature_names, model_name, \n",
    "                                           architecture, output_dir='visualizations', filter_outliers=True):\n",
    "        \"\"\"Crear visualizaciones individuales para cada variable con valores sin escalar\"\"\"\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        \n",
    "        n_features = X_real_original.shape[1]\n",
    "        print(f\"\\n=== CREANDO VISUALIZACIONES PARA {n_features} VARIABLES - {model_name.upper()} ({architecture}) ===\")\n",
    "        \n",
    "        # FIX: Ensure all arrays have the same length\n",
    "        min_len = min(len(X_real_original), len(y_real))\n",
    "        X_real_original = X_real_original[:min_len]\n",
    "        y_real = y_real[:min_len]\n",
    "        \n",
    "        # Also ensure random data matches\n",
    "        min_random_len = min(len(X_random_original), len(y_pred_random))\n",
    "        X_random_original = X_random_original[:min_random_len]\n",
    "        y_pred_random = y_pred_random[:min_random_len]\n",
    "        \n",
    "        print(f\"Real data points: {len(X_real_original)}, Random data points: {len(X_random_original)}\")\n",
    "        \n",
    "        fig, axes = plt.subplots(2, (n_features + 1) // 2, figsize=(6 * ((n_features + 1) // 2), 12))\n",
    "        if n_features == 1:\n",
    "            axes = [axes]\n",
    "        elif (n_features + 1) // 2 == 1:\n",
    "            axes = axes.reshape(-1, 1)\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i, feature_name in enumerate(feature_names):\n",
    "            if i >= len(axes):\n",
    "                break\n",
    "                \n",
    "            df_viz = pd.DataFrame({\n",
    "                'Feature_Value': np.concatenate([X_real_original[:, i], X_random_original[:, i]]),\n",
    "                'Predictions': np.concatenate([y_real, y_pred_random]),\n",
    "                'Data_Type': ['Real'] * len(X_real_original) + ['Random'] * len(X_random_original)\n",
    "            })\n",
    "            \n",
    "            if filter_outliers:\n",
    "                feature_p5 = df_viz['Feature_Value'].quantile(0.05)\n",
    "                feature_p95 = df_viz['Feature_Value'].quantile(0.95)\n",
    "                pred_p5 = df_viz['Predictions'].quantile(0.05)\n",
    "                pred_p95 = df_viz['Predictions'].quantile(0.95)\n",
    "                \n",
    "                mask = (\n",
    "                    (df_viz['Feature_Value'] >= feature_p5) & \n",
    "                    (df_viz['Feature_Value'] <= feature_p95) &\n",
    "                    (df_viz['Predictions'] >= pred_p5) &\n",
    "                    (df_viz['Predictions'] <= pred_p95)\n",
    "                )\n",
    "                \n",
    "                df_viz_filtered = df_viz[mask].copy()\n",
    "                removed_points = len(df_viz) - len(df_viz_filtered)\n",
    "                \n",
    "                if removed_points > 0:\n",
    "                    print(f\"{feature_name}: Removidos {removed_points} outliers en visualización\")\n",
    "                \n",
    "                df_viz = df_viz_filtered\n",
    "            \n",
    "            ax = axes[i]\n",
    "            \n",
    "            real_data = df_viz[df_viz['Data_Type'] == 'Real']\n",
    "            random_data = df_viz[df_viz['Data_Type'] == 'Random']\n",
    "            \n",
    "            ax.scatter(real_data['Feature_Value'], real_data['Predictions'], \n",
    "                    color='blue', alpha=0.6, s=30, label='Real Data')\n",
    "            ax.scatter(random_data['Feature_Value'], random_data['Predictions'], \n",
    "                    color='red', alpha=0.6, s=30, label='Random Data')\n",
    "            \n",
    "            colors = {'Real': 'blue', 'Random': 'red'}\n",
    "            for data_type, color in colors.items():\n",
    "                subset = df_viz[df_viz['Data_Type'] == data_type]\n",
    "                if len(subset) > 1:\n",
    "                    lr = LinearRegression()\n",
    "                    X_trend = subset['Feature_Value'].values.reshape(-1, 1)\n",
    "                    y_trend = subset['Predictions'].values\n",
    "                    \n",
    "                    if np.var(X_trend) > 1e-10:\n",
    "                        lr.fit(X_trend, y_trend)\n",
    "                        \n",
    "                        x_line = np.linspace(subset['Feature_Value'].min(), \n",
    "                                        subset['Feature_Value'].max(), 100)\n",
    "                        y_line = lr.predict(x_line.reshape(-1, 1))\n",
    "                        \n",
    "                        try:\n",
    "                            r2 = lr.score(X_trend, y_trend)\n",
    "                            ax.plot(x_line, y_line, color=color, linestyle='--', \n",
    "                                alpha=0.8, linewidth=2, \n",
    "                                label=f'{data_type} Trend (R²={r2:.3f})')\n",
    "                        except:\n",
    "                            pass\n",
    "            \n",
    "            ax.set_xlabel(f'{feature_name}')\n",
    "            ax.set_ylabel('Predicciones')\n",
    "            ax.set_title(f'{feature_name}\\n({len(df_viz)} puntos) - Valores Originales')\n",
    "            ax.legend(fontsize=8)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        for i in range(len(feature_names), len(axes)):\n",
    "            axes[i].set_visible(False)\n",
    "        \n",
    "        filter_status = \"CON FILTRO\" if filter_outliers else \"SIN FILTRO\"\n",
    "        fig.suptitle(f'Análisis por Variable - {model_name.upper()} ({architecture}) ({filter_status}) - Valores Originales', \n",
    "                    fontsize=16, y=0.98)\n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(top=0.93)\n",
    "        \n",
    "        suffix = \"_filtered\" if filter_outliers else \"_original\"\n",
    "        filename = f\"{output_dir}/{model_name}_{architecture.lower()}_individual_features{suffix}.png\"\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Gráfico guardado: {filename}\")\n",
    "        \n",
    "    def train_and_visualize_all(self, df, architectures=['MLP'], filter_outliers=True, \n",
    "                               outlier_method='iqr', filter_viz_outliers=True, \n",
    "                               n_random_samples=1000, output_dir='multi_arch_results'):\n",
    "        \"\"\"Entrenar múltiples arquitecturas y crear visualizaciones\"\"\"\n",
    "        print(\"=\" * 80)\n",
    "        print(\"ENTRENANDO MÚLTIPLES ARQUITECTURAS CON VISUALIZACIONES\")\n",
    "        print(f\"Arquitecturas seleccionadas: {', '.join(architectures)}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        \n",
    "        viz_dir = f\"{output_dir}/visualizations\"\n",
    "        if not os.path.exists(viz_dir):\n",
    "            os.makedirs(viz_dir)\n",
    "        \n",
    "        if filter_outliers:\n",
    "            df_processed = self.filter_outliers(df, method=outlier_method)\n",
    "        else:\n",
    "            df_processed = df.copy()\n",
    "            print(f\"\\nSin filtrado de outliers - Dataset: {len(df_processed)} registros\")\n",
    "        \n",
    "        self.analyze_business_patterns(df_processed)\n",
    "        \n",
    "        X_base = df_processed[['INITIAL_AMOUNT','AVG_BET','Cluster','Weekday','Weekend','Month']]\n",
    "        y_tiempo = df_processed['TIME_ON_DEVICE_MIN']\n",
    "        y_bet = df_processed['BET_TOTAL'] \n",
    "        y_win = df_processed['WIN_TOTAL']\n",
    "        \n",
    "        X_base_train, X_base_test, y_tiempo_train, y_tiempo_test = train_test_split(\n",
    "            X_base, y_tiempo, test_size=0.2, random_state=42, \n",
    "            stratify=pd.cut(y_tiempo, bins=5, labels=False)\n",
    "        )\n",
    "        \n",
    "        train_idx, test_idx = X_base_train.index, X_base_test.index\n",
    "        \n",
    "        all_results = {'tiempo': {}, 'bet': {}, 'win': {}}\n",
    "        all_visualizations = {'tiempo': {}, 'bet': {}, 'win': {}}\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(\"MODELOS PARA PREDICCIÓN DE TIEMPO\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        X_tiempo_train, feature_names_tiempo = self.create_business_features(X_base_train)\n",
    "        X_tiempo_test, _ = self.create_business_features(X_base_test)\n",
    "        \n",
    "        self.feature_names['tiempo'] = feature_names_tiempo\n",
    "        \n",
    "        scaler_tiempo = RobustScaler()\n",
    "        X_tiempo_train_scaled = scaler_tiempo.fit_transform(X_tiempo_train)\n",
    "        X_tiempo_test_scaled = scaler_tiempo.transform(X_tiempo_test)\n",
    "        self.scalers['tiempo'] = scaler_tiempo\n",
    "        \n",
    "        best_tiempo_r2 = -np.inf\n",
    "        best_tiempo_arch = None\n",
    "        best_tiempo_pred_train = None\n",
    "        best_tiempo_pred_test = None\n",
    "        best_tiempo_model = None\n",
    "        \n",
    "        for arch in architectures:\n",
    "            try:\n",
    "                model, predictions, r2, mae, rmse, X_train_orig, X_test_orig = self.train_single_model(\n",
    "                    X_tiempo_train_scaled, X_tiempo_test_scaled, \n",
    "                    y_tiempo_train.values, y_tiempo_test.values,\n",
    "                    'tiempo', 'tiempo', arch\n",
    "                )\n",
    "                \n",
    "                if model is not None:\n",
    "                    all_results['tiempo'][arch] = {\n",
    "                        'R2': r2, 'MAE': mae, 'RMSE': rmse, \n",
    "                        'model': model, 'predictions': predictions\n",
    "                    }\n",
    "                    \n",
    "                    X_random_tiempo = self.generate_random_data(X_tiempo_train_scaled, n_random_samples)\n",
    "                    \n",
    "                    X_tiempo_test_original = scaler_tiempo.inverse_transform(X_test_orig)\n",
    "                    X_random_tiempo_original = scaler_tiempo.inverse_transform(X_random_tiempo)\n",
    "                    \n",
    "                    if arch in ['RNN', 'LSTM', 'GRU', 'CNN1D', 'TRANSFORMER']:\n",
    "                        X_random_prepared = self.prepare_sequence_data(X_random_tiempo, min(10, len(X_tiempo_train_scaled)))\n",
    "                    elif arch == 'CNN2D':\n",
    "                        X_random_prepared = self.prepare_cnn2d_data(X_random_tiempo)\n",
    "                    else:\n",
    "                        X_random_prepared = X_random_tiempo\n",
    "                    \n",
    "                    if arch == 'AUTOENCODER':\n",
    "                        y_random_tiempo_pred = model.predict(X_random_prepared, verbose=0)[0].flatten()\n",
    "                    else:\n",
    "                        y_random_tiempo_pred = model.predict(X_random_prepared, verbose=0).flatten()\n",
    "                    \n",
    "                    viz = self.create_individual_feature_visualizations(\n",
    "                        X_tiempo_test_scaled, y_tiempo_test.values, \n",
    "                        X_random_tiempo, y_random_tiempo_pred, \n",
    "                        X_tiempo_test_original, X_random_tiempo_original,\n",
    "                        feature_names_tiempo, 'tiempo', arch, viz_dir, filter_viz_outliers\n",
    "                    )\n",
    "                    all_visualizations['tiempo'][arch] = viz\n",
    "                    \n",
    "                    if r2 > best_tiempo_r2:\n",
    "                        best_tiempo_r2 = r2\n",
    "                        best_tiempo_arch = arch\n",
    "                        best_tiempo_model = model\n",
    "                        # Generate predictions for FULL train/test sets\n",
    "                        if arch in ['RNN', 'LSTM', 'GRU', 'CNN1D', 'TRANSFORMER']:\n",
    "                            X_train_for_pred = self.prepare_sequence_data(X_tiempo_train_scaled, min(10, len(X_tiempo_train_scaled)))\n",
    "                            X_test_for_pred = self.prepare_sequence_data(X_tiempo_test_scaled, min(10, len(X_tiempo_test_scaled)))\n",
    "                        elif arch == 'CNN2D':\n",
    "                            X_train_for_pred = self.prepare_cnn2d_data(X_tiempo_train_scaled)\n",
    "                            X_test_for_pred = self.prepare_cnn2d_data(X_tiempo_test_scaled)\n",
    "                        else:\n",
    "                            X_train_for_pred = X_tiempo_train_scaled\n",
    "                            X_test_for_pred = X_tiempo_test_scaled\n",
    "                        \n",
    "                        if arch == 'AUTOENCODER':\n",
    "                            best_tiempo_pred_train = model.predict(X_train_for_pred, verbose=0)[0].flatten()\n",
    "                            best_tiempo_pred_test = model.predict(X_test_for_pred, verbose=0)[0].flatten()\n",
    "                        else:\n",
    "                            best_tiempo_pred_train = model.predict(X_train_for_pred, verbose=0).flatten()\n",
    "                            best_tiempo_pred_test = model.predict(X_test_for_pred, verbose=0).flatten()\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Error con {arch} para tiempo: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\n🏆 Mejor modelo TIEMPO: {best_tiempo_arch} (R² = {best_tiempo_r2:.4f})\")\n",
    "        \n",
    "        # FIX: Ensure predictions match the original DataFrame lengths\n",
    "        print(f\"\\nAjustando predicciones de TIEMPO:\")\n",
    "        print(f\"  Train original: {len(X_base_train)}, Predicciones: {len(best_tiempo_pred_train)}\")\n",
    "        print(f\"  Test original: {len(X_base_test)}, Predicciones: {len(best_tiempo_pred_test)}\")\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(\"MODELOS PARA PREDICCIÓN DE BET TOTAL\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        X_bet_train, feature_names_bet = self.create_business_features(X_base_train, best_tiempo_pred_train)\n",
    "        X_bet_test, _ = self.create_business_features(X_base_test, best_tiempo_pred_test)\n",
    "        \n",
    "        self.feature_names['bet'] = feature_names_bet\n",
    "        \n",
    "        scaler_bet = RobustScaler()\n",
    "        X_bet_train_scaled = scaler_bet.fit_transform(X_bet_train)\n",
    "        X_bet_test_scaled = scaler_bet.transform(X_bet_test)\n",
    "        self.scalers['bet'] = scaler_bet\n",
    "        \n",
    "        best_bet_r2 = -np.inf\n",
    "        best_bet_arch = None\n",
    "        best_bet_pred_test = None\n",
    "        best_bet_model = None\n",
    "        \n",
    "        for arch in architectures:\n",
    "            try:\n",
    "                model, predictions, r2, mae, rmse, X_train_orig, X_test_orig = self.train_single_model(\n",
    "                    X_bet_train_scaled, X_bet_test_scaled,\n",
    "                    y_bet.loc[train_idx].values, y_bet.loc[test_idx].values,\n",
    "                    'bet', 'bet_total', arch\n",
    "                )\n",
    "                \n",
    "                if model is not None:\n",
    "                    all_results['bet'][arch] = {\n",
    "                        'R2': r2, 'MAE': mae, 'RMSE': rmse, \n",
    "                        'model': model, 'predictions': predictions\n",
    "                    }\n",
    "                    \n",
    "                    X_random_bet = self.generate_random_data(X_bet_train_scaled, n_random_samples)\n",
    "                    \n",
    "                    X_bet_test_original = scaler_bet.inverse_transform(X_test_orig)\n",
    "                    X_random_bet_original = scaler_bet.inverse_transform(X_random_bet)\n",
    "                    \n",
    "                    if arch in ['RNN', 'LSTM', 'GRU', 'CNN1D', 'TRANSFORMER']:\n",
    "                        X_random_prepared = self.prepare_sequence_data(X_random_bet, min(10, len(X_bet_train_scaled)))\n",
    "                    elif arch == 'CNN2D':\n",
    "                        X_random_prepared = self.prepare_cnn2d_data(X_random_bet)\n",
    "                    else:\n",
    "                        X_random_prepared = X_random_bet\n",
    "                    \n",
    "                    if arch == 'AUTOENCODER':\n",
    "                        y_random_bet_pred = model.predict(X_random_prepared, verbose=0)[0].flatten()\n",
    "                    else:\n",
    "                        y_random_bet_pred = model.predict(X_random_prepared, verbose=0).flatten()\n",
    "                    \n",
    "                    viz = self.create_individual_feature_visualizations(\n",
    "                        X_bet_test_scaled, y_bet.loc[test_idx].values,\n",
    "                        X_random_bet, y_random_bet_pred,\n",
    "                        X_bet_test_original, X_random_bet_original,\n",
    "                        feature_names_bet, 'bet', arch, viz_dir, filter_viz_outliers\n",
    "                    )\n",
    "                    all_visualizations['bet'][arch] = viz\n",
    "                    \n",
    "                    if r2 > best_bet_r2:\n",
    "                        best_bet_r2 = r2\n",
    "                        best_bet_arch = arch\n",
    "                        best_bet_model = model\n",
    "                        \n",
    "                        if arch in ['RNN', 'LSTM', 'GRU', 'CNN1D', 'TRANSFORMER']:\n",
    "                            X_test_for_pred = self.prepare_sequence_data(X_bet_test_scaled, min(10, len(X_bet_test_scaled)))\n",
    "                        elif arch == 'CNN2D':\n",
    "                            X_test_for_pred = self.prepare_cnn2d_data(X_bet_test_scaled)\n",
    "                        else:\n",
    "                            X_test_for_pred = X_bet_test_scaled\n",
    "                        \n",
    "                        if arch == 'AUTOENCODER':\n",
    "                            best_bet_pred_test = model.predict(X_test_for_pred, verbose=0)[0].flatten()\n",
    "                        else:\n",
    "                            best_bet_pred_test = model.predict(X_test_for_pred, verbose=0).flatten()\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Error con {arch} para bet: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\n🏆 Mejor modelo BET: {best_bet_arch} (R² = {best_bet_r2:.4f})\")\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(\"MODELOS PARA PREDICCIÓN DE WIN TOTAL\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Generate bet predictions for train set\n",
    "        if best_bet_arch in ['RNN', 'LSTM', 'GRU', 'CNN1D', 'TRANSFORMER']:\n",
    "            X_bet_train_prep = self.prepare_sequence_data(X_bet_train_scaled, min(10, len(X_bet_train_scaled)))\n",
    "        elif best_bet_arch == 'CNN2D':\n",
    "            X_bet_train_prep = self.prepare_cnn2d_data(X_bet_train_scaled)\n",
    "        else:\n",
    "            X_bet_train_prep = X_bet_train_scaled\n",
    "        \n",
    "        if best_bet_arch == 'AUTOENCODER':\n",
    "            best_bet_pred_train = best_bet_model.predict(X_bet_train_prep, verbose=0)[0].flatten()\n",
    "        else:\n",
    "            best_bet_pred_train = best_bet_model.predict(X_bet_train_prep, verbose=0).flatten()\n",
    "        \n",
    "        print(f\"\\nAjustando predicciones de BET:\")\n",
    "        print(f\"  Train original: {len(X_base_train)}, Predicciones: {len(best_bet_pred_train)}\")\n",
    "        print(f\"  Test original: {len(X_base_test)}, Predicciones: {len(best_bet_pred_test)}\")\n",
    "        \n",
    "        X_win_train, feature_names_win = self.create_business_features(\n",
    "            X_base_train, best_tiempo_pred_train, best_bet_pred_train\n",
    "        )\n",
    "        X_win_test, _ = self.create_business_features(\n",
    "            X_base_test, best_tiempo_pred_test, best_bet_pred_test\n",
    "        )\n",
    "        \n",
    "        self.feature_names['win'] = feature_names_win\n",
    "        \n",
    "        scaler_win = RobustScaler()\n",
    "        X_win_train_scaled = scaler_win.fit_transform(X_win_train)\n",
    "        X_win_test_scaled = scaler_win.transform(X_win_test)\n",
    "        self.scalers['win'] = scaler_win\n",
    "        \n",
    "        best_win_r2 = -np.inf\n",
    "        best_win_arch = None\n",
    "        \n",
    "        for arch in architectures:\n",
    "            try:\n",
    "                model, predictions, r2, mae, rmse, X_train_orig, X_test_orig = self.train_single_model(\n",
    "                    X_win_train_scaled, X_win_test_scaled,\n",
    "                    y_win.loc[train_idx].values, y_win.loc[test_idx].values,\n",
    "                    'win', 'win_total', arch\n",
    "                )\n",
    "                \n",
    "                if model is not None:\n",
    "                    all_results['win'][arch] = {\n",
    "                        'R2': r2, 'MAE': mae, 'RMSE': rmse, \n",
    "                        'model': model, 'predictions': predictions\n",
    "                    }\n",
    "                    \n",
    "                    X_random_win = self.generate_random_data(X_win_train_scaled, n_random_samples)\n",
    "                    \n",
    "                    X_win_test_original = scaler_win.inverse_transform(X_test_orig)\n",
    "                    X_random_win_original = scaler_win.inverse_transform(X_random_win)\n",
    "                    \n",
    "                    if arch in ['RNN', 'LSTM', 'GRU', 'CNN1D', 'TRANSFORMER']:\n",
    "                        X_random_prepared = self.prepare_sequence_data(X_random_win, min(10, len(X_win_train_scaled)))\n",
    "                    elif arch == 'CNN2D':\n",
    "                        X_random_prepared = self.prepare_cnn2d_data(X_random_win)\n",
    "                    else:\n",
    "                        X_random_prepared = X_random_win\n",
    "                    \n",
    "                    if arch == 'AUTOENCODER':\n",
    "                        y_random_win_pred = model.predict(X_random_prepared, verbose=0)[0].flatten()\n",
    "                    else:\n",
    "                        y_random_win_pred = model.predict(X_random_prepared, verbose=0).flatten()\n",
    "                    \n",
    "                    viz = self.create_individual_feature_visualizations(\n",
    "                        X_win_test_scaled, y_win.loc[test_idx].values,\n",
    "                        X_random_win, y_random_win_pred,\n",
    "                        X_win_test_original, X_random_win_original,\n",
    "                        feature_names_win, 'win', arch, viz_dir, filter_viz_outliers\n",
    "                    )\n",
    "                    all_visualizations['win'][arch] = viz\n",
    "                    \n",
    "                    if r2 > best_win_r2:\n",
    "                        best_win_r2 = r2\n",
    "                        best_win_arch = arch\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Error con {arch} para win: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "        \n",
    "        if best_win_arch:\n",
    "            print(f\"\\n🏆 Mejor modelo WIN: {best_win_arch} (R² = {best_win_r2:.4f})\")\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"RESUMEN COMPARATIVO DE TODAS LAS ARQUITECTURAS\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        comparison_data = []\n",
    "        for target in ['tiempo', 'bet', 'win']:\n",
    "            for arch in architectures:\n",
    "                if arch in all_results[target]:\n",
    "                    result = all_results[target][arch]\n",
    "                    comparison_data.append({\n",
    "                        'Target': target.upper(),\n",
    "                        'Architecture': arch,\n",
    "                        'R2_Score': result['R2'],\n",
    "                        'MAE': result['MAE'],\n",
    "                        'RMSE': result['RMSE'],\n",
    "                        'N_Features': len(self.feature_names[target]),\n",
    "                        'Features': ', '.join(self.feature_names[target])\n",
    "                    })\n",
    "        \n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        comparison_filename = f\"{output_dir}/architecture_comparison.csv\"\n",
    "        comparison_df.to_csv(comparison_filename, index=False)\n",
    "        print(f\"✓ Comparación guardada: {comparison_filename}\")\n",
    "        \n",
    "        print(f\"\\n🏆 MEJORES MODELOS POR OBJETIVO:\")\n",
    "        print(f\"   TIEMPO: {best_tiempo_arch} (R² = {best_tiempo_r2:.4f})\")\n",
    "        print(f\"   BET: {best_bet_arch} (R² = {best_bet_r2:.4f})\")\n",
    "        if best_win_arch:\n",
    "            print(f\"   WIN: {best_win_arch} (R² = {best_win_r2:.4f})\")\n",
    "        \n",
    "        arch_rankings = {}\n",
    "        for arch in architectures:\n",
    "            r2_scores = []\n",
    "            for target in ['tiempo', 'bet', 'win']:\n",
    "                if arch in all_results[target]:\n",
    "                    r2_scores.append(all_results[target][arch]['R2'])\n",
    "            if r2_scores:\n",
    "                arch_rankings[arch] = np.mean(r2_scores)\n",
    "        \n",
    "        print(f\"\\n📊 RANKING DE ARQUITECTURAS (R² promedio):\")\n",
    "        for i, (arch, avg_r2) in enumerate(sorted(arch_rankings.items(), key=lambda x: x[1], reverse=True), 1):\n",
    "            print(f\"   {i}. {arch}: {avg_r2:.4f}\")\n",
    "        \n",
    "        print(f\"\\n📁 Resultados guardados en: '{output_dir}'\")\n",
    "        print(f\"📁 Visualizaciones guardadas en: '{viz_dir}'\")\n",
    "        \n",
    "        return {\n",
    "            'results': all_results,\n",
    "            'comparison': comparison_df,\n",
    "            'visualizations': all_visualizations,\n",
    "            'best_models': {\n",
    "                'tiempo': best_tiempo_arch,\n",
    "                'bet': best_bet_arch,\n",
    "                'win': best_win_arch\n",
    "            }\n",
    "        }\n",
    "    def train_single_model(self, X_train, X_test, y_train, y_test, model_type, target_name, architecture='MLP'):\n",
    "        \"\"\"Entrenar un solo modelo con arquitectura específica\"\"\"\n",
    "        print(f\"\\n=== {architecture} - {target_name.upper()} ===\")\n",
    "        \n",
    "        X_train_original = X_train.copy()\n",
    "        X_test_original = X_test.copy()\n",
    "        \n",
    "        if architecture in ['RNN', 'LSTM', 'GRU', 'CNN1D', 'TRANSFORMER']:\n",
    "            sequence_length = min(10, len(X_train))\n",
    "            X_train_prepared = self.prepare_sequence_data(X_train, sequence_length)\n",
    "            X_test_prepared = self.prepare_sequence_data(X_test, sequence_length)\n",
    "            \n",
    "            if len(y_train) > len(X_train_prepared):\n",
    "                y_train = y_train[len(y_train) - len(X_train_prepared):]\n",
    "            if len(y_test) > len(X_test_prepared):\n",
    "                y_test = y_test[len(y_test) - len(X_test_prepared):]\n",
    "                \n",
    "            input_shape = X_train_prepared.shape[1:]\n",
    "            \n",
    "        elif architecture == 'CNN2D':\n",
    "            X_train_prepared = self.prepare_cnn2d_data(X_train)\n",
    "            X_test_prepared = self.prepare_cnn2d_data(X_test)\n",
    "            input_shape = X_train_prepared.shape[1:]\n",
    "            \n",
    "        else:\n",
    "            X_train_prepared = X_train_original\n",
    "            X_test_prepared = X_test_original\n",
    "            input_shape = X_train.shape[1]\n",
    "        \n",
    "        model = None\n",
    "        if architecture == 'MLP':\n",
    "            model = self.create_mlp_model(input_shape, model_type)\n",
    "        elif architecture == 'RNN':\n",
    "            model = self.create_rnn_model(input_shape, model_type)\n",
    "        elif architecture == 'LSTM':\n",
    "            model = self.create_lstm_model(input_shape, model_type)\n",
    "        elif architecture == 'GRU':\n",
    "            model = self.create_gru_model(input_shape, model_type)\n",
    "        elif architecture == 'CNN1D':\n",
    "            model = self.create_cnn1d_model(input_shape, model_type)\n",
    "        elif architecture == 'CNN2D':\n",
    "            model = self.create_cnn2d_model(input_shape, model_type)\n",
    "        elif architecture == 'TRANSFORMER':\n",
    "            model = self.create_transformer_model(input_shape, model_type)\n",
    "        elif architecture == 'AUTOENCODER':\n",
    "            model = self.create_autoencoder_model(input_shape, model_type)\n",
    "        else:\n",
    "            print(f\"Error: Arquitectura '{architecture}' no reconocida\")\n",
    "    \n",
    "    def analyze_business_patterns(self, df):\n",
    "        \"\"\"Análisis de patrones de negocio\"\"\"\n",
    "        print(f\"\\n=== ANÁLISIS DE PATRONES DE NEGOCIO ===\")\n",
    "        reinvested = df['BET_TOTAL'] > df['INITIAL_AMOUNT']\n",
    "        print(f\"Sesiones con reinversión: {reinvested.sum()} ({reinvested.sum()/len(df)*100:.1f}%)\")\n",
    "        \n",
    "        net_winners = df['FINAL_AMOUNT'] > df['INITIAL_AMOUNT']\n",
    "        net_losers = df['FINAL_AMOUNT'] < df['INITIAL_AMOUNT']\n",
    "        \n",
    "        print(f\"Ganadores netos: {net_winners.sum()} ({net_winners.sum()/len(df)*100:.1f}%)\")\n",
    "        print(f\"Perdedores netos: {net_losers.sum()} ({net_losers.sum()/len(df)*100:.1f}%)\")\n",
    "        \n",
    "        win_bet_corr = np.corrcoef(df['WIN_TOTAL'], df['BET_TOTAL'])[0,1]\n",
    "        print(f\"Correlación WIN vs BET: {win_bet_corr:.3f}\")\n",
    "        \n",
    "        print(f\"Dataset shape: {df.shape}\")\n",
    "        print(f\"Rango TIEMPO: {df['TIME_ON_DEVICE_MIN'].min():.1f} - {df['TIME_ON_DEVICE_MIN'].max():.1f}\")\n",
    "        print(f\"Rango BET: {df['BET_TOTAL'].min():.1f} - {df['BET_TOTAL'].max():.1f}\")\n",
    "        print(f\"Rango WIN: {df['WIN_TOTAL'].min():.1f} - {df['WIN_TOTAL'].max():.1f}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa39967f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ENTRENANDO MÚLTIPLES ARQUITECTURAS CON VISUALIZACIONES\n",
      "Arquitecturas seleccionadas: RNN\n",
      "================================================================================\n",
      "\n",
      "Sin filtrado de outliers - Dataset: 209079 registros\n",
      "\n",
      "=== ANÁLISIS DE PATRONES DE NEGOCIO ===\n",
      "Sesiones con reinversión: 160049 (76.5%)\n",
      "Ganadores netos: 36184 (17.3%)\n",
      "Perdedores netos: 172469 (82.5%)\n",
      "Correlación WIN vs BET: 0.870\n",
      "Dataset shape: (209079, 41)\n",
      "Rango TIEMPO: 0.1 - 537.9\n",
      "Rango BET: 0.0 - 134666.6\n",
      "Rango WIN: 0.0 - 461502.0\n",
      "\n",
      "==================================================\n",
      "MODELOS PARA PREDICCIÓN DE TIEMPO\n",
      "==================================================\n",
      "\n",
      "=== RNN - TIEMPO ===\n",
      "Error con RNN para tiempo: cannot unpack non-iterable NoneType object\n",
      "\n",
      "🏆 Mejor modelo TIEMPO: None (R² = -inf)\n",
      "\n",
      "Ajustando predicciones de TIEMPO:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/ld/2yb_9mvs5t90r3x4r3v3btbc0000gn/T/ipykernel_33721/1356875699.py\", line 749, in train_and_visualize_all\n",
      "    model, predictions, r2, mae, rmse, X_train_orig, X_test_orig = self.train_single_model(\n",
      "TypeError: cannot unpack non-iterable NoneType object\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m casino_ml \u001b[38;5;241m=\u001b[39m MultiArchitectureCasinoML()\n\u001b[0;32m----> 3\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mcasino_ml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_and_visualize_all\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdf_data_general\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43marchitectures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRNN\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilter_viz_outliers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilter_outliers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_random_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmlp_only_results\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 818\u001b[0m, in \u001b[0;36mMultiArchitectureCasinoML.train_and_visualize_all\u001b[0;34m(self, df, architectures, filter_outliers, outlier_method, filter_viz_outliers, n_random_samples, output_dir)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# FIX: Ensure predictions match the original DataFrame lengths\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAjustando predicciones de TIEMPO:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 818\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Train original: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(X_base_train)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Predicciones: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(best_tiempo_pred_train)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Test original: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(X_base_test)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Predicciones: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(best_tiempo_pred_test)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "casino_ml = MultiArchitectureCasinoML()\n",
    "\n",
    "results = casino_ml.train_and_visualize_all(\n",
    "        df_data_general,\n",
    "        architectures=['RNN'],\n",
    "        filter_viz_outliers=False,\n",
    "        filter_outliers=False,\n",
    "        n_random_samples=1000,\n",
    "        output_dir='mlp_only_results'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11251c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "--------------------------------------------------------------------------- ValueError Traceback (most recent call last) Cell In[8], line 3 1 casino_ml = MultiArchitectureCasinoML() ----> 3 results = casino_ml.train_and_visualize_all( 4 df_data_general, 5 architectures=['RNN'], 6 filter_viz_outliers=False, 7 filter_outliers=False, 8 n_random_samples=1000, 9 output_dir='mlp_only_results' 10 ) Cell In[4], line 922 919 print(\"MODELOS PARA PREDICCIÓN DE BET TOTAL\") 920 print(f\"{'='*50}\") --> 922 X_bet_train, feature_names_bet = self.create_business_features(X_base_train, best_tiempo_pred_train) 923 X_bet_test, _ = self.create_business_features(X_base_test, best_tiempo_pred_test) 925 self.feature_names['bet'] = feature_names_bet Cell In[4], line 85 82 feature_names = ['INITIAL_AMOUNT', 'AVG_BET', 'Cluster'] 84 if tiempo_pred is not None: ---> 85 features['tiempo_pred'] = tiempo_pred 86 feature_names.append('tiempo_pred')\n",
    "...\n",
    " 560 \"does not match length of index \" 561 f\"({len(index)})\" 562 ) ValueError: Length of values (167254) does not match length of index (167263)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
